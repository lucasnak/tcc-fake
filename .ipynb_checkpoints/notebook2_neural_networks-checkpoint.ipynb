{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5644c2fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\vitor\\anaconda3\\lib\\site-packages (3.2.4)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from spacy) (2.4.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from spacy) (3.0.6)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from spacy) (1.8.2)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from spacy) (8.0.15)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from spacy) (1.0.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from spacy) (1.20.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from spacy) (1.0.7)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from spacy) (0.9.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from spacy) (21.0)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from spacy) (0.6.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: click<8.1.0 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from spacy) (8.0.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from spacy) (0.7.7)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from spacy) (4.62.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from spacy) (2.26.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from spacy) (58.0.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from click<8.1.0->spacy) (0.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy) (3.10.0.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from jinja2->spacy) (1.1.1)\n",
      "Collecting pt-core-news-sm==3.2.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-25 23:32:06.187788: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2022-05-25 23:32:06.188929: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-3.2.0/pt_core_news_sm-3.2.0-py3-none-any.whl (22.2 MB)\n",
      "Requirement already satisfied: spacy<3.3.0,>=3.2.0 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from pt-core-news-sm==3.2.0) (3.2.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (4.62.3)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (0.4.1)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (1.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (21.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (58.0.4)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (8.0.15)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: click<8.1.0 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (8.0.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (2.11.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (1.0.7)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (0.9.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (2.0.7)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (3.0.9)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (3.3.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (3.0.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (2.26.0)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (0.6.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (1.20.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (1.8.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (2.4.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (0.7.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from click<8.1.0->spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (0.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (3.0.4)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (3.10.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (3.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (1.26.7)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (1.1.1)\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('pt_core_news_sm')\n",
      "Requirement already satisfied: wordcloud in c:\\users\\vitor\\anaconda3\\lib\\site-packages (1.8.1)\n",
      "Requirement already satisfied: pillow in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from wordcloud) (8.4.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from wordcloud) (3.4.3)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from wordcloud) (1.20.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (3.0.4)\n",
      "Requirement already satisfied: six in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib->wordcloud) (1.16.0)\n",
      "Requirement already satisfied: gensim in c:\\users\\vitor\\anaconda3\\lib\\site-packages (4.1.2)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from gensim) (1.20.3)\n",
      "Requirement already satisfied: Cython==0.29.23 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from gensim) (0.29.23)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from gensim) (1.7.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\vitor\\anaconda3\\lib\\site-packages (2.8.0)\n",
      "Requirement already satisfied: flatbuffers>=1.12 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from tensorflow) (2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from tensorflow) (3.10.0.2)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from tensorflow) (3.20.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from tensorflow) (1.44.0)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from tensorflow) (1.0.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from tensorflow) (3.2.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from tensorflow) (58.0.4)\n",
      "Requirement already satisfied: gast>=0.2.1 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from tensorflow) (0.5.3)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from tensorflow) (1.20.3)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from tensorflow) (2.8.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from tensorflow) (0.24.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: tensorboard<2.9,>=2.8 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from tensorflow) (2.8.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from tensorflow) (2.8.0.dev2021122109)\n",
      "Requirement already satisfied: libclang>=9.0.1 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from tensorflow) (13.0.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.37.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.6.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.26.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (3.3.6)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (5.0.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (4.8.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (3.6.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (3.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\vitor\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (3.2.0)\n"
     ]
    }
   ],
   "source": [
    "# Bibliotecas além do gerenciador Anaconda\n",
    "!pip install spacy\n",
    "!python -m spacy download pt_core_news_sm\n",
    "!pip install wordcloud\n",
    "!pip install gensim\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f4eced3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Módulos básicos para manuseio de dados e arquivos\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from os.path import isfile, join\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "\n",
    "# Módulos para visualização de dados\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "%matplotlib inline\n",
    "\n",
    "# Módulo para processamento de linguagem\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788de39a",
   "metadata": {},
   "source": [
    "## Carregamento de textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4597ed41",
   "metadata": {},
   "outputs": [],
   "source": [
    "limited_news_path = r'Software\\Fake.br-Corpus' #\\fake_10 or \\true_10\n",
    "news_path = r'Software\\Fake.br-Corpus\\full_texts' #\\fake or \\true\n",
    "\n",
    "paths = [limited_news_path, news_path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97d20bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sortDir(dir_path: str, is_meta=False) ->list:\n",
    "    '''\n",
    "    Ordena os arquivos dentro de dir_path e os retorna no formato de lista.\n",
    "    '''\n",
    "    if is_meta:\n",
    "        number_separator = \"-meta.txt\"\n",
    "    else:\n",
    "        number_separator = \".txt\"\n",
    "\n",
    "    first_list = os.listdir(dir_path)\n",
    "    int_list = [int(element.split(number_separator)[0]) for element in first_list]\n",
    "    int_list.sort()\n",
    "    final_list = [(str(element) + number_separator) for element in int_list]\n",
    "\n",
    "    return final_list\n",
    "\n",
    "def txtToDataframe(path, is_limited=True):\n",
    "    '''\n",
    "    Function for converting full texts to a single DataFrame.\n",
    "    '''\n",
    "    if is_limited:\n",
    "        true_files = [path+\"\\\\true_10\\\\\"+f for f in sortDir(dir_path = path+'\\\\true_10') if isfile(join(path+'\\\\true_10', f))]\n",
    "        fake_files = [path+\"\\\\fake_10\\\\\"+f for f in sortDir(dir_path = path+'\\\\fake_10') if isfile(join(path+'\\\\fake_10', f))]\n",
    "    else:\n",
    "        true_files = [path+\"\\\\true\\\\\"+f for f in sortDir(dir_path = path+'\\\\true') if isfile(join(path+'\\\\true', f))]\n",
    "        fake_files = [path+\"\\\\fake\\\\\"+f for f in sortDir(dir_path = path+'\\\\fake') if isfile(join(path+'\\\\fake', f))]\n",
    "    \n",
    "    texts = []\n",
    "    labels = []\n",
    "    \n",
    "    for file in true_files:\n",
    "        with open(file, encoding='utf-8-sig') as f:\n",
    "            texts.append(f.read())\n",
    "            labels.append(0)\n",
    "    for file in fake_files:\n",
    "        with open(file, encoding='utf-8-sig') as f:\n",
    "            texts.append(f.read())\n",
    "            labels.append(1)\n",
    "            \n",
    "    df = pd.DataFrame(list(zip(texts,labels)),columns=['texts','labels'])\n",
    "    \n",
    "    # Com esta função, textos e labels foram inseridos em um DataFrame de maneira sequencial. Todas as notícias verdadeiras vêm\n",
    "    # ANTES do bloco de notícias falsas.\n",
    "    \n",
    "    return df\n",
    "\n",
    "def appendMetadata(path,df, is_limited=True):\n",
    "    '''\n",
    "    Function for appending metadata to previously generated news DataFrame.\n",
    "    '''\n",
    "    if is_limited:\n",
    "        true_meta = [path+\"\\\\true-meta-information-10\\\\\"+f for f in sortDir(dir_path = path+'\\\\true-meta-information-10',is_meta=True) if isfile(join(path+'\\\\true-meta-information-10', f))]\n",
    "        fake_meta = [path+\"\\\\fake-meta-information-10\\\\\"+f for f in sortDir(dir_path = path+'\\\\fake-meta-information-10',is_meta=True) if isfile(join(path+'\\\\fake-meta-information-10', f))]\n",
    "    else:\n",
    "        true_meta = [path+\"\\\\true-meta-information\\\\\"+f for f in sortDir(dir_path = path+'\\\\true-meta-information',is_meta=True) if isfile(join(path+'\\\\true-meta-information', f))]\n",
    "        fake_meta = [path+\"\\\\fake-meta-information\\\\\"+f for f in sortDir(dir_path = path+'\\\\fake-meta-information',is_meta=True) if isfile(join(path+'\\\\fake-meta-information', f))]\n",
    "    \n",
    "\n",
    "    #true_meta e fake_meta são listas com todas os paths para arquivos de metadata.\n",
    "    \n",
    "    columns = [\"author\", \"source\", \"category\", \"date\",\"tokens\",\"words_without_punctuation\",\"types\",\"number_of_links\",\"uppercase_words\",\"verbs\",\"subjuntive_imperative\",\"nouns\",\"adjectives\",\"adverbs\",\"modal_verbs\",\"singular_first_and_second_personal_pronouns\",\"plural_first_personal_pronouns\",\"pronouns\",\"pausality\",\"characters\",\"avg_sentence_length\",\"avg_word_length\",\"percentage_of_spelling_errors\",\"emotiveness\",\"diversity\"]\n",
    "    \n",
    "    true_metadata = pd.DataFrame(columns=columns)\n",
    "    fake_metadata = pd.DataFrame(columns=columns)\n",
    "    \n",
    "    for file in true_meta:\n",
    "        #print(file)\n",
    "        aux = pd.read_csv(file, header=None, sep = '\\n').transpose()\n",
    "        aux.columns = columns\n",
    "        true_metadata=true_metadata.append(aux)\n",
    "        \n",
    "        \n",
    "    for file in fake_meta:\n",
    "        #print(file)\n",
    "        aux = pd.read_csv(file, header=None, sep = '\\n').transpose()\n",
    "        aux.columns = columns\n",
    "        fake_metadata=fake_metadata.append(aux)\n",
    "        \n",
    "    \n",
    "    metadata = pd.DataFrame(columns=columns)\n",
    "    metadata = metadata.append(true_metadata,ignore_index=True)\n",
    "    metadata = metadata.append(fake_metadata,ignore_index=True) \n",
    "\n",
    "\n",
    "    complete_df = pd.concat([df,metadata],axis=1)\n",
    "    # Este DataFrame possui todos os textos/labels (2 colunas) e metadata (25 colunas).\n",
    "    \n",
    "    return complete_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9db387d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - Base com 10 notícias verdadeiras e 10 notícias falsas\n",
      "1 - Base completa de notícias\n",
      "1\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12032/3355685898.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mcomplete_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mappendMetadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Dataframe contendo notícias, labels e metadata.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtxtToDataframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mis_limited\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mcomplete_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mappendMetadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mis_limited\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12032/3996252045.py\u001b[0m in \u001b[0;36mtxtToDataframe\u001b[1;34m(path, is_limited)\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mfake_files\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"\\\\fake_10\\\\\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mf\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msortDir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdir_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'\\\\fake_10'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'\\\\fake_10'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0mtrue_files\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"\\\\true\\\\\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mf\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msortDir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdir_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'\\\\true'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'\\\\true'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[0mfake_files\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"\\\\fake\\\\\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mf\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msortDir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdir_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'\\\\fake'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'\\\\fake'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12032/3996252045.py\u001b[0m in \u001b[0;36msortDir\u001b[1;34m(dir_path, is_meta)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mint_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melement\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumber_separator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0melement\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfirst_list\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mint_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mfinal_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melement\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnumber_separator\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0melement\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mint_list\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mfinal_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12032/3996252045.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mint_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melement\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumber_separator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0melement\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfirst_list\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mint_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mfinal_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melement\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnumber_separator\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0melement\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mint_list\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mfinal_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "ai = int(input('''0 - Base com 10 notícias verdadeiras e 10 notícias falsas\n",
    "1 - Base completa de notícias\n",
    "'''))\n",
    "\n",
    "path = paths[ai]\n",
    "\n",
    "if ai == 0:\n",
    "    data = txtToDataframe(path) # Dataframe contendo notícias e labels.\n",
    "    complete_data = appendMetadata(path,data) # Dataframe contendo notícias, labels e metadata.\n",
    "else:\n",
    "    data = txtToDataframe(path,is_limited=False)\n",
    "    complete_data = appendMetadata(path,data,is_limited=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc2db81",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_data['texts'][14]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8090df3d",
   "metadata": {},
   "source": [
    "## Preprocessamento de textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea64800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando o pacote de língua portuguesa para o processador Spacy\n",
    "nlp = spacy.load('pt_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf9fa87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defininido funções de preprocessamento\n",
    "\n",
    "def removePunct(text):\n",
    "    '''\n",
    "    Removes any punctuation included in string.punctuation.\n",
    "    '''\n",
    "    translator = text.maketrans({key:'' for key in string.punctuation+'“”'}) # Translates any punctuation into ''\n",
    "    return text.translate(translator)\n",
    "\n",
    "def removeNumbers(text):\n",
    "    '''\n",
    "    Removes any number character in text.\n",
    "    '''\n",
    "    return re.sub('[0-9]', '' , text) # Translates any number into ''\n",
    "\n",
    "def removeStopWords(string):\n",
    "    '''\n",
    "    Removes any portuguese stopwords, using Spacy's standard package.\n",
    "    '''\n",
    "    doc = nlp(string)\n",
    "    return ' '.join([token.text for token in doc if token.is_stop is False])\n",
    "\n",
    "def lemmatize(string):\n",
    "    '''\n",
    "    Lemmatizes text word-by-word. Notice that lemmatizing is not as harsh as stemming, which makes the final text easier to read and understand in common language.\n",
    "    '''\n",
    "    doc = nlp(string)\n",
    "    return ' '.join([token.lemma_ for token in doc])\n",
    "\n",
    "def prep(string, useStopWords = False, lemma = False):\n",
    "    '''\n",
    "    Executes previously defined preprocessing in text.\n",
    "    '''\n",
    "\n",
    "    result = removeNumbers(removePunct(string)).lower()\n",
    "    \n",
    "    if useStopWords and lemma:\n",
    "        doc = nlp(result)\n",
    "        result = ' '.join([token.lemma_ for token in doc if token.is_stop is False])\n",
    "    elif useStopWords:\n",
    "        doc = nlp(result)\n",
    "        result = ' '.join([token.text for token in doc if token.is_stop is False])\n",
    "    elif lemma:\n",
    "        doc = nlp(result)\n",
    "        result = ' '.join([token.lemma_ for token in doc])\n",
    "\n",
    "    result = result.replace('\\n',\"\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e24c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizando preprocessamento de textos presentes no Dataframe de notícias completo.\n",
    "\n",
    "complete_data['texts'] = complete_data['texts'].apply(prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bba55be",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_data['texts'][14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fe0241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignando variáveis dependentes e independentes\n",
    "\n",
    "y = complete_data['labels'].values # y is strings for labels; but should be fake-0/true-1\n",
    "X = [d.split() for d in complete_data['texts'].tolist()] # X is a list of lists of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bd2774",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X[14])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0849b4e4",
   "metadata": {},
   "source": [
    "## Tokenization (TensorFlow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5c20fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando módulos para tokenização de textos\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a2d75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignando bases de teste e treino\n",
    "\n",
    "test_limit = 0.10\n",
    "training_sentences,testing_sentences,training_labels,testing_labels = train_test_split(X,y,test_size=test_limit, random_state=42)\n",
    "\n",
    "#training_sentences = X[0:test_limit]\n",
    "#testing_sentences = X[test_limit:]\n",
    "\n",
    "#training_labels = y[0:test_limit]\n",
    "#testing_labels = y[test_limit:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9faaa7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(training_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52174586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerando dicionário de tokens (com base nos textos de treinamento)\n",
    "\n",
    "tokenizer = Tokenizer()#oov_token='<OOV>')\n",
    "\n",
    "tokenizer.fit_on_texts(training_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014ec728",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0290bf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizando a sequencialização das bases de treinamento e teste\n",
    "\n",
    "training_sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127841cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analisar notícias maiores do que max_length\n",
    "\n",
    "min_length = 250\n",
    "max_length = 250\n",
    "\n",
    "#plt.hist([len(x) for x in X], bins = 750)\n",
    "#plt.show()\n",
    "\n",
    "nos = np.array([len(x) for x in training_sentences])\n",
    "print(\"There are \"+ str(len(nos[nos>=max_length])) + \" news equal or longer than \"+ str(max_length) + \" words (will be truncated).\")\n",
    "\n",
    "nos = np.array([len(x) for x in training_sentences])\n",
    "print(\"There are \"+ str(len(nos[nos<min_length])) + \" news shorter than \"+ str(min_length) + \" words (will be killed).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebdd2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matar notícias menores do que min_length\n",
    "\n",
    "flag_dict = {}\n",
    "for i in range(0,len(training_sequences)):\n",
    "    if len(training_sequences[i]) < min_length:\n",
    "        flag_dict[i] = True\n",
    "    else:\n",
    "        flag_dict[i] = False\n",
    "        \n",
    "training_labels = [training_labels[i] for i in range(0,len(flag_dict)) if (flag_dict[i] == False)]\n",
    "\n",
    "training_sequences = [seq for seq in training_sequences if len(seq)>=min_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db3ec50",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(training_sequences)):    \n",
    "    print(len(training_sequences[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11936835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truncar (padding) das sequências tokenizadas\n",
    "\n",
    "training_padded = pad_sequences(training_sequences,maxlen=max_length,padding='post',truncating='pre')\n",
    "#testing_padded = pad_sequences(testing_sequences,maxlen=max_length,padding='post',truncating='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932623fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(training_padded)):    \n",
    "    print(len(training_padded[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64eb8aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = tokenizer.word_index\n",
    "inv_vocab = {v: k for k, v in vocab.items()} # reconverte o token para a palavra correpondente.\n",
    "\n",
    "training_texts = []\n",
    "testing_texts = []\n",
    "\n",
    "# Com estas listas \"_texts\", temos os textos novamente completos, mas agora com 'OOV's para palavras desconhecidas\n",
    "# na base de testes.\n",
    "for sequence in training_sequences:\n",
    "    news=[]\n",
    "    for token in sequence:\n",
    "        news.append(inv_vocab[token])  #inv_vocab[token] é a palavra correspondente ao token.\n",
    "    training_texts.append(news)\n",
    "\n",
    "\n",
    "for sequence in testing_sequences:\n",
    "    news=[]\n",
    "    for token in sequence:\n",
    "        news.append(inv_vocab[token])\n",
    "    testing_texts.append(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c964c5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_texts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a046fba",
   "metadata": {},
   "source": [
    "## Vectorization (gensim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf4699b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea46cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM = 100\n",
    "w2v_model = gensim.models.Word2Vec(sentences=training_sentences, vector_size=DIM, window=10, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bb1d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar('bolsonaro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f2c569",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv['bolsonaro']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18e1af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index)+1\n",
    "\n",
    "def get_weight_matrix(model):\n",
    "    ''' Inserts every word vector in a NumPy array, ordering them by token icon (which is determined by their appearing frequencies). '''\n",
    "    weight_matrix = np.zeros((vocab_size, DIM))\n",
    "\n",
    "    for word, token in vocab.items():\n",
    "        weight_matrix[token] = model.wv[word]\n",
    "\n",
    "    return weight_matrix\n",
    "\n",
    "embedding_vectors = get_weight_matrix(model=w2v_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bccc60",
   "metadata": {},
   "source": [
    "## Criação da rede neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9355159d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, Conv1D, MaxPool1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75b4f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=DIM, weights = [embedding_vectors], input_length=max_length, trainable=False))\n",
    "model.add(LSTM(units=128))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef00d2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(training_padded, np.transpose(np.asarray(training_labels)), validation_split=0.3, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84b73b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(testing_sentences) # Sequencing testing sentences.\n",
    "padded = pad_sequences(testing_sequences,maxlen=max_length,padding='post',truncating='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37358e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = (model.predict(padded) >=0.5).astype(int)\n",
    "\n",
    "accuracy_score(testing_labels, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7ceff0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
