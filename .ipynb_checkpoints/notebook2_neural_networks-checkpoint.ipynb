{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5644c2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibliotecas além do gerenciador Anaconda\n",
    "!pip install spacy\n",
    "!python -m spacy download pt_core_news_sm\n",
    "!pip install wordcloud\n",
    "!pip install gensim\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f4eced3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Módulos básicos para manuseio de dados e arquivos\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from os.path import isfile, join\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "\n",
    "# Módulos para visualização de dados\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "%matplotlib inline\n",
    "\n",
    "# Módulo para processamento de linguagem\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788de39a",
   "metadata": {},
   "source": [
    "## Carregamento de textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "4597ed41",
   "metadata": {},
   "outputs": [],
   "source": [
    "limited_news_path = r'Software\\Fake.br-Corpus' #\\fake_10 or \\true_10\n",
    "news_path = r'Software\\Fake.br-Corpus\\full_texts' #\\fake or \\true\n",
    "\n",
    "paths = [limited_news_path, news_path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "97d20bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sortDir(dir_path: str, is_meta=False) ->list:\n",
    "    '''\n",
    "    Ordena os arquivos dentro de dir_path e os retorna no formato de lista.\n",
    "    '''\n",
    "    if is_meta:\n",
    "        number_separator = \"-meta.txt\"\n",
    "    else:\n",
    "        number_separator = \".txt\"\n",
    "\n",
    "    first_list = os.listdir(dir_path)\n",
    "    int_list = [int(element.split(number_separator)[0]) for element in first_list]\n",
    "    int_list.sort()\n",
    "    final_list = [(str(element) + number_separator) for element in int_list]\n",
    "\n",
    "    return final_list\n",
    "\n",
    "def txtToDataframe(path, is_limited=True):\n",
    "    '''\n",
    "    Function for converting full texts to a single DataFrame.\n",
    "    '''\n",
    "    if is_limited:\n",
    "        true_files = [path+\"\\\\true_10\\\\\"+f for f in sortDir(dir_path = path+'\\\\true_10') if isfile(join(path+'\\\\true_10', f))]\n",
    "        fake_files = [path+\"\\\\fake_10\\\\\"+f for f in sortDir(dir_path = path+'\\\\fake_10') if isfile(join(path+'\\\\fake_10', f))]\n",
    "    else:\n",
    "        true_files = [path+\"\\\\true\\\\\"+f for f in sortDir(dir_path = path+'\\\\true') if isfile(join(path+'\\\\true', f))]\n",
    "        fake_files = [path+\"\\\\fake\\\\\"+f for f in sortDir(dir_path = path+'\\\\fake') if isfile(join(path+'\\\\fake', f))]\n",
    "    \n",
    "    texts = []\n",
    "    labels = []\n",
    "    \n",
    "    for file in true_files:\n",
    "        with open(file, encoding='utf-8-sig') as f:\n",
    "            texts.append(f.read())\n",
    "            labels.append(0)\n",
    "    for file in fake_files:\n",
    "        with open(file, encoding='utf-8-sig') as f:\n",
    "            texts.append(f.read())\n",
    "            labels.append(1)\n",
    "            \n",
    "    df = pd.DataFrame(list(zip(texts,labels)),columns=['texts','labels'])\n",
    "    \n",
    "    # Com esta função, textos e labels foram inseridos em um DataFrame de maneira sequencial. Todas as notícias verdadeiras vêm\n",
    "    # ANTES do bloco de notícias falsas.\n",
    "    \n",
    "    return df\n",
    "\n",
    "def appendMetadata(path,df, is_limited=True):\n",
    "    '''\n",
    "    Function for appending metadata to previously generated news DataFrame.\n",
    "    '''\n",
    "    if is_limited:\n",
    "        true_meta = [path+\"\\\\true-meta-information-10\\\\\"+f for f in sortDir(dir_path = path+'\\\\true-meta-information-10',is_meta=True) if isfile(join(path+'\\\\true-meta-information-10', f))]\n",
    "        fake_meta = [path+\"\\\\fake-meta-information-10\\\\\"+f for f in sortDir(dir_path = path+'\\\\fake-meta-information-10',is_meta=True) if isfile(join(path+'\\\\fake-meta-information-10', f))]\n",
    "    else:\n",
    "        true_meta = [path+\"\\\\true-meta-information\\\\\"+f for f in sortDir(dir_path = path+'\\\\true-meta-information',is_meta=True) if isfile(join(path+'\\\\true-meta-information', f))]\n",
    "        fake_meta = [path+\"\\\\fake-meta-information\\\\\"+f for f in sortDir(dir_path = path+'\\\\fake-meta-information',is_meta=True) if isfile(join(path+'\\\\fake-meta-information', f))]\n",
    "    \n",
    "\n",
    "    #true_meta e fake_meta são listas com todas os paths para arquivos de metadata.\n",
    "    \n",
    "    columns = [\"author\", \"source\", \"category\", \"date\",\"tokens\",\"words_without_punctuation\",\"types\",\"number_of_links\",\"uppercase_words\",\"verbs\",\"subjuntive_imperative\",\"nouns\",\"adjectives\",\"adverbs\",\"modal_verbs\",\"singular_first_and_second_personal_pronouns\",\"plural_first_personal_pronouns\",\"pronouns\",\"pausality\",\"characters\",\"avg_sentence_length\",\"avg_word_length\",\"percentage_of_spelling_errors\",\"emotiveness\",\"diversity\"]\n",
    "    \n",
    "    true_metadata = pd.DataFrame(columns=columns)\n",
    "    fake_metadata = pd.DataFrame(columns=columns)\n",
    "    \n",
    "    for file in true_meta:\n",
    "        #print(file)\n",
    "        aux = pd.read_csv(file, header=None, sep = '\\n').transpose()\n",
    "        aux.columns = columns\n",
    "        true_metadata=true_metadata.append(aux)\n",
    "        \n",
    "        \n",
    "    for file in fake_meta:\n",
    "        #print(file)\n",
    "        aux = pd.read_csv(file, header=None, sep = '\\n').transpose()\n",
    "        aux.columns = columns\n",
    "        fake_metadata=fake_metadata.append(aux)\n",
    "        \n",
    "    \n",
    "    metadata = pd.DataFrame(columns=columns)\n",
    "    metadata = metadata.append(true_metadata,ignore_index=True)\n",
    "    metadata = metadata.append(fake_metadata,ignore_index=True) \n",
    "\n",
    "\n",
    "    complete_df = pd.concat([df,metadata],axis=1)\n",
    "    # Este DataFrame possui todos os textos/labels (2 colunas) e metadata (25 colunas).\n",
    "    \n",
    "    return complete_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "d9db387d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - Base com 10 notícias verdadeiras e 10 notícias falsas\n",
      "1 - Base completa de notícias\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "ai = int(input('''0 - Base com 10 notícias verdadeiras e 10 notícias falsas\n",
    "1 - Base completa de notícias\n",
    "'''))\n",
    "\n",
    "path = paths[ai]\n",
    "\n",
    "if ai == 0:\n",
    "    data = txtToDataframe(path) # Dataframe contendo notícias e labels.\n",
    "    complete_data = appendMetadata(path,data) # Dataframe contendo notícias, labels e metadata.\n",
    "else:\n",
    "    data = txtToDataframe(path,is_limited=False)\n",
    "    complete_data = appendMetadata(path,data,is_limited=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "32982cf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0\n",
       "1     0\n",
       "2     0\n",
       "3     0\n",
       "4     0\n",
       "5     0\n",
       "6     0\n",
       "7     0\n",
       "8     0\n",
       "9     0\n",
       "10    1\n",
       "11    1\n",
       "12    1\n",
       "13    1\n",
       "14    1\n",
       "15    1\n",
       "16    1\n",
       "17    1\n",
       "18    1\n",
       "19    1\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_data['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fbc2db81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'temer resolve o problema de luislinda \\x93liberdade ainda que tardia\\x94a ministra dos direitos humanos luislinda valois entrou para a galeria dos \\x93sem noção\\x94 ao pedir para acumular o salário da função de ministra com a aposentadoria de desembargadora o que daria r  mil muito além do teto constitucional ao fazêlo alegou que sua condição \\x93sem sombra de dúvida se assemelha a trabalho escravo\\x94agora o jornal carioca o dia divulga que temer decidiu demitir luislinda e dar um fim ao seu problema ao que parece o presidente ainda busca uma saída honrosa para luislinda talvez outro cargo não sabemos aindasua substituta deve ser a deputada federal licenciada tia eron prb atual secretária municipal de promoção socialo que importa é que luislinda enfim conseguiu sua liberdade ainda que tardia'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_data['texts'][14]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8090df3d",
   "metadata": {},
   "source": [
    "## Preprocessamento de textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ea64800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando o pacote de língua portuguesa para o processador Spacy\n",
    "nlp = spacy.load('pt_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "dbf9fa87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defininido funções de preprocessamento\n",
    "\n",
    "def removePunct(text):\n",
    "    '''\n",
    "    Removes any punctuation included in string.punctuation.\n",
    "    '''\n",
    "    translator = text.maketrans({key:'' for key in string.punctuation+'“”'}) # Translates any punctuation into ''\n",
    "    return text.translate(translator)\n",
    "\n",
    "def removeNumbers(text):\n",
    "    '''\n",
    "    Removes any number character in text.\n",
    "    '''\n",
    "    return re.sub('[0-9]', '' , text) # Translates any number into ''\n",
    "\n",
    "def removeStopWords(string):\n",
    "    '''\n",
    "    Removes any portuguese stopwords, using Spacy's standard package.\n",
    "    '''\n",
    "    doc = nlp(string)\n",
    "    return ' '.join([token.text for token in doc if token.is_stop is False])\n",
    "\n",
    "def lemmatize(string):\n",
    "    '''\n",
    "    Lemmatizes text word-by-word. Notice that lemmatizing is not as harsh as stemming, which makes the final text easier to read and understand in common language.\n",
    "    '''\n",
    "    doc = nlp(string)\n",
    "    return ' '.join([token.lemma_ for token in doc])\n",
    "\n",
    "def prep(string, useStopWords = False, lemma = False):\n",
    "    '''\n",
    "    Executes previously defined preprocessing in text.\n",
    "    '''\n",
    "\n",
    "    result = removeNumbers(removePunct(string)).lower()\n",
    "    \n",
    "    if useStopWords and lemma:\n",
    "        doc = nlp(result)\n",
    "        result = ' '.join([token.lemma_ for token in doc if token.is_stop is False])\n",
    "    elif useStopWords:\n",
    "        doc = nlp(result)\n",
    "        result = ' '.join([token.text for token in doc if token.is_stop is False])\n",
    "    elif lemma:\n",
    "        doc = nlp(result)\n",
    "        result = ' '.join([token.lemma_ for token in doc])\n",
    "    return result.replace('\\n',\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "58e24c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizando preprocessamento de textos presentes no Dataframe de notícias completo.\n",
    "\n",
    "complete_data['texts'] = complete_data['texts'].apply(prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8bba55be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'temer resolve o problema de luislinda \\x93liberdade ainda que tardia\\x94a ministra dos direitos humanos luislinda valois entrou para a galeria dos \\x93sem noção\\x94 ao pedir para acumular o salário da função de ministra com a aposentadoria de desembargadora o que daria r  mil muito além do teto constitucional ao fazêlo alegou que sua condição \\x93sem sombra de dúvida se assemelha a trabalho escravo\\x94agora o jornal carioca o dia divulga que temer decidiu demitir luislinda e dar um fim ao seu problema ao que parece o presidente ainda busca uma saída honrosa para luislinda talvez outro cargo não sabemos aindasua substituta deve ser a deputada federal licenciada tia eron prb atual secretária municipal de promoção socialo que importa é que luislinda enfim conseguiu sua liberdade ainda que tardia'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_data['texts'][14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "68fe0241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignando variáveis dependentes e independentes\n",
    "\n",
    "y = complete_data['labels'].values # y is strings for labels; but should be fake-0/true-1\n",
    "X = [d.split() for d in complete_data['texts'].tolist()] # X is a list of lists of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "95bd2774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['o', 'podemos', 'decidiu', 'expulsar', 'o', 'deputado', 'federal', 'carlos', 'gaguim', 'do', 'partido', 'após', 'a', 'polícia', 'federal', 'fazer', 'buscas', 'a', 'apreensões', 'no', 'gabinete', 'dele', 'na', 'câmara', 'com', 'isso', 'a', 'legenda', 'abre', 'espaço', 'para', 'receber', 'a', 'senadora', 'expulsa', 'pelo', 'pmdb', 'katia', 'abreu', 'por', 'meio', 'de', 'nota', 'a', 'legenda', 'informou', 'que', 'o', 'afastamento', 'do', 'parlamentar', 'já', 'era', 'algo', 'acordado', 'entre', 'os', 'filiados', 'da', 'sigla', 'ainda', 'que', 'o', 'parlamentar', 'tenha', 'comunicado', 'a', 'conclusão', 'de', 'sua', 'desfiliação', 'para', 'esta', 'semana', 'diante', 'dos', 'fatos', 'noticiados', 'hoje', 'a', 'executiva', 'nacional', 'do', 'podemos', 'solicita', 'o', 'imediato', 'cancelamento', 'de', 'sua', 'filiação', 'dos', 'quadros', 'do', 'partidoo', 'partido', 'que', 'no', 'passado', 'chegou', 'a', 'cogitar', 'lançar', 'o', 'parlamentar', 'como', 'candidato', 'ao', 'senado', 'diz', 'que', 'apoia', 'a', 'investigação', 'com', 'a', 'ampla', 'apuração', 'dos', 'eventuais', 'crimes', 'cometidos', 'e', 'a', 'consequente', 'responsabilização', 'dos', 'envolvidos', 'para', 'que', 'todos', 'sejam', 'punidos', 'com', 'o', 'máximo', 'rigor', 'da', 'lei', 'independentemente', 'de', 'posição', 'ou', 'cargo', 'ocupado']\n"
     ]
    }
   ],
   "source": [
    "print(X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0849b4e4",
   "metadata": {},
   "source": [
    "## Tokenization (TensorFlow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "ec5c20fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando módulos para tokenização de textos\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "18a2d75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignando bases de teste e treino\n",
    "\n",
    "test_limit = 0.20\n",
    "training_sentences,testing_sentences,training_labels,testing_labels = train_test_split(X,y,test_size=test_limit, random_state=42)\n",
    "\n",
    "#training_sentences = X[0:test_limit]\n",
    "#testing_sentences = X[test_limit:]\n",
    "\n",
    "#training_labels = y[0:test_limit]\n",
    "#testing_labels = y[test_limit:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "9faaa7da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "52174586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerando dicionário de tokens (com base nos textos de treinamento)\n",
    "\n",
    "tokenizer = Tokenizer()#oov_token='<OOV>')\n",
    "\n",
    "tokenizer.fit_on_texts(training_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "014ec728",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'de': 1,\n",
       " 'que': 2,\n",
       " 'o': 3,\n",
       " 'a': 4,\n",
       " 'do': 5,\n",
       " 'e': 6,\n",
       " 'da': 7,\n",
       " 'em': 8,\n",
       " 'para': 9,\n",
       " 'não': 10,\n",
       " 'no': 11,\n",
       " 'um': 12,\n",
       " 'uma': 13,\n",
       " 'na': 14,\n",
       " 'foi': 15,\n",
       " 'ao': 16,\n",
       " 'é': 17,\n",
       " 'com': 18,\n",
       " 'por': 19,\n",
       " 'à': 20,\n",
       " 'as': 21,\n",
       " 'como': 22,\n",
       " 'se': 23,\n",
       " 'sua': 24,\n",
       " 'os': 25,\n",
       " 'dos': 26,\n",
       " 'ser': 27,\n",
       " 'são': 28,\n",
       " 'ele': 29,\n",
       " 'pelo': 30,\n",
       " 'seu': 31,\n",
       " 'partido': 32,\n",
       " 'bolsonaro': 33,\n",
       " 'federal': 34,\n",
       " 'das': 35,\n",
       " 'vai': 36,\n",
       " 'disse': 37,\n",
       " 'diz': 38,\n",
       " 'público': 39,\n",
       " 'presidente': 40,\n",
       " 'ter': 41,\n",
       " 'governo': 42,\n",
       " 'tem': 43,\n",
       " 'paulo': 44,\n",
       " 'pela': 45,\n",
       " 'pt': 46,\n",
       " 'também': 47,\n",
       " 'mas': 48,\n",
       " 'ou': 49,\n",
       " 'silvio': 50,\n",
       " 'prefeito': 51,\n",
       " 'psdb': 52,\n",
       " 'anos': 53,\n",
       " 'alckmin': 54,\n",
       " 'já': 55,\n",
       " 'ministério': 56,\n",
       " 'contra': 57,\n",
       " 'estado': 58,\n",
       " 'até': 59,\n",
       " 'isso': 60,\n",
       " 'pode': 61,\n",
       " 'dia': 62,\n",
       " 'sobre': 63,\n",
       " 'ainda': 64,\n",
       " 'dinheiro': 65,\n",
       " 'após': 66,\n",
       " 'muito': 67,\n",
       " 'projeto': 68,\n",
       " 'campanha': 69,\n",
       " 'r': 70,\n",
       " 'está': 71,\n",
       " 'deve': 72,\n",
       " 'você': 73,\n",
       " 'santos': 74,\n",
       " 'mais': 75,\n",
       " 'candidato': 76,\n",
       " 'suas': 77,\n",
       " 'kátia': 78,\n",
       " 'estão': 79,\n",
       " 'segundo': 80,\n",
       " 'tempo': 81,\n",
       " 'senador': 82,\n",
       " 'país': 83,\n",
       " 'ano': 84,\n",
       " 'aos': 85,\n",
       " 'outro': 86,\n",
       " 'dois': 87,\n",
       " 'pessoas': 88,\n",
       " 'bivar': 89,\n",
       " 'às': 90,\n",
       " 'grande': 91,\n",
       " 'lei': 92,\n",
       " 'tipo': 93,\n",
       " 'vezes': 94,\n",
       " 'depois': 95,\n",
       " 'menos': 96,\n",
       " 'doria': 97,\n",
       " 'celso': 98,\n",
       " 'joão': 99,\n",
       " 'teatro': 100,\n",
       " 'maior': 101,\n",
       " 'atual': 102,\n",
       " 'direitos': 103,\n",
       " 'ela': 104,\n",
       " 'presidência': 105,\n",
       " 'prévia': 106,\n",
       " 'psl': 107,\n",
       " 'momento': 108,\n",
       " 'deveria': 109,\n",
       " 'ferraz': 110,\n",
       " 'fidel': 111,\n",
       " 'raúl': 112,\n",
       " 'luislinda': 113,\n",
       " 'portal': 114,\n",
       " 'sérgio': 115,\n",
       " 'bala': 116,\n",
       " 'volta': 117,\n",
       " 'onde': 118,\n",
       " 'parte': 119,\n",
       " 'era': 120,\n",
       " 'juiz': 121,\n",
       " 'foram': 122,\n",
       " 'irmãos': 123,\n",
       " 'durante': 124,\n",
       " 'milhões': 125,\n",
       " 'seja': 126,\n",
       " 'político': 127,\n",
       " 'justiça': 128,\n",
       " 'acordo': 129,\n",
       " 'caso': 130,\n",
       " 'vitória': 131,\n",
       " 'palavra': 132,\n",
       " 'eles': 133,\n",
       " 'fez': 134,\n",
       " 'fazer': 135,\n",
       " 'afirma': 136,\n",
       " 'cada': 137,\n",
       " 'zé': 138,\n",
       " 'oficina': 139,\n",
       " 'grupo': 140,\n",
       " 'informações': 141,\n",
       " 'poder': 142,\n",
       " 'crime': 143,\n",
       " 'nesta': 144,\n",
       " 'nova': 145,\n",
       " 'virgílio': 146,\n",
       " 'governador': 147,\n",
       " 'eu': 148,\n",
       " 'liberal': 149,\n",
       " 'nacional': 150,\n",
       " 'cuba': 151,\n",
       " 'ustra': 152,\n",
       " 'então': 153,\n",
       " 'discurso': 154,\n",
       " 'dias': 155,\n",
       " 'nas': 156,\n",
       " 'geddel': 157,\n",
       " 'assessoria': 158,\n",
       " 'pmdb': 159,\n",
       " 'valor': 160,\n",
       " 'expulsão': 161,\n",
       " 'ação': 162,\n",
       " 'mpma': 163,\n",
       " 'paço': 164,\n",
       " 'lumiar': 165,\n",
       " 'domingos': 166,\n",
       " 'dutra': 167,\n",
       " 'município': 168,\n",
       " 'acesso': 169,\n",
       " 'exgovernador': 170,\n",
       " 'cabral': 171,\n",
       " 'juca': 172,\n",
       " 'tony': 173,\n",
       " 'chegaram': 174,\n",
       " 'janeiro': 175,\n",
       " 'pública': 176,\n",
       " 'lava': 177,\n",
       " 'jato': 178,\n",
       " 'delação': 179,\n",
       " 'antes': 180,\n",
       " 'operação': 181,\n",
       " 'situação': 182,\n",
       " 'pelos': 183,\n",
       " 'entrevista': 184,\n",
       " 'outros': 185,\n",
       " 'política': 186,\n",
       " 'eleitoral': 187,\n",
       " 'reclamar': 188,\n",
       " 'dedada': 189,\n",
       " 'me': 190,\n",
       " 'dedadas': 191,\n",
       " 'tréllez': 192,\n",
       " 'primeiro': 193,\n",
       " 'quando': 194,\n",
       " 'dedo': 195,\n",
       " 'entre': 196,\n",
       " 'jogador': 197,\n",
       " 'falar': 198,\n",
       " 'bunda': 199,\n",
       " 'tão': 200,\n",
       " 'recebeu': 201,\n",
       " 'sentido': 202,\n",
       " 'sem': 203,\n",
       " 'tenha': 204,\n",
       " 'apenas': 205,\n",
       " 'algo': 206,\n",
       " '\\x96': 207,\n",
       " 'homem': 208,\n",
       " 'reunião': 209,\n",
       " 'processo': 210,\n",
       " 'porque': 211,\n",
       " 'palocci': 212,\n",
       " 'pagou': 213,\n",
       " 'expresidente': 214,\n",
       " 'exministro': 215,\n",
       " 'sendo': 216,\n",
       " 'filmagens': 217,\n",
       " 'evitar': 218,\n",
       " 'qualquer': 219,\n",
       " 'quanto': 220,\n",
       " 'três': 221,\n",
       " 'propriedade': 222,\n",
       " 'feita': 223,\n",
       " 'há': 224,\n",
       " 'busca': 225,\n",
       " 'mesmo': 226,\n",
       " 'municipal': 227,\n",
       " 'feito': 228,\n",
       " 'passado': 229,\n",
       " 'arthur': 230,\n",
       " 'divide': 231,\n",
       " 'cargo': 232,\n",
       " 'imprensa': 233,\n",
       " 'prefeitura': 234,\n",
       " 'somente': 235,\n",
       " 'presidencial': 236,\n",
       " 'militar': 237,\n",
       " 'dilma': 238,\n",
       " 'câmara': 239,\n",
       " 'nunca': 240,\n",
       " 'morte': 241,\n",
       " 'povo': 242,\n",
       " 'vez': 243,\n",
       " 'dr': 244,\n",
       " 'site': 245,\n",
       " 'parece': 246,\n",
       " 'além': 247,\n",
       " 'veja': 248,\n",
       " 'gustavo': 249,\n",
       " 'pf': 250,\n",
       " 'aécio': 251,\n",
       " 'salvador': 252,\n",
       " 'bahia': 253,\n",
       " 'encontradas': 254,\n",
       " 'função': 255,\n",
       " 'fim': 256,\n",
       " 'líder': 257,\n",
       " 'novembro': 258,\n",
       " 'falta': 259,\n",
       " 'abreu': 260,\n",
       " 'informação': 261,\n",
       " 'transparência': 262,\n",
       " 'doleiros': 263,\n",
       " 'acusados': 264,\n",
       " 'lavar': 265,\n",
       " 'claret': 266,\n",
       " 'barbosa': 267,\n",
       " 'rio': 268,\n",
       " 'quintafeira': 269,\n",
       " 'voo': 270,\n",
       " 'h': 271,\n",
       " 'semana': 272,\n",
       " 'agentes': 273,\n",
       " 'polícia': 274,\n",
       " 'zona': 275,\n",
       " 'cadeia': 276,\n",
       " 'propina': 277,\n",
       " 'através': 278,\n",
       " 'esquema': 279,\n",
       " 'corrupção': 280,\n",
       " 'chefiado': 281,\n",
       " 'presos': 282,\n",
       " 'desde': 283,\n",
       " 'uruguai': 284,\n",
       " 'vara': 285,\n",
       " 'criminal': 286,\n",
       " 'marcelo': 287,\n",
       " 'chebar': 288,\n",
       " 'mpf': 289,\n",
       " 'contaram': 290,\n",
       " 'assumir': 291,\n",
       " 'disso': 292,\n",
       " 'dólares': 293,\n",
       " 'exterior': 294,\n",
       " 'existir': 295,\n",
       " 'delcídio': 296,\n",
       " 'amaral': 297,\n",
       " 'preso': 298,\n",
       " 'moro': 299,\n",
       " 'recursos': 300,\n",
       " 'anistia': 301,\n",
       " 'fiscais': 302,\n",
       " 'incluindo': 303,\n",
       " 'tesouro': 304,\n",
       " 'políticos': 305,\n",
       " 'difícil': 306,\n",
       " 'adiante': 307,\n",
       " 'época': 308,\n",
       " 'teve': 309,\n",
       " 'contas': 310,\n",
       " 'mil': 311,\n",
       " 'blog': 312,\n",
       " 'dá': 313,\n",
       " 'entender': 314,\n",
       " 'futebol': 315,\n",
       " 'jornal': 316,\n",
       " 'pessoal': 317,\n",
       " 'demais': 318,\n",
       " 'rodrigo': 319,\n",
       " 'ponte': 320,\n",
       " 'preta': 321,\n",
       " 'jogo': 322,\n",
       " 'qual': 323,\n",
       " 'ocorreu': 324,\n",
       " 'duas': 325,\n",
       " '\\x93o': 326,\n",
       " 'medo': 327,\n",
       " 'segue': 328,\n",
       " 'acho': 329,\n",
       " 'muitos': 330,\n",
       " 'sequer': 331,\n",
       " 'conseguem': 332,\n",
       " 'alguns': 333,\n",
       " 'dizer': 334,\n",
       " 'gente': 335,\n",
       " 'melhor': 336,\n",
       " 'hetero': 337,\n",
       " 'luz': 338,\n",
       " 'lá': 339,\n",
       " 'blogueiro': 340,\n",
       " 'colocar': 341,\n",
       " 'receber': 342,\n",
       " 'santoso': 343,\n",
       " 'josé': 344,\n",
       " 'torres': 345,\n",
       " 'construir': 346,\n",
       " 'sede': 347,\n",
       " 'maestro': 348,\n",
       " 'carlos': 349,\n",
       " 'martins': 350,\n",
       " 'defesa': 351,\n",
       " 'dentro': 352,\n",
       " 'fará': 353,\n",
       " 'local': 354,\n",
       " 'movimento': 355,\n",
       " 'lula': 356,\n",
       " 'peru': 357,\n",
       " 'pressão': 358,\n",
       " 'divulga': 359,\n",
       " 'veio': 360,\n",
       " 'fato': 361,\n",
       " 'petistas': 362,\n",
       " 'nos': 363,\n",
       " 'meses': 364,\n",
       " 'mp': 365,\n",
       " 'pede': 366,\n",
       " 'proibição': 367,\n",
       " 'mam': 368,\n",
       " 'minuta': 369,\n",
       " 'termo': 370,\n",
       " 'conduta': 371,\n",
       " 'museu': 372,\n",
       " 'arte': 373,\n",
       " 'assinatura': 374,\n",
       " 'fosse': 375,\n",
       " 'brasileira': 376,\n",
       " 'ocasião': 377,\n",
       " 'fica': 378,\n",
       " 'responsável': 379,\n",
       " 'fotografias': 380,\n",
       " 'toda': 381,\n",
       " 'ambiente': 382,\n",
       " 'precisam': 383,\n",
       " 'cara': 384,\n",
       " 'terçafeira': 385,\n",
       " 'direito': 386,\n",
       " 'teatral': 387,\n",
       " 'mesma': 388,\n",
       " 'alega': 389,\n",
       " 'danos': 390,\n",
       " 'cidade': 391,\n",
       " 'aberto': 392,\n",
       " 'privada': 393,\n",
       " 'aceitar': 394,\n",
       " 'rompeu': 395,\n",
       " 'diálogo': 396,\n",
       " 'decisão': 397,\n",
       " 'estadual': 398,\n",
       " 'agora': 399,\n",
       " 'levar': 400,\n",
       " 'seus': 401,\n",
       " 'manaus': 402,\n",
       " 'carta': 403,\n",
       " 'prévias': 404,\n",
       " 'geraldo': 405,\n",
       " 'legenda': 406,\n",
       " 'escolhe': 407,\n",
       " 'escolher': 408,\n",
       " 'podem': 409,\n",
       " 'dar': 410,\n",
       " 'exemplo': 411,\n",
       " 'tucanocídio': 412,\n",
       " 'resultado': 413,\n",
       " 'afirmou': 414,\n",
       " 'críticas': 415,\n",
       " 'terça': 416,\n",
       " 'colega': 417,\n",
       " 'expressão': 418,\n",
       " 'sigla': 419,\n",
       " 'jair': 420,\n",
       " 'deputado': 421,\n",
       " 'quatro': 422,\n",
       " 'précandidato': 423,\n",
       " 'hoje': 424,\n",
       " 'pesquisas': 425,\n",
       " 'livres': 426,\n",
       " 'união': 427,\n",
       " 'representa': 428,\n",
       " 'liberais': 429,\n",
       " 'tanto': 430,\n",
       " 'fase': 431,\n",
       " 'vejo': 432,\n",
       " 'comportamento': 433,\n",
       " 'dele': 434,\n",
       " 'essa': 435,\n",
       " 'coronel': 436,\n",
       " 'brilhante': 437,\n",
       " 'ditadura': 438,\n",
       " 'votação': 439,\n",
       " 'memória': 440,\n",
       " 'meu': 441,\n",
       " 'final': 442,\n",
       " 'verdade': 443,\n",
       " 'gestão': 444,\n",
       " 'desaparecimento': 445,\n",
       " 'suposto': 446,\n",
       " 'elogios': 447,\n",
       " 'naquele': 448,\n",
       " 'período': 449,\n",
       " 'comunista': 450,\n",
       " 'militares': 451,\n",
       " 'regime': 452,\n",
       " 'brasil': 453,\n",
       " 'transição': 454,\n",
       " 'próprios': 455,\n",
       " 'ninguém': 456,\n",
       " 'supremo': 457,\n",
       " 'tribunal': 458,\n",
       " 'sob': 459,\n",
       " 'incitação': 460,\n",
       " 'exministra': 461,\n",
       " 'humanos': 462,\n",
       " 'falou': 463,\n",
       " 'deputada': 464,\n",
       " 'nenhuma': 465,\n",
       " 'querem': 466,\n",
       " 'constitucional': 467,\n",
       " 'essas': 468,\n",
       " 'futuro': 469,\n",
       " 'nós': 470,\n",
       " 'bem': 471,\n",
       " 'vem': 472,\n",
       " 'relação': 473,\n",
       " 'mandato': 474,\n",
       " 'sport': 475,\n",
       " 'marketing': 476,\n",
       " 'comando': 477,\n",
       " 'eleição': 478,\n",
       " 'aquela': 479,\n",
       " 'boa': 480,\n",
       " 'pouco': 481,\n",
       " 'petrobrás': 482,\n",
       " 'mãos': 483,\n",
       " 'chineses\\x94': 484,\n",
       " 'garanto': 485,\n",
       " 'brechas': 486,\n",
       " 'venda': 487,\n",
       " 'direita': 488,\n",
       " 'reduzido': 489,\n",
       " 'diminuição': 490,\n",
       " 'rey': 491,\n",
       " 'admitiu': 492,\n",
       " 'malas': 493,\n",
       " 'trabalhado': 494,\n",
       " 'neves': 495,\n",
       " 'civil': 496,\n",
       " 'bunker': 497,\n",
       " 'digitais': 498,\n",
       " 'história': 499,\n",
       " 'mala': 500,\n",
       " 'souto': 501,\n",
       " 'espaço': 502,\n",
       " 'reformas': 503,\n",
       " 'acrescentou': 504,\n",
       " 'tucana': 505,\n",
       " 'go': 506,\n",
       " '\\x93desembarque\\x94': 507,\n",
       " 'castro': 508,\n",
       " 'partir': 509,\n",
       " 'estados': 510,\n",
       " 'unidos': 511,\n",
       " 'algumas': 512,\n",
       " 'trabalho': 513,\n",
       " 'sabemos': 514,\n",
       " 'saída': 515,\n",
       " 'algum': 516,\n",
       " 'santiago': 517,\n",
       " 'moldura': 518,\n",
       " 'currículo': 519,\n",
       " 'servem': 520,\n",
       " 'benefícios': 521,\n",
       " 'temer': 522,\n",
       " 'problema': 523,\n",
       " 'ministra': 524,\n",
       " '\\x93sem': 525,\n",
       " 'condenação': 526,\n",
       " 'comunicação': 527,\n",
       " 'informou': 528,\n",
       " 'todas': 529,\n",
       " 'providências': 530,\n",
       " 'atenda': 531,\n",
       " 'exigências': 532,\n",
       " 'nota': 533,\n",
       " 'completa': 534,\n",
       " 'pagamento': 535,\n",
       " 'prazo': 536,\n",
       " 'procuradoria': 537,\n",
       " 'dentre': 538,\n",
       " 'impossibilidade': 539,\n",
       " 'real': 540,\n",
       " 'conteúdo': 541,\n",
       " 'acompanhamento': 542,\n",
       " 's': 543,\n",
       " 'roubado': 544,\n",
       " 'quadrilha': 545,\n",
       " 'vinícius': 546,\n",
       " 'conhecido': 547,\n",
       " 'cláudio': 548,\n",
       " 'fernando': 549,\n",
       " 'tarde': 550,\n",
       " 'desta': 551,\n",
       " 'vindos': 552,\n",
       " 'uruguaieles': 553,\n",
       " 'comercial': 554,\n",
       " 'pousou': 555,\n",
       " 'aeroporto': 556,\n",
       " 'galeão': 557,\n",
       " 'extradição': 558,\n",
       " 'deles': 559,\n",
       " 'concedida': 560,\n",
       " 'uruguaio': 561,\n",
       " 'passadaescoltados': 562,\n",
       " 'instituto': 563,\n",
       " 'médico': 564,\n",
       " 'legal': 565,\n",
       " 'portuária': 566,\n",
       " 'fizeram': 567,\n",
       " 'exames': 568,\n",
       " 'seguiram': 569,\n",
       " 'benfica': 570,\n",
       " 'norte': 571,\n",
       " 'presosambos': 572,\n",
       " 'forçatarefa': 573,\n",
       " 'obtida': 574,\n",
       " 'pmdbjuca': 575,\n",
       " 'março': 576,\n",
       " 'rj': 577,\n",
       " 'bretas': 578,\n",
       " 'expedir': 579,\n",
       " 'mandado': 580,\n",
       " 'prisão': 581,\n",
       " 'duplavinícius': 582,\n",
       " 'claudio': 583,\n",
       " 'citados': 584,\n",
       " 'premiada': 585,\n",
       " 'renato': 586,\n",
       " 'precisaram': 587,\n",
       " 'acionar': 588,\n",
       " 'lavagem': 589,\n",
       " 'dinheiroalém': 590,\n",
       " 'escondia': 591,\n",
       " 'tentar': 592,\n",
       " 'interferir': 593,\n",
       " 'estiveram': 594,\n",
       " 'atagonismo': 595,\n",
       " 'sulmatogrossense': 596,\n",
       " 'autor': 597,\n",
       " 'premitiria': 598,\n",
       " 'trazer': 599,\n",
       " 'declarados': 600,\n",
       " 'proposta': 601,\n",
       " 'amplamente': 602,\n",
       " 'criticada': 603,\n",
       " 'magistrados': 604,\n",
       " 'qualificaram': 605,\n",
       " 'corruptos': 606,\n",
       " 'comanda': 607,\n",
       " 'qualificou': 608,\n",
       " 'vergonha': 609,\n",
       " 'embora': 610,\n",
       " 'destinada': 611,\n",
       " 'crimes': 612,\n",
       " 'descaminho': 613,\n",
       " 'financeiros': 614,\n",
       " 'evasão': 615,\n",
       " 'divisas': 616,\n",
       " 'prática': 617,\n",
       " 'favorecer': 618,\n",
       " 'todo': 619,\n",
       " 'criminoso': 620,\n",
       " 'titular': 621,\n",
       " 'curitiba': 622,\n",
       " 'magistrado': 623,\n",
       " 'poderia': 624,\n",
       " 'contemplar': 625,\n",
       " 'fraudadores': 626,\n",
       " 'remeteram': 627,\n",
       " 'públicos': 628,\n",
       " 'paraísos': 629,\n",
       " 'corrupto': 630,\n",
       " 'internar': 631,\n",
       " 'declarando': 632,\n",
       " 'produto': 633,\n",
       " 'investigar': 634,\n",
       " 'discriminar': 635,\n",
       " 'origem': 636,\n",
       " 'desse': 637,\n",
       " 'naquela': 638,\n",
       " 'esse': 639,\n",
       " 'aposta': 640,\n",
       " 'aprovação': 641,\n",
       " 'reforçar': 642,\n",
       " 'orçamentoveja': 643,\n",
       " 'pontos': 644,\n",
       " 'polêmicos': 645,\n",
       " 'carreira': 646,\n",
       " 'investigadas': 647,\n",
       " 'ms': 648,\n",
       " 'ligado': 649,\n",
       " 'roberto': 650,\n",
       " 'costa': 651,\n",
       " 'pinho': 652,\n",
       " 'sacou': 653,\n",
       " 'conta': 654,\n",
       " 'agência': 655,\n",
       " 'smpb': 656,\n",
       " 'marcos': 657,\n",
       " 'valérioquando': 658,\n",
       " 'cpi': 659,\n",
       " 'correios': 660,\n",
       " 'costurou': 661,\n",
       " 'preservar': 662,\n",
       " 'fundos': 663,\n",
       " 'pensão': 664,\n",
       " 'quebra': 665,\n",
       " 'sigilo': 666,\n",
       " 'bancário': 667,\n",
       " 'fiscal': 668,\n",
       " 'mato': 669,\n",
       " 'grosso': 670,\n",
       " 'sul': 671,\n",
       " 'investigou': 672,\n",
       " 'envolvimento': 673,\n",
       " 'mensalão': 674,\n",
       " 'zeca': 675,\n",
       " 'esquerdista': 676,\n",
       " 'sinal': 677,\n",
       " 'homofobiaum': 678,\n",
       " 'texto': 679,\n",
       " 'jorge': 680,\n",
       " 'gauthier': 681,\n",
       " 'salte': 682,\n",
       " 'correio': 683,\n",
       " 'horas': 684,\n",
       " 'acha': 685,\n",
       " 'reclamando': 686,\n",
       " 'zagueiro': 687,\n",
       " 'deu': 688,\n",
       " 'atacante': 689,\n",
       " 'time': 690,\n",
       " 'paulista': 691,\n",
       " 'perdeu': 692,\n",
       " 'lance': 693,\n",
       " 'minutos': 694,\n",
       " 'vencia': 695,\n",
       " '×': 696,\n",
       " 'expulso': 697,\n",
       " '\\x93introduzir': 698,\n",
       " 'médio': 699,\n",
       " 'nádegas\\x94': 700,\n",
       " 'vitória\\x94gauthier': 701,\n",
       " 'estar': 702,\n",
       " 'espantado': 703,\n",
       " 'especialmente': 704,\n",
       " 'heteros': 705,\n",
       " 'tendo': 706,\n",
       " 'abordar': 707,\n",
       " 'assunto\\x94ele': 708,\n",
       " 'interessante': 709,\n",
       " 'heterossexualidade': 710,\n",
       " 'frágil': 711,\n",
       " 'botou': 712,\n",
       " 'dói': 713,\n",
       " 'vi': 714,\n",
       " 'vídeo': 715,\n",
       " 'dor': 716,\n",
       " 'gols': 717,\n",
       " 'garantiram': 718,\n",
       " 'triunfo': 719,\n",
       " 'vitória\\x94\\x93falar': 720,\n",
       " '\\x91botar': 721,\n",
       " 'senão': 722,\n",
       " 'gostar\\x92': 723,\n",
       " 'criatura': 724,\n",
       " 'sexualidade': 725,\n",
       " 'viva': 726,\n",
       " 'vida\\x94': 727,\n",
       " 'blogueiropara': 728,\n",
       " 'repulsa': 729,\n",
       " 'decada': 730,\n",
       " '\\x93corrobora': 731,\n",
       " 'machismo': 732,\n",
       " 'futebol\\x94': 733,\n",
       " '\\x93a': 734,\n",
       " 'necessidade': 735,\n",
       " 'masculina': 736,\n",
       " 'irritante': 737,\n",
       " 'desnecessária\\x94': 738,\n",
       " 'afirmaaté': 739,\n",
       " 'entendemos': 740,\n",
       " 'argumentação': 741,\n",
       " 'gays': 742,\n",
       " 'dão': 743,\n",
       " 'porém': 744,\n",
       " 'falamos': 745,\n",
       " 'sexos': 746,\n",
       " 'opostosé': 747,\n",
       " 'representante': 748,\n",
       " 'receberá': 749,\n",
       " 'dramaturgo': 750,\n",
       " 'martinez': 751,\n",
       " 'corrêa': 752,\n",
       " 'arquitetas': 753,\n",
       " 'discutir': 754,\n",
       " 'imbróglio': 755,\n",
       " 'quer': 756,\n",
       " 'lado': 757,\n",
       " 'companhia': 758,\n",
       " 'vereador': 759,\n",
       " 'eduardo': 760,\n",
       " 'suplicy': 761,\n",
       " 'pediu': 762,\n",
       " 'devem': 763,\n",
       " 'comparecer': 764,\n",
       " 'representantes': 765,\n",
       " 'gerou': 766,\n",
       " 'protestos': 767,\n",
       " 'ladotô': 768,\n",
       " 'dentroo': 769,\n",
       " 'entrando': 770,\n",
       " 'valer': 771,\n",
       " 'participou': 772,\n",
       " 'tombamento': 773,\n",
       " 'secretário': 774,\n",
       " 'cultura': 775,\n",
       " 'causa': 776,\n",
       " 'considera': 777,\n",
       " 'herói': 778,\n",
       " 'concerto': 779,\n",
       " 'apoio': 780,\n",
       " 'contar': 781,\n",
       " 'aumentar': 782,\n",
       " 'petistao': 783,\n",
       " 'radar': 784,\n",
       " 'online': 785,\n",
       " '\\x93em': 786,\n",
       " 'antônio': 787,\n",
       " 'confirmar': 788,\n",
       " 'pedido': 789,\n",
       " 'odebrecht': 790,\n",
       " 'pagasse': 791,\n",
       " 'ollanta': 792,\n",
       " 'humala': 793,\n",
       " 'atualmente': 794,\n",
       " 'preso\\x94só': 795,\n",
       " 'adicionais': 796,\n",
       " 'ordem': 797,\n",
       " 'diretamente': 798,\n",
       " 'elo': 799,\n",
       " 'ligação': 800,\n",
       " 'empreiteiraa': 801,\n",
       " 'temida': 802,\n",
       " 'panela': 803,\n",
       " 'explodir': 804,\n",
       " 'próximos': 805,\n",
       " 'acaba': 806,\n",
       " 'praticando': 807,\n",
       " 'atentado': 808,\n",
       " 'humanidadeo': 809,\n",
       " 'assinou': 810,\n",
       " 'ajuste': 811,\n",
       " 'moderna': 812,\n",
       " 'conforme': 813,\n",
       " 'ilispa': 814,\n",
       " 'documento': 815,\n",
       " 'permitiu': 816,\n",
       " 'arquivado': 817,\n",
       " 'inquérito': 818,\n",
       " 'investiga': 819,\n",
       " 'mamsp': 820,\n",
       " '\\x93performance\\x94': 821,\n",
       " 'la': 822,\n",
       " 'bête': 823,\n",
       " 'abertura': 824,\n",
       " 'panorama': 825,\n",
       " 'criança': 826,\n",
       " 'induzida': 827,\n",
       " 'mãe': 828,\n",
       " 'tocar': 829,\n",
       " 'corpo': 830,\n",
       " 'nu': 831,\n",
       " 'ganhou': 832,\n",
       " 'repercussão': 833,\n",
       " 'internacionalde': 834,\n",
       " 'impedir': 835,\n",
       " 'divulgação': 836,\n",
       " 'casos': 837,\n",
       " 'similares': 838,\n",
       " 'aconteçaisso': 839,\n",
       " 'grave': 840,\n",
       " 'históriano': 841,\n",
       " 'estágio': 842,\n",
       " 'evolução': 843,\n",
       " 'humanidade': 844,\n",
       " 'câmeras': 845,\n",
       " 'aumentam': 846,\n",
       " 'nossa': 847,\n",
       " 'segurança': 848,\n",
       " 'bandidos': 849,\n",
       " 'capturados': 850,\n",
       " 'imagens': 851,\n",
       " 'restringe': 852,\n",
       " 'pedofiliamas': 853,\n",
       " 'obrigar': 854,\n",
       " 'proibir': 855,\n",
       " 'definem': 856,\n",
       " 'seguro': 857,\n",
       " 'cidadãos': 858,\n",
       " 'terrível': 859,\n",
       " 'configurado': 860,\n",
       " 'humanidadeos': 861,\n",
       " 'deputados': 862,\n",
       " 'posicionar': 863,\n",
       " 'pau': 864,\n",
       " 'limites': 865,\n",
       " 'aciona': 866,\n",
       " 'procurou': 867,\n",
       " 'tentativa': 868,\n",
       " 'violar': 869,\n",
       " 'deseja': 870,\n",
       " 'erguer': 871,\n",
       " 'residenciais': 872,\n",
       " 'cem': 873,\n",
       " 'metros': 874,\n",
       " 'altura': 875,\n",
       " 'vejao': 876,\n",
       " 'empresário': 877,\n",
       " 'dono': 878,\n",
       " 'terrenos': 879,\n",
       " 'executa': 880,\n",
       " 'negandose': 881,\n",
       " 'trocálos': 882,\n",
       " 'oferta': 883,\n",
       " 'planejamentoo': 884,\n",
       " 'cerca': 885,\n",
       " 'sessenta': 886,\n",
       " 'ocupa': 887,\n",
       " 'prédio': 888,\n",
       " 'leva': 889,\n",
       " 'lina': 890,\n",
       " 'bo': 891,\n",
       " 'bardi': 892,\n",
       " 'masp': 893,\n",
       " 'sesc': 894,\n",
       " 'pompeia': 895,\n",
       " 'arquiteta': 896,\n",
       " 'prevê': 897,\n",
       " 'contracenação': 898,\n",
       " 'bairro': 899,\n",
       " 'bexiga': 900,\n",
       " 'janelão': 901,\n",
       " 'paredes': 902,\n",
       " 'edifício': 903,\n",
       " 'inclui': 904,\n",
       " 'parque': 905,\n",
       " 'cultural': 906,\n",
       " 'chegou': 907,\n",
       " 'morrido': 908,\n",
       " 'concluir': 909,\n",
       " 'propostaa': 910,\n",
       " 'procura': 911,\n",
       " 'estratégia': 912,\n",
       " 'desrespeito': 913,\n",
       " 'deixou': 914,\n",
       " 'provocações': 915,\n",
       " 'companhiaem': 916,\n",
       " 'outubro': 917,\n",
       " 'reverteu': 918,\n",
       " 'condephaat': 919,\n",
       " 'órgão': 920,\n",
       " 'proteção': 921,\n",
       " 'patrimônio': 922,\n",
       " 'impedia': 923,\n",
       " 'imobiliário': 924,\n",
       " 'aval': 925,\n",
       " 'níveis': 926,\n",
       " 'federalé': 927,\n",
       " 'pisou': 928,\n",
       " 'bola': 929,\n",
       " 'respeita': 930,\n",
       " 'haver': 931,\n",
       " 'vá': 932,\n",
       " 'pra': 933,\n",
       " 'briga': 934,\n",
       " 'pois': 935,\n",
       " 'proprietário': 936,\n",
       " 'enviado': 937,\n",
       " 'cobrando': 938,\n",
       " 'definir': 939,\n",
       " 'república': 940,\n",
       " 'negou': 941,\n",
       " 'racha': 942,\n",
       " 'postulam': 943,\n",
       " 'restrito': 944,\n",
       " 'universo': 945,\n",
       " 'amplia': 946,\n",
       " 'escuta': 947,\n",
       " 'erra': 948,\n",
       " 'acerta': 949,\n",
       " 'democracias': 950,\n",
       " 'partidos': 951,\n",
       " 'cartório': 952,\n",
       " 'vida': 953,\n",
       " 'democráticaquestionado': 954,\n",
       " 'possível': 955,\n",
       " 'desunião': 956,\n",
       " 'tucanos': 957,\n",
       " 'minimizou': 958,\n",
       " 'olha': 959,\n",
       " 'lembro': 960,\n",
       " 'matéria': 961,\n",
       " 'capital': 962,\n",
       " 'chama': 963,\n",
       " 'turno': 964,\n",
       " 'referência': 965,\n",
       " 'escreveu': 966,\n",
       " 'pedindo': 967,\n",
       " 'acabe': 968,\n",
       " 'dissemedisse': 969,\n",
       " 'questionado': 970,\n",
       " 'apreço': 971,\n",
       " 'completo': 972,\n",
       " 'pslpartido': 973,\n",
       " 'pouca': 974,\n",
       " 'vive': 975,\n",
       " 'momentos': 976,\n",
       " 'destaque': 977,\n",
       " 'inédito': 978,\n",
       " 'trajetória': 979,\n",
       " 'décadasno': 980,\n",
       " 'anunciou': 981,\n",
       " 'receberia': 982,\n",
       " 'précandidatura': 983,\n",
       " 'repúblicaa': 984,\n",
       " 'filiação': 985,\n",
       " 'consta': 986,\n",
       " 'divulgado': 987,\n",
       " 'luciano': 988,\n",
       " 'pe': 989,\n",
       " 'folha': 990,\n",
       " 'certo': 991,\n",
       " 'psc': 992,\n",
       " 'migrará': 993,\n",
       " 'marçoo': 994,\n",
       " 'colocado': 995,\n",
       " 'corrida': 996,\n",
       " 'datafolha': 997,\n",
       " 'lançou': 998,\n",
       " 'nanica': 999,\n",
       " 'provocou': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "0290bf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizando a sequencialização das bases de treinamento e teste\n",
    "\n",
    "training_sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "127841cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 13 news equal or longer than 150 words (will be truncated).\n",
      "There are 3 news shorter than 150 words (will be killed).\n"
     ]
    }
   ],
   "source": [
    "# Analisar notícias maiores do que max_length\n",
    "\n",
    "min_length = 150\n",
    "max_length = 150\n",
    "\n",
    "#plt.hist([len(x) for x in X], bins = 750)\n",
    "#plt.show()\n",
    "\n",
    "nos = np.array([len(x) for x in training_sentences])\n",
    "print(\"There are \"+ str(len(nos[nos>=max_length])) + \" news equal or longer than \"+ str(max_length) + \" words (will be truncated).\")\n",
    "\n",
    "nos = np.array([len(x) for x in training_sentences])\n",
    "print(\"There are \"+ str(len(nos[nos<min_length])) + \" news shorter than \"+ str(min_length) + \" words (will be killed).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "cebdd2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matar notícias menores do que min_length\n",
    "\n",
    "flag_dict = {}\n",
    "for i in range(0,len(training_sequences)):\n",
    "    if len(training_sequences[i]) < min_length:\n",
    "        flag_dict[i] = True\n",
    "    else:\n",
    "        flag_dict[i] = False\n",
    "        \n",
    "training_labels = [training_labels[i] for i in range(0,len(flag_dict)) if (flag_dict[i] == False)]\n",
    "\n",
    "training_sequences = [seq for seq in training_sequences if len(seq)>=min_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "0db3ec50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221\n",
      "275\n",
      "291\n",
      "194\n",
      "281\n",
      "197\n",
      "818\n",
      "245\n",
      "319\n",
      "217\n",
      "419\n",
      "177\n",
      "436\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(training_sequences)):    \n",
    "    print(len(training_sequences[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "11936835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truncar (padding) das sequências tokenizadas\n",
    "\n",
    "training_padded = pad_sequences(training_sequences,maxlen=max_length,padding='post',truncating='pre')\n",
    "#testing_padded = pad_sequences(testing_sequences,maxlen=max_length,padding='post',truncating='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "932623fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n",
      "150\n",
      "150\n",
      "150\n",
      "150\n",
      "150\n",
      "150\n",
      "150\n",
      "150\n",
      "150\n",
      "150\n",
      "150\n",
      "150\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(training_padded)):    \n",
    "    print(len(training_padded[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "64eb8aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = tokenizer.word_index\n",
    "inv_vocab = {v: k for k, v in vocab.items()} # reconverte o token para a palavra correpondente.\n",
    "\n",
    "training_texts = []\n",
    "testing_texts = []\n",
    "\n",
    "# Com estas listas \"_texts\", temos os textos novamente completos, mas agora com 'OOV's para palavras desconhecidas\n",
    "# na base de testes.\n",
    "for sequence in training_sequences:\n",
    "    news=[]\n",
    "    for token in sequence:\n",
    "        news.append(inv_vocab[token])  #inv_vocab[token] é a palavra correspondente ao token.\n",
    "    training_texts.append(news)\n",
    "\n",
    "\n",
    "for sequence in testing_sequences:\n",
    "    news=[]\n",
    "    for token in sequence:\n",
    "        news.append(inv_vocab[token])\n",
    "    testing_texts.append(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "c964c5ff",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['o',\n",
       " '<OOV>',\n",
       " 'decidiu',\n",
       " '<OOV>',\n",
       " 'o',\n",
       " 'deputado',\n",
       " 'federal',\n",
       " 'carlos',\n",
       " '<OOV>',\n",
       " 'do',\n",
       " 'partido',\n",
       " 'após',\n",
       " 'a',\n",
       " 'polícia',\n",
       " 'federal',\n",
       " 'fazer',\n",
       " '<OOV>',\n",
       " 'a',\n",
       " '<OOV>',\n",
       " 'no',\n",
       " '<OOV>',\n",
       " 'dele',\n",
       " 'na',\n",
       " 'câmara',\n",
       " 'com',\n",
       " 'isso',\n",
       " 'a',\n",
       " 'legenda',\n",
       " '<OOV>',\n",
       " 'espaço',\n",
       " 'para',\n",
       " 'receber',\n",
       " 'a',\n",
       " 'senadora',\n",
       " '<OOV>',\n",
       " 'pelo',\n",
       " 'pmdb',\n",
       " '<OOV>',\n",
       " 'abreu',\n",
       " 'por',\n",
       " 'meio',\n",
       " 'de',\n",
       " 'nota',\n",
       " 'a',\n",
       " 'legenda',\n",
       " 'informou',\n",
       " 'que',\n",
       " 'o',\n",
       " '<OOV>',\n",
       " 'do',\n",
       " '<OOV>',\n",
       " 'já',\n",
       " 'era',\n",
       " 'algo',\n",
       " '<OOV>',\n",
       " 'entre',\n",
       " 'os',\n",
       " '<OOV>',\n",
       " 'da',\n",
       " 'sigla',\n",
       " 'ainda',\n",
       " 'que',\n",
       " 'o',\n",
       " '<OOV>',\n",
       " 'tenha',\n",
       " '<OOV>',\n",
       " 'a',\n",
       " '<OOV>',\n",
       " 'de',\n",
       " 'sua',\n",
       " '<OOV>',\n",
       " 'para',\n",
       " '<OOV>',\n",
       " 'semana',\n",
       " 'diante',\n",
       " 'dos',\n",
       " '<OOV>',\n",
       " '<OOV>',\n",
       " 'hoje',\n",
       " 'a',\n",
       " '<OOV>',\n",
       " 'nacional',\n",
       " 'do',\n",
       " '<OOV>',\n",
       " '<OOV>',\n",
       " 'o',\n",
       " '<OOV>',\n",
       " '<OOV>',\n",
       " 'de',\n",
       " 'sua',\n",
       " 'filiação',\n",
       " 'dos',\n",
       " '<OOV>',\n",
       " 'do',\n",
       " '<OOV>',\n",
       " 'partido',\n",
       " 'que',\n",
       " 'no',\n",
       " 'passado',\n",
       " 'chegou',\n",
       " 'a',\n",
       " '<OOV>',\n",
       " '<OOV>',\n",
       " 'o',\n",
       " '<OOV>',\n",
       " 'como',\n",
       " 'candidato',\n",
       " 'ao',\n",
       " 'senado',\n",
       " 'diz',\n",
       " 'que',\n",
       " 'apoia',\n",
       " 'a',\n",
       " '<OOV>',\n",
       " 'com',\n",
       " 'a',\n",
       " '<OOV>',\n",
       " '<OOV>',\n",
       " 'dos',\n",
       " '<OOV>',\n",
       " 'crimes',\n",
       " '<OOV>',\n",
       " 'e',\n",
       " 'a',\n",
       " '<OOV>',\n",
       " '<OOV>',\n",
       " 'dos',\n",
       " '<OOV>',\n",
       " 'para',\n",
       " 'que',\n",
       " 'todos',\n",
       " '<OOV>',\n",
       " '<OOV>',\n",
       " 'com',\n",
       " 'o',\n",
       " '<OOV>',\n",
       " '<OOV>',\n",
       " 'da',\n",
       " 'lei',\n",
       " '<OOV>',\n",
       " 'de',\n",
       " '<OOV>',\n",
       " 'ou',\n",
       " 'cargo',\n",
       " '<OOV>']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_texts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a046fba",
   "metadata": {},
   "source": [
    "## Vectorization (gensim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fdf4699b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "dea46cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM = 100\n",
    "w2v_model = gensim.models.Word2Vec(sentences=training_sentences, vector_size=DIM, window=10, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "59bb1d70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('que', 0.7961148023605347),\n",
       " ('ao', 0.782345175743103),\n",
       " ('em', 0.78111732006073),\n",
       " ('de', 0.775870680809021),\n",
       " ('o', 0.7723456621170044),\n",
       " ('não', 0.7696959376335144),\n",
       " ('a', 0.7681581377983093),\n",
       " ('com', 0.7658106684684753),\n",
       " ('do', 0.7648646831512451),\n",
       " ('da', 0.762366771697998)]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar('bolsonaro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "42f2c569",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.08384872e-02,  2.24295957e-03,  1.08779222e-02,  1.09694432e-03,\n",
       "        1.20860373e-03, -2.32031234e-02, -4.74540470e-03,  1.38203548e-02,\n",
       "       -2.57660099e-03, -1.30627202e-02,  3.52965028e-04, -1.96763892e-02,\n",
       "        1.50028232e-03,  1.05501935e-02,  3.20692034e-03, -1.27249239e-02,\n",
       "        7.42186233e-03, -6.39519375e-03,  4.31142002e-03, -1.53123429e-02,\n",
       "        3.92288761e-03,  4.21803631e-03,  1.62784960e-02, -1.50171816e-02,\n",
       "        5.38172154e-03,  2.06759153e-03, -1.24571305e-02,  4.12423583e-03,\n",
       "       -1.06467372e-02,  1.27509721e-02,  1.84684712e-02,  5.60031086e-03,\n",
       "        7.98126496e-03, -1.55025041e-02, -2.18587019e-03,  1.66077744e-02,\n",
       "        5.84457535e-03, -1.15320152e-02, -4.37451852e-03, -1.58331171e-02,\n",
       "        6.17942214e-03, -1.74000263e-02,  3.41450330e-03,  4.83150827e-03,\n",
       "        1.70688871e-02,  5.87627510e-05, -1.32182920e-02, -1.47889473e-03,\n",
       "        5.61891869e-03, -3.84384044e-03,  1.32964216e-02,  8.59728723e-04,\n",
       "        6.30329596e-04, -6.33669924e-03, -1.97088309e-02,  8.31010565e-03,\n",
       "        6.35670219e-03,  7.72832474e-03, -4.98213107e-03,  5.01682935e-03,\n",
       "        7.47476937e-03,  2.91154021e-03, -2.62217061e-03,  2.20193947e-03,\n",
       "       -3.16164270e-03,  7.64468545e-03,  5.17889205e-03,  5.83239179e-03,\n",
       "       -1.17460638e-02,  1.83852818e-02, -9.10757668e-03,  7.24288356e-03,\n",
       "        1.17203761e-02,  2.32881028e-03,  1.70348529e-02,  8.24904907e-03,\n",
       "       -3.70119291e-04,  5.70393517e-04,  7.65961086e-05,  1.17626302e-02,\n",
       "       -5.28609334e-03, -6.05874695e-03, -9.47525073e-03,  8.80719163e-03,\n",
       "       -4.23263153e-03,  3.20742372e-03, -5.13277133e-04,  1.84478180e-03,\n",
       "        1.89478844e-02,  1.20365070e-02,  2.00022925e-02,  8.13180488e-03,\n",
       "       -1.57673762e-03, -1.17778201e-02,  2.19427664e-02, -2.03648512e-03,\n",
       "        3.59528069e-03, -6.02673925e-03,  2.65855691e-03, -1.28063168e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv['bolsonaro']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "c18e1af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index)+1\n",
    "\n",
    "def get_weight_matrix(model):\n",
    "    ''' Inserts every word vector in a NumPy array, ordering them by token icon (which is determined by their appearing frequencies). '''\n",
    "    weight_matrix = np.zeros((vocab_size, DIM))\n",
    "\n",
    "    for word, token in vocab.items():\n",
    "        weight_matrix[token] = model.wv[word]\n",
    "\n",
    "    return weight_matrix\n",
    "\n",
    "embedding_vectors = get_weight_matrix(model=w2v_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bccc60",
   "metadata": {},
   "source": [
    "## Criação da rede neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "9355159d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, Conv1D, MaxPool1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "a75b4f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 150, 100)          170500    \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 128)               117248    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 287,877\n",
      "Trainable params: 117,377\n",
      "Non-trainable params: 170,500\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, output_dim=DIM, weights = [embedding_vectors], input_length=max_length, trainable=False))\n",
    "model.add(LSTM(units=128))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "13803401",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray(training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "464c8fb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "624b069b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[array([[   3],\n",
       "         [   0],\n",
       "         [1591],\n",
       "         [   0],\n",
       "         [   3],\n",
       "         [ 421],\n",
       "         [  34],\n",
       "         [ 349],\n",
       "         [   0],\n",
       "         [   5],\n",
       "         [  32],\n",
       "         [  66],\n",
       "         [   4],\n",
       "         [ 274],\n",
       "         [  34],\n",
       "         [ 135],\n",
       "         [   0],\n",
       "         [   4],\n",
       "         [   0],\n",
       "         [  11],\n",
       "         [   0],\n",
       "         [ 434],\n",
       "         [  14],\n",
       "         [ 239],\n",
       "         [  18],\n",
       "         [  60],\n",
       "         [   4],\n",
       "         [ 406],\n",
       "         [   0],\n",
       "         [ 502],\n",
       "         [   9],\n",
       "         [ 342],\n",
       "         [   4],\n",
       "         [1534],\n",
       "         [   0],\n",
       "         [  30],\n",
       "         [ 159],\n",
       "         [   0],\n",
       "         [ 260],\n",
       "         [  19],\n",
       "         [1279],\n",
       "         [   1],\n",
       "         [ 533],\n",
       "         [   4],\n",
       "         [ 406],\n",
       "         [ 528],\n",
       "         [   2],\n",
       "         [   3],\n",
       "         [   0],\n",
       "         [   5],\n",
       "         [   0],\n",
       "         [  55],\n",
       "         [ 120],\n",
       "         [ 206],\n",
       "         [   0],\n",
       "         [ 196],\n",
       "         [  25],\n",
       "         [   0],\n",
       "         [   7],\n",
       "         [ 419],\n",
       "         [  64],\n",
       "         [   2],\n",
       "         [   3],\n",
       "         [   0],\n",
       "         [ 204],\n",
       "         [   0],\n",
       "         [   4],\n",
       "         [   0],\n",
       "         [   1],\n",
       "         [  24],\n",
       "         [   0],\n",
       "         [   9],\n",
       "         [   0],\n",
       "         [ 272],\n",
       "         [1091],\n",
       "         [  26],\n",
       "         [   0],\n",
       "         [   0],\n",
       "         [ 424],\n",
       "         [   4],\n",
       "         [   0],\n",
       "         [ 150],\n",
       "         [   5],\n",
       "         [   0],\n",
       "         [   0],\n",
       "         [   3],\n",
       "         [   0],\n",
       "         [   0],\n",
       "         [   1],\n",
       "         [  24],\n",
       "         [ 985],\n",
       "         [  26],\n",
       "         [   0],\n",
       "         [   5],\n",
       "         [   0],\n",
       "         [  32],\n",
       "         [   2],\n",
       "         [  11],\n",
       "         [ 229],\n",
       "         [ 907],\n",
       "         [   4],\n",
       "         [   0],\n",
       "         [   0],\n",
       "         [   3],\n",
       "         [   0],\n",
       "         [  22],\n",
       "         [  76],\n",
       "         [  16],\n",
       "         [1315],\n",
       "         [  38],\n",
       "         [   2],\n",
       "         [1046],\n",
       "         [   4],\n",
       "         [   0],\n",
       "         [  18],\n",
       "         [   4],\n",
       "         [   0],\n",
       "         [   0],\n",
       "         [  26],\n",
       "         [   0],\n",
       "         [ 612],\n",
       "         [   0],\n",
       "         [   6],\n",
       "         [   4],\n",
       "         [   0],\n",
       "         [   0],\n",
       "         [  26],\n",
       "         [   0],\n",
       "         [   9],\n",
       "         [   2],\n",
       "         [1563],\n",
       "         [   0],\n",
       "         [   0],\n",
       "         [  18],\n",
       "         [   3],\n",
       "         [   0],\n",
       "         [   0],\n",
       "         [   7],\n",
       "         [  92],\n",
       "         [   0],\n",
       "         [   1],\n",
       "         [   0],\n",
       "         [  49],\n",
       "         [ 232],\n",
       "         [   0]])],\n",
       " [array([[ 424],\n",
       "         [   0],\n",
       "         [  12],\n",
       "         [  84],\n",
       "         [   7],\n",
       "         [ 241],\n",
       "         [   1],\n",
       "         [  12],\n",
       "         [  26],\n",
       "         [   0],\n",
       "         [  75],\n",
       "         [   0],\n",
       "         [   7],\n",
       "         [ 120],\n",
       "         [ 102],\n",
       "         [ 111],\n",
       "         [   0],\n",
       "         [1336],\n",
       "         [ 734],\n",
       "         [ 241],\n",
       "         [   5],\n",
       "         [   0],\n",
       "         [   1],\n",
       "         [   0],\n",
       "         [   0],\n",
       "         [   1],\n",
       "         [   0],\n",
       "         [   8],\n",
       "         [ 109],\n",
       "         [  41],\n",
       "         [   0],\n",
       "         [  13],\n",
       "         [   0],\n",
       "         [  75],\n",
       "         [   0],\n",
       "         [  11],\n",
       "         [ 453],\n",
       "         [   0],\n",
       "         [   0],\n",
       "         [   0],\n",
       "         [1383],\n",
       "         [1304],\n",
       "         [ 336],\n",
       "         [   0],\n",
       "         [  22],\n",
       "         [   0],\n",
       "         [   0],\n",
       "         [  49],\n",
       "         [ 206],\n",
       "         [   5],\n",
       "         [   0],\n",
       "         [   0],\n",
       "         [ 734],\n",
       "         [ 241],\n",
       "         [   5],\n",
       "         [   0],\n",
       "         [   0],\n",
       "         [ 336],\n",
       "         [   9],\n",
       "         [  13],\n",
       "         [   0],\n",
       "         [   0],\n",
       "         [   3],\n",
       "         [  62],\n",
       "         [   8],\n",
       "         [   2],\n",
       "         [ 111],\n",
       "         [ 508],\n",
       "         [   0],\n",
       "         [   8],\n",
       "         [   1],\n",
       "         [ 258],\n",
       "         [   1],\n",
       "         [ 224],\n",
       "         [   0],\n",
       "         [   0],\n",
       "         [  38],\n",
       "         [ 961],\n",
       "         [   5],\n",
       "         [   0],\n",
       "         [   4],\n",
       "         [   0],\n",
       "         [   1],\n",
       "         [  84],\n",
       "         [   7],\n",
       "         [ 241],\n",
       "         [   5],\n",
       "         [   0],\n",
       "         [   8],\n",
       "         [1512],\n",
       "         [  10],\n",
       "         [ 309],\n",
       "         [1076],\n",
       "         [   0],\n",
       "         [   0],\n",
       "         [   0],\n",
       "         [1171],\n",
       "         [   0],\n",
       "         [ 567],\n",
       "         [   0],\n",
       "         [   4],\n",
       "         [ 111],\n",
       "         [  48],\n",
       "         [  17],\n",
       "         [   0],\n",
       "         [   2],\n",
       "         [   4],\n",
       "         [   0],\n",
       "         [   5],\n",
       "         [ 242],\n",
       "         [   0],\n",
       "         [  19],\n",
       "         [   0],\n",
       "         [   0],\n",
       "         [ 111],\n",
       "         [   0],\n",
       "         [  75],\n",
       "         [   1],\n",
       "         [1431],\n",
       "         [   0],\n",
       "         [   3],\n",
       "         [  83],\n",
       "         [   3],\n",
       "         [   0],\n",
       "         [   7],\n",
       "         [   0],\n",
       "         [   1],\n",
       "         [   0],\n",
       "         [  64],\n",
       "         [ 424],\n",
       "         [  31],\n",
       "         [1427],\n",
       "         [   0],\n",
       "         [ 508],\n",
       "         [  17],\n",
       "         [ 878],\n",
       "         [   1],\n",
       "         [ 619],\n",
       "         [   3],\n",
       "         [ 242],\n",
       "         [  22],\n",
       "         [  23],\n",
       "         [ 375],\n",
       "         [  12],\n",
       "         [  91],\n",
       "         [   0],\n",
       "         [   1],\n",
       "         [   0],\n",
       "         [   3],\n",
       "         [  75],\n",
       "         [   0],\n",
       "         [   1],\n",
       "         [   0],\n",
       "         [  17],\n",
       "         [  22],\n",
       "         [   0],\n",
       "         [ 335],\n",
       "         [  64],\n",
       "         [   0],\n",
       "         [ 639],\n",
       "         [   0],\n",
       "         [1375],\n",
       "         [  64],\n",
       "         [   8],\n",
       "         [  39],\n",
       "         [  60],\n",
       "         [  17],\n",
       "         [  64],\n",
       "         [  75],\n",
       "         [   0]])],\n",
       " [array([[ 299],\n",
       "         [  71],\n",
       "         [ 991],\n",
       "         [  16],\n",
       "         [  10],\n",
       "         [  23],\n",
       "         [   0],\n",
       "         [   1],\n",
       "         [ 836],\n",
       "         [   1],\n",
       "         [   0],\n",
       "         [   1],\n",
       "         [ 356],\n",
       "         [   6],\n",
       "         [ 238],\n",
       "         [  29],\n",
       "         [   0],\n",
       "         [  12],\n",
       "         [   0],\n",
       "         [   0],\n",
       "         [ 248],\n",
       "         [   0],\n",
       "         [   3],\n",
       "         [   0],\n",
       "         [   0],\n",
       "         [  16],\n",
       "         [   0],\n",
       "         [   2],\n",
       "         [   0],\n",
       "         [  18],\n",
       "         [   4],\n",
       "         [   0],\n",
       "         [   5],\n",
       "         [ 121],\n",
       "         [  34],\n",
       "         [ 115],\n",
       "         [   0],\n",
       "         [   0],\n",
       "         [  29],\n",
       "         [  15],\n",
       "         [  15],\n",
       "         [   0],\n",
       "         [  63],\n",
       "         [   4],\n",
       "         [ 836],\n",
       "         [   5],\n",
       "         [   0],\n",
       "         [ 196],\n",
       "         [ 356],\n",
       "         [   6],\n",
       "         [ 238],\n",
       "         [   8],\n",
       "         [ 576],\n",
       "         [   5],\n",
       "         [  84],\n",
       "         [ 229],\n",
       "         [1346],\n",
       "         [  29],\n",
       "         [   0],\n",
       "         [   0],\n",
       "         [ 190],\n",
       "         [   0],\n",
       "         [   1],\n",
       "         [1099],\n",
       "         [ 465],\n",
       "         [ 610],\n",
       "         [ 204],\n",
       "         [   0],\n",
       "         [   0],\n",
       "         [  18],\n",
       "         [   4],\n",
       "         [   0],\n",
       "         [   2],\n",
       "         [   4],\n",
       "         [ 836],\n",
       "         [   0],\n",
       "         [1542],\n",
       "         [   0],\n",
       "         [ 148],\n",
       "         [1190],\n",
       "         [   3],\n",
       "         [   2],\n",
       "         [   4],\n",
       "         [  92],\n",
       "         [   0],\n",
       "         [   6],\n",
       "         [   3],\n",
       "         [   2],\n",
       "         [ 148],\n",
       "         [   0],\n",
       "         [   2],\n",
       "         [ 120],\n",
       "         [   0],\n",
       "         [  37],\n",
       "         [   0],\n",
       "         [   0],\n",
       "         [   0],\n",
       "         [   0],\n",
       "         [   0],\n",
       "         [  37],\n",
       "         [   0],\n",
       "         [   0],\n",
       "         [  16],\n",
       "         [ 142],\n",
       "         [   0],\n",
       "         [   0],\n",
       "         [   0],\n",
       "         [  26],\n",
       "         [   0],\n",
       "         [   0],\n",
       "         [  26],\n",
       "         [   0],\n",
       "         [   0],\n",
       "         [   0],\n",
       "         [1265],\n",
       "         [  10],\n",
       "         [  23],\n",
       "         [   0],\n",
       "         [  14],\n",
       "         [ 308],\n",
       "         [ 356],\n",
       "         [   6],\n",
       "         [ 238],\n",
       "         [   0],\n",
       "         [  16],\n",
       "         [   0],\n",
       "         [  63],\n",
       "         [   4],\n",
       "         [   0],\n",
       "         [   5],\n",
       "         [ 214],\n",
       "         [  16],\n",
       "         [  56],\n",
       "         [   7],\n",
       "         [1289],\n",
       "         [   0],\n",
       "         [1004],\n",
       "         [1422],\n",
       "         [  75],\n",
       "         [   5],\n",
       "         [   2],\n",
       "         [   0],\n",
       "         [ 238],\n",
       "         [   3],\n",
       "         [   0],\n",
       "         [   9],\n",
       "         [   0],\n",
       "         [   0],\n",
       "         [1066],\n",
       "         [   0],\n",
       "         [   8],\n",
       "         [   0],\n",
       "         [   0],\n",
       "         [   7],\n",
       "         [   0],\n",
       "         [   4],\n",
       "         [ 509],\n",
       "         [   7],\n",
       "         [   0],\n",
       "         [   0],\n",
       "         [   0],\n",
       "         [   2],\n",
       "         [  23],\n",
       "         [   0],\n",
       "         [   3],\n",
       "         [1357],\n",
       "         [   9],\n",
       "         [1605],\n",
       "         [   0],\n",
       "         [   3],\n",
       "         [  42],\n",
       "         [   0],\n",
       "         [   1],\n",
       "         [ 238],\n",
       "         [   0],\n",
       "         [ 509],\n",
       "         [ 637],\n",
       "         [  62],\n",
       "         [ 115],\n",
       "         [ 299],\n",
       "         [  23],\n",
       "         [   0],\n",
       "         [  12],\n",
       "         [ 778],\n",
       "         [ 150]])],\n",
       " [array([[   8],\n",
       "         [   0],\n",
       "         [   0],\n",
       "         [ 144],\n",
       "         [ 385],\n",
       "         [   9],\n",
       "         [   0],\n",
       "         [   3],\n",
       "         [ 322],\n",
       "         [  35],\n",
       "         [   0],\n",
       "         [  12],\n",
       "         [   0],\n",
       "         [   0],\n",
       "         [   2],\n",
       "         [   0],\n",
       "         [   0],\n",
       "         [ 144],\n",
       "         [   0],\n",
       "         [  11],\n",
       "         [   0],\n",
       "         [   5],\n",
       "         [   0],\n",
       "         [  11],\n",
       "         [ 268],\n",
       "         [   1],\n",
       "         [ 175],\n",
       "         [   0],\n",
       "         [  15],\n",
       "         [ 970],\n",
       "         [  63],\n",
       "         [   3],\n",
       "         [  84],\n",
       "         [   5],\n",
       "         [   0],\n",
       "         [   6],\n",
       "         [   3],\n",
       "         [   0],\n",
       "         [   5],\n",
       "         [1198],\n",
       "         [  10],\n",
       "         [   0],\n",
       "         [ 415],\n",
       "         [  20],\n",
       "         [   0],\n",
       "         [   5],\n",
       "         [   0],\n",
       "         [1260],\n",
       "         [  17],\n",
       "         [   2],\n",
       "         [ 333],\n",
       "         [   0],\n",
       "         [   2],\n",
       "         [ 122],\n",
       "         [ 228],\n",
       "         [  10],\n",
       "         [   0],\n",
       "         [   0],\n",
       "         [ 352],\n",
       "         [   5],\n",
       "         [   0],\n",
       "         [ 329],\n",
       "         [   2],\n",
       "         [  17],\n",
       "         [  13],\n",
       "         [   0],\n",
       "         [   0],\n",
       "         [   0],\n",
       "         [  60],\n",
       "         [  67],\n",
       "         [  81],\n",
       "         [ 180],\n",
       "         [   0],\n",
       "         [  10],\n",
       "         [  17],\n",
       "         [1224],\n",
       "         [  73],\n",
       "         [ 599],\n",
       "         [   0],\n",
       "         [ 335],\n",
       "         [  64],\n",
       "         [  75],\n",
       "         [  11],\n",
       "         [1279],\n",
       "         [   1],\n",
       "         [   0],\n",
       "         [  18],\n",
       "         [   0],\n",
       "         [   0],\n",
       "         [   0],\n",
       "         [   4],\n",
       "         [   0],\n",
       "         [  10],\n",
       "         [   0],\n",
       "         [1649],\n",
       "         [   1],\n",
       "         [   0],\n",
       "         [   0],\n",
       "         [  12],\n",
       "         [ 513],\n",
       "         [ 336],\n",
       "         [   1],\n",
       "         [1649],\n",
       "         [  26],\n",
       "         [   0],\n",
       "         [   0],\n",
       "         [ 704],\n",
       "         [  25],\n",
       "         [   2],\n",
       "         [   0],\n",
       "         [   5],\n",
       "         [ 315],\n",
       "         [ 294],\n",
       "         [   3],\n",
       "         [ 315],\n",
       "         [  17],\n",
       "         [  60],\n",
       "         [ 226],\n",
       "         [   0],\n",
       "         [   0],\n",
       "         [   8],\n",
       "         [1468],\n",
       "         [   6],\n",
       "         [   0],\n",
       "         [ 194],\n",
       "         [   0],\n",
       "         [1123],\n",
       "         [  25],\n",
       "         [   0],\n",
       "         [   0],\n",
       "         [ 194],\n",
       "         [  73],\n",
       "         [  36],\n",
       "         [   0],\n",
       "         [   1],\n",
       "         [   0],\n",
       "         [   2],\n",
       "         [  79],\n",
       "         [  11],\n",
       "         [ 294],\n",
       "         [  17],\n",
       "         [1089],\n",
       "         [   0],\n",
       "         [  22],\n",
       "         [ 133],\n",
       "         [  79],\n",
       "         [   3],\n",
       "         [   0],\n",
       "         [   0],\n",
       "         [   1],\n",
       "         [   0],\n",
       "         [   0],\n",
       "         [   6],\n",
       "         [   0],\n",
       "         [  10],\n",
       "         [  15],\n",
       "         [  60],\n",
       "         [   2],\n",
       "         [   0],\n",
       "         [  18],\n",
       "         [ 333],\n",
       "         [   0],\n",
       "         [   2],\n",
       "         [   3],\n",
       "         [   0],\n",
       "         [   0],\n",
       "         [   0],\n",
       "         [  47],\n",
       "         [   2],\n",
       "         [   4],\n",
       "         [   0],\n",
       "         [   0],\n",
       "         [  23],\n",
       "         [   0],\n",
       "         [   8],\n",
       "         [ 599],\n",
       "         [   0],\n",
       "         [   0],\n",
       "         [  18],\n",
       "         [   3],\n",
       "         [1198],\n",
       "         [  14],\n",
       "         [   0],\n",
       "         [ 442],\n",
       "         [   7],\n",
       "         [   0],\n",
       "         [  67],\n",
       "         [  23],\n",
       "         [   0],\n",
       "         [   4],\n",
       "         [   0],\n",
       "         [ 259],\n",
       "         [   1],\n",
       "         [   0],\n",
       "         [   1],\n",
       "         [ 333],\n",
       "         [   0],\n",
       "         [   0],\n",
       "         [   0],\n",
       "         [  47],\n",
       "         [   3],\n",
       "         [   2],\n",
       "         [ 428],\n",
       "         [   4],\n",
       "         [   0],\n",
       "         [   5],\n",
       "         [   0],\n",
       "         [   4],\n",
       "         [ 499],\n",
       "         [   5],\n",
       "         [1198],\n",
       "         [  37],\n",
       "         [  17],\n",
       "         [1089],\n",
       "         [  27],\n",
       "         [ 471],\n",
       "         [   0],\n",
       "         [   2],\n",
       "         [   3],\n",
       "         [   0],\n",
       "         [  17],\n",
       "         [  12],\n",
       "         [1198],\n",
       "         [  67],\n",
       "         [   0],\n",
       "         [  26],\n",
       "         [   0],\n",
       "         [ 413],\n",
       "         [1346],\n",
       "         [  80],\n",
       "         [  29],\n",
       "         [  17],\n",
       "         [   2],\n",
       "         [   3],\n",
       "         [  84],\n",
       "         [1490],\n",
       "         [   1],\n",
       "         [   0],\n",
       "         [   0],\n",
       "         [  16],\n",
       "         [   0],\n",
       "         [  17],\n",
       "         [   0],\n",
       "         [   2],\n",
       "         [   3],\n",
       "         [1004],\n",
       "         [   0],\n",
       "         [ 120],\n",
       "         [  41],\n",
       "         [  13],\n",
       "         [ 480],\n",
       "         [1174],\n",
       "         [  14],\n",
       "         [   0],\n",
       "         [   3],\n",
       "         [   2],\n",
       "         [  10],\n",
       "         [   0],\n",
       "         [   0],\n",
       "         [1606],\n",
       "         [   0],\n",
       "         [  14],\n",
       "         [   0],\n",
       "         [ 907],\n",
       "         [   4],\n",
       "         [ 325],\n",
       "         [   0],\n",
       "         [   0],\n",
       "         [   2],\n",
       "         [  30],\n",
       "         [   0],\n",
       "         [ 120],\n",
       "         [   0],\n",
       "         [   2],\n",
       "         [   0],\n",
       "         [   0],\n",
       "         [  48],\n",
       "         [   0],\n",
       "         [  12],\n",
       "         [   0],\n",
       "         [   0],\n",
       "         [   1],\n",
       "         [  10],\n",
       "         [  41],\n",
       "         [   0],\n",
       "         [  13],\n",
       "         [   0],\n",
       "         [   0]])]]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "f82c4e51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(_padded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "37358e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 0s 172ms/step - loss: 0.6103 - acc: 0.8889 - val_loss: 0.7599 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.5960 - acc: 0.7778 - val_loss: 0.8163 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.5860 - acc: 0.5556 - val_loss: 0.8080 - val_acc: 0.2500\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 326ms/step - loss: 0.5598 - acc: 1.0000 - val_loss: 0.9239 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.5481 - acc: 0.7778 - val_loss: 0.9079 - val_acc: 0.2500\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.5545 - acc: 0.7778 - val_loss: 1.0606 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.7787 - acc: 0.5556 - val_loss: 0.9439 - val_acc: 0.2500\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.5312 - acc: 0.8889 - val_loss: 0.8725 - val_acc: 0.2500\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.5095 - acc: 0.8889 - val_loss: 1.0263 - val_acc: 0.5000\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 0.6103 - acc: 0.5556 - val_loss: 0.8966 - val_acc: 0.5000\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Data cardinality is ambiguous:\n  x sizes: 145, 170, 185, 288\nMake sure all arrays contain the same number of samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-214-204b3c6f09ae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtesting_list\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtesting_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m_check_data_cardinality\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m   1653\u001b[0m                            for i in tf.nest.flatten(single_data)))\n\u001b[0;32m   1654\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m\"Make sure all arrays contain the same number of samples.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1655\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1656\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1657\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 145, 170, 185, 288\nMake sure all arrays contain the same number of samples."
     ]
    }
   ],
   "source": [
    "model.fit(training_padded, np.asarray(training_labels), validation_split=0.3, epochs=10)\n",
    "\n",
    "testing_list = []\n",
    "\n",
    "for test_news in testing_sentences:\n",
    "    aux_list = []\n",
    "    sequences = tokenizer.texts_to_sequences(test_news)\n",
    "    padded = pad_sequences(sequences)\n",
    "    aux_list.append(np.asarray(padded))\n",
    "    testing_list.append(aux_list)\n",
    "\n",
    "    \n",
    "y_pred = (model.predict(testing_list) >=0.5).astype(int)\n",
    "\n",
    "accuracy_score(testing_labels, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "b1b41c5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(testing_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
