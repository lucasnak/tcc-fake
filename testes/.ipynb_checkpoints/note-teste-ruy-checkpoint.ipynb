{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16c0a2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import unicodedata\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e181ce0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a05dd0a",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'stopwords.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-a0d2c03e87ff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'stopwords.txt'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mstopwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m', '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mstemmer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSnowballStemmer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'portuguese'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'stopwords.txt'"
     ]
    }
   ],
   "source": [
    "with open('stopwords.txt') as f:\n",
    "    stopwords = f.read().split(', ')\n",
    "\n",
    "stemmer = nltk.stem.SnowballStemmer('portuguese')\n",
    "corpus = []\n",
    "\n",
    "with open('true1.txt',encoding='utf8') as f:\n",
    "    sentences = nltk.sent_tokenize(f.read())\n",
    "    for i in range(len(sentences)):\n",
    "        review = re.sub('[^a-zA-Z]',' ',sentences[i])\n",
    "        review = review.lower()\n",
    "        review = review.split()\n",
    "        review = [stemmer.stem(word) for word in review if not word in set(stopwords)]\n",
    "        review = ' '.join(review)\n",
    "        corpus.append(review)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6c005ca6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 77 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0   1   2   3   4   5   6   7   8   9   ...  67  68  69  70  71  72  73  \\\n",
       "0   0   0   0   0   0   0   0   1   0   1  ...   0   0   0   0   0   0   0   \n",
       "1   1   1   0   0   0   0   0   0   0   0  ...   1   0   0   0   0   1   0   \n",
       "2   0   0   1   1   0   1   0   0   0   0  ...   0   0   0   0   0   0   1   \n",
       "3   0   0   0   0   1   0   0   0   0   0  ...   0   0   0   1   0   0   0   \n",
       "4   0   0   0   0   0   0   1   0   1   0  ...   0   1   1   0   1   0   0   \n",
       "\n",
       "   74  75  76  \n",
       "0   0   0   0  \n",
       "1   0   0   0  \n",
       "2   0   0   0  \n",
       "3   1   0   0  \n",
       "4   0   1   1  \n",
       "\n",
       "[5 rows x 77 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "\n",
    "pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "327ac2d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('pt_core_news_sm')\n",
    "\n",
    "with open(r'C:\\Users\\nakam\\Documents\\Python Scripts\\Fake\\var\\stopwords.txt') as f:\n",
    "    stopwords = f.read().split(', ')\n",
    "\n",
    "corpus = []\n",
    "\n",
    "with open('fake1.txt',encoding='utf8') as f:\n",
    "    text = f.read()\n",
    "    doc = nlp(text)\n",
    "    sentences = nltk.sent_tokenize(f.read())\n",
    "    for i in range(len(sentences)):\n",
    "        review = re.sub('[^a-zA-Z]',' ',sentences[i])\n",
    "        review = review.lower()\n",
    "        review = review.split()\n",
    "        review = [token.lemma_ for token in nlp(review) if not token in set(stopwords)]\n",
    "        review = ' '.join(review)\n",
    "        corpus.append(review)\n",
    "corpus\n",
    "\n",
    "\n",
    "\n",
    "#for token in doc:\n",
    "#    print(token, token.lemma, token.lemma_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "624dcc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('pt_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "763b2a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=[]\n",
    "\n",
    "with open('fake1.txt',encoding='utf8') as f:\n",
    "    texto = f.read()\n",
    "    texto = nltk.sent_tokenize(texto)\n",
    "    \n",
    "    for sentence in texto:\n",
    "        doc = nlp(sentence)\n",
    "        review = [token.lemma_ for token in doc if token.is_stop is False]\n",
    "        #review = ' '.join(review)\n",
    "        corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "68128e1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Kátia', 'Abreu', 'colocar', 'expulsão', 'moldurar', ',', 'reclamar', '.'],\n",
       " ['senador',\n",
       "  'Kátia',\n",
       "  'Abreu',\n",
       "  '(',\n",
       "  'partido-TO',\n",
       "  ')',\n",
       "  'dizer',\n",
       "  'expulsão',\n",
       "  'PMDB',\n",
       "  'resultar',\n",
       "  'ação',\n",
       "  'cúpula',\n",
       "  'atual',\n",
       "  'legendar',\n",
       "  ',',\n",
       "  ',',\n",
       "  'oportunista',\n",
       "  '.'],\n",
       " ['“',\n",
       "  'Amanhã',\n",
       "  'ir',\n",
       "  'botar',\n",
       "  'moldurar',\n",
       "  'dourar',\n",
       "  'expulsão',\n",
       "  ',',\n",
       "  'mão',\n",
       "  'vir',\n",
       "  ',',\n",
       "  'atestar',\n",
       "  'conduta',\n",
       "  'currículo',\n",
       "  '.'],\n",
       " ['pessoa', 'expulsar', 'servir', 'país', '.'],\n",
       " ['servir',\n",
       "  'país',\n",
       "  'benefício',\n",
       "  'próprio',\n",
       "  '”',\n",
       "  ',',\n",
       "  'dizer',\n",
       "  'Kátia',\n",
       "  'Abreu',\n",
       "  '.'],\n",
       " ['Ué', ',', 'expulsão', 'currículo', ',', 'choradeira', ',', 'Kátia', '?'],\n",
       " ['Sabemos', 'motivar', '.'],\n",
       " ['Provavelmente',\n",
       "  'Kátia',\n",
       "  'PT',\n",
       "  ',',\n",
       "  'partir',\n",
       "  'dever',\n",
       "  'tê-la',\n",
       "  'absorver',\n",
       "  '.'],\n",
       " ['PT', 'gostar', 'Kátia', 'ficar', 'entrincheirar', 'PMDB', '.'],\n",
       " ['ser', ',', 'rebaixar', '.'],\n",
       " ['Resta', 'Kátia', 'ficar', 'chorar', 'pitanga', 'canto', '.'],\n",
       " [':', 'PT', 'cadastrar', 'Kátia', 'Abreu', 'fileira', '.'],\n",
       " ['situação', 'patético', 'ex-ministra', 'Agricultura', 'Dilma', '.']]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6241a9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'C:\\Users\\nakam\\Documents\\Python Scripts\\Fake\\var\\stopwords.txt') as f:\n",
    "    stopwords = f.read().split(', ')\n",
    "\n",
    "# def normalize(comment, lowercase, remove_stopwords):\n",
    "#     if lowercase:\n",
    "#         comment = comment.lower()\n",
    "#     comment = nlp(comment)\n",
    "#     lemmatized = list()\n",
    "#     for word in comment:\n",
    "#         lemma = word.lemma_.strip()\n",
    "#         if lemma:\n",
    "#             if not remove_stopwords or (remove_stopwords and lemma not in stopwords):\n",
    "#                 lemmatized.append(lemma)\n",
    "#     return \" \".join(lemmatized)\n",
    "\n",
    "corpus = []\n",
    "\n",
    "with open('fake1.txt',encoding='utf8') as f:\n",
    "    text = f.read()\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    for i in range(len(sentences)):\n",
    "        review = re.sub('[^a-zA-Z]',' ',sentences[i]) # Take punctuations out\n",
    "        review = review.lower() # Lowercasing that sentence\n",
    "        #review = review.split() # Word Tokenizing\n",
    "        review = nlp(review) # Converting to Spacy token format\n",
    "        review = [token.lemma_ for token in review if not str(token) in stopwords] # Lemmatizing skipping stopwords\n",
    "        review = ' '.join(review) # Re-building sentence over review\n",
    "        corpus.append(str(review))\n",
    "corpus\n",
    "\n",
    "\n",
    "Data = {'Text':text,\n",
    "       'Text_After_Clean':None}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a5a31058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "nome = 'Kátia'\n",
    "doc = nlp(nome)\n",
    "for token in doc:\n",
    "    print(type(token.lemma_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9aceafb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Text': 'Kátia Abreu diz que vai colocar sua expulsão em uma moldura, mas não para de reclamar.\\t\\n\\nA senadora Kátia Abreu (sem partido-TO) disse que sua expulsão do PMDB foi resultado de uma ação da cúpula atual da legenda que, segundo ela, é oportunista.\\n\\n“Amanhã eu vou botar numa moldura dourada a minha expulsão, porque das mãos de onde veio, é um atestado de boa conduta para o meu currículo. Essas pessoas que me expulsaram não servem ao país. Eles se servem do país em seus benefícios próprios”, disse Kátia Abreu.\\n\\nUé, mas se a expulsão é algo tão bom para seu currículo, por que tanta choradeira, Kátia?\\n\\nSabemos o motivo. Provavelmente Kátia não tem valor para o PT, partido que já deveria tê-la absorvido. Ao que parece o PT gostava de Kátia somente se ela ficasse entrincheirada dentro do PMDB.\\n\\nOu seja, isso é se rebaixar demais. Resta a Kátia ficar chorando as pitangas por todos os cantos.\\n\\nEm tempo: até o momento o PT não cadastrou Kátia Abreu em suas fileiras. Que situação patética para a ex-ministra da Agricultura de Dilma.',\n",
       " 'Text_After_Clean': 'kátia abreu dizer ir colocar suar expulsão umar moldurar , parir reclamar . senador kátia abreu ( partido-to ) dizer suar expulsão pmdb ser resultar umar ação cúpula atual legendar , segundar , ser oportunista . “ amanhã ir botar moldurar dourar expulsão , porque mão onde vir , ser atestar bom conduta parir currículo . pessoa expulsar servir país . servir país benefício próprio ” , dizer kátia abreu . ué , expulsão ser algo tão bom parir currículo , tanto choradeira , kátia ? saber motivar . provavelmente kátia ter valor parir pt , partir dever tê-la absorver . parecer pt gostar kátia somente ficar entrincheirar dentro pmdb . ser , ser rebaixar demais . restar kátia ficar chorar pitanga todo canto . tempo : momento pt cadastrar kátia abreu suar fileira . situação patético parir ex-ministra agricultura dilma .'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data['Text_After_Clean'] = normalize(Data['Text'],lowercase=True, remove_stopwords=True)\n",
    "Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63c2546e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'de a o que e é do da em um para com não uma os no se na por mais as dos como mas ao ele das à seu sua ou quando muito nos já eu também só pelo pela até isso ela entre depois sem mesmo aos seus quem nas me esse eles você essa num nem suas meu às minha numa pelos elas qual nós lhe deles essas esses pelas este dele tu te vocês vos lhes meus minhas teu tua teus tuas nosso nossa nossos nossas dela delas esta estes estas aquele aquela aqueles aquelas isto aquilo estou está estamos estão estive esteve estivemos estiveram estava estávamos estavam estivera estivéramos esteja estejamos estejam estivesse estivéssemos estivessem estiver estivermos estiverem hei há havemos hão houve houvemos houveram houvera houvéramos haja hajamos hajam houvesse houvéssemos houvessem houver houvermos houverem houverei houverá houveremos houverão houveria houveríamos houveriam sou somos são era éramos eram fui foi fomos foram fora fôramos seja sejamos sejam fosse fôssemos fossem for formos forem serei será seremos serão seria seríamos seriam tenho tem temos tém tinha tínhamos tinham tive teve tivemos tiveram tivera tivéramos tenha tenhamos tenham tivesse tivéssemos tivessem tiver tivermos tiverem terei terá teremos terão teria teríamos teriam'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c7b7625",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "301048cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lc = 5 #learning curve steps?\n",
    "y = 100\n",
    "\n",
    "s = (np.linspace(0,1,lc+1) * y).astype(np.int64)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da258984",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 20,  40,  60,  80, 100], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3961dc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
