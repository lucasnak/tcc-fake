{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5644c2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibliotecas além do gerenciador Anaconda\n",
    "!pip install spacy\n",
    "!python -m spacy download pt_core_news_sm\n",
    "!pip install wordcloud\n",
    "!pip install gensim\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f4eced3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Módulos básicos para manuseio de dados e arquivos\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from os.path import isfile, join\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "import json\n",
    "\n",
    "# Módulos para visualização de dados\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#from wordcloud import WordCloud\n",
    "%matplotlib inline\n",
    "\n",
    "# Módulo para processamento de linguagem\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788de39a",
   "metadata": {},
   "source": [
    "## Carregamento de textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4597ed41",
   "metadata": {},
   "outputs": [],
   "source": [
    "limited_news_path = r'Software\\Fake.br-Corpus' #\\fake_10 or \\true_10\n",
    "news_path = r'Software\\Fake.br-Corpus\\full_texts' #\\fake or \\true\n",
    "\n",
    "paths = [limited_news_path, news_path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97d20bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sortDir(dir_path: str, is_meta=False) ->list:\n",
    "    '''\n",
    "    Ordena os arquivos dentro de dir_path e os retorna no formato de lista.\n",
    "    '''\n",
    "    if is_meta:\n",
    "        number_separator = \"-meta.txt\"\n",
    "    else:\n",
    "        number_separator = \".txt\"\n",
    "\n",
    "    first_list = os.listdir(dir_path)\n",
    "    int_list = [int(element.split(number_separator)[0]) for element in first_list]\n",
    "    int_list.sort()\n",
    "    final_list = [(str(element) + number_separator) for element in int_list]\n",
    "\n",
    "    return final_list\n",
    "\n",
    "def txtToDataframe(path, is_limited=True):\n",
    "    '''\n",
    "    Function for converting full texts to a single DataFrame.\n",
    "    '''\n",
    "    if is_limited:\n",
    "        true_files = [path+\"\\\\true_10\\\\\"+f for f in sortDir(dir_path = path+'\\\\true_10') if isfile(join(path+'\\\\true_10', f))]\n",
    "        fake_files = [path+\"\\\\fake_10\\\\\"+f for f in sortDir(dir_path = path+'\\\\fake_10') if isfile(join(path+'\\\\fake_10', f))]\n",
    "    else:\n",
    "        true_files = [path+\"\\\\true\\\\\"+f for f in sortDir(dir_path = path+'\\\\true') if isfile(join(path+'\\\\true', f))]\n",
    "        fake_files = [path+\"\\\\fake\\\\\"+f for f in sortDir(dir_path = path+'\\\\fake') if isfile(join(path+'\\\\fake', f))]\n",
    "    \n",
    "    texts = []\n",
    "    labels = []\n",
    "    \n",
    "    for file in true_files:\n",
    "        with open(file, encoding='utf-8-sig') as f:\n",
    "            texts.append(f.read())\n",
    "            labels.append(0)\n",
    "    for file in fake_files:\n",
    "        with open(file, encoding='utf-8-sig') as f:\n",
    "            texts.append(f.read())\n",
    "            labels.append(1)\n",
    "            \n",
    "    df = pd.DataFrame(list(zip(texts,labels)),columns=['texts','labels'])\n",
    "    \n",
    "    # Com esta função, textos e labels foram inseridos em um DataFrame de maneira sequencial. Todas as notícias verdadeiras vêm\n",
    "    # ANTES do bloco de notícias falsas.\n",
    "    \n",
    "    return df\n",
    "\n",
    "def appendMetadata(path,df, is_limited=True):\n",
    "    '''\n",
    "    Function for appending metadata to previously generated news DataFrame.\n",
    "    '''\n",
    "    if is_limited:\n",
    "        true_meta = [path+\"\\\\true-meta-information-10\\\\\"+f for f in sortDir(dir_path = path+'\\\\true-meta-information-10',is_meta=True) if isfile(join(path+'\\\\true-meta-information-10', f))]\n",
    "        fake_meta = [path+\"\\\\fake-meta-information-10\\\\\"+f for f in sortDir(dir_path = path+'\\\\fake-meta-information-10',is_meta=True) if isfile(join(path+'\\\\fake-meta-information-10', f))]\n",
    "    else:\n",
    "        true_meta = [path+\"\\\\true-meta-information\\\\\"+f for f in sortDir(dir_path = path+'\\\\true-meta-information',is_meta=True) if isfile(join(path+'\\\\true-meta-information', f))]\n",
    "        fake_meta = [path+\"\\\\fake-meta-information\\\\\"+f for f in sortDir(dir_path = path+'\\\\fake-meta-information',is_meta=True) if isfile(join(path+'\\\\fake-meta-information', f))]\n",
    "    \n",
    "\n",
    "    #true_meta e fake_meta são listas com todas os paths para arquivos de metadata.\n",
    "    \n",
    "    columns = [\"author\", \"source\", \"category\", \"date\",\"tokens\",\"words_without_punctuation\",\"types\",\"number_of_links\",\"uppercase_words\",\"verbs\",\"subjuntive_imperative\",\"nouns\",\"adjectives\",\"adverbs\",\"modal_verbs\",\"singular_first_and_second_personal_pronouns\",\"plural_first_personal_pronouns\",\"pronouns\",\"pausality\",\"characters\",\"avg_sentence_length\",\"avg_word_length\",\"percentage_of_spelling_errors\",\"emotiveness\",\"diversity\"]\n",
    "    \n",
    "    true_metadata = pd.DataFrame(columns=columns)\n",
    "    fake_metadata = pd.DataFrame(columns=columns)\n",
    "    \n",
    "    for file in true_meta:\n",
    "        #print(file)\n",
    "        aux = pd.read_csv(file, header=None, sep = '\\n').transpose()\n",
    "        aux.columns = columns\n",
    "        true_metadata=true_metadata.append(aux)\n",
    "        \n",
    "        \n",
    "    for file in fake_meta:\n",
    "        #print(file)\n",
    "        aux = pd.read_csv(file, header=None, sep = '\\n').transpose()\n",
    "        aux.columns = columns\n",
    "        fake_metadata=fake_metadata.append(aux)\n",
    "        \n",
    "    \n",
    "    metadata = pd.DataFrame(columns=columns)\n",
    "    metadata = metadata.append(true_metadata,ignore_index=True)\n",
    "    metadata = metadata.append(fake_metadata,ignore_index=True) \n",
    "\n",
    "\n",
    "    complete_df = pd.concat([df,metadata],axis=1)\n",
    "    # Este DataFrame possui todos os textos/labels (2 colunas) e metadata (25 colunas).\n",
    "    \n",
    "    return complete_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9db387d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - Base com 10 notícias verdadeiras e 10 notícias falsas\n",
      "1 - Base completa de notícias\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "ai = int(input('''0 - Base com 10 notícias verdadeiras e 10 notícias falsas\n",
    "1 - Base completa de notícias\n",
    "'''))\n",
    "\n",
    "path = paths[ai]\n",
    "\n",
    "if ai == 0:\n",
    "    data = txtToDataframe(path) # Dataframe contendo notícias e labels.\n",
    "    complete_data = appendMetadata(path,data) # Dataframe contendo notícias, labels e metadata.\n",
    "else:\n",
    "    data = txtToDataframe(path,is_limited=False)\n",
    "    complete_data = appendMetadata(path,data,is_limited=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8090df3d",
   "metadata": {},
   "source": [
    "## Preprocessamento de textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ea64800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando o pacote de língua portuguesa para o processador Spacy\n",
    "nlp = spacy.load('pt_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbf9fa87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defininido funções de preprocessamento\n",
    "\n",
    "def removePunct(text):\n",
    "    '''\n",
    "    Removes any punctuation included in string.punctuation.\n",
    "    '''\n",
    "    translator = text.maketrans({key:'' for key in string.punctuation+'“”'}) # Translates any punctuation into ''\n",
    "    return text.translate(translator)\n",
    "\n",
    "def removeNumbers(text):\n",
    "    '''\n",
    "    Removes any number character in text.\n",
    "    '''\n",
    "    return re.sub('[0-9]', '' , text) # Translates any number into ''\n",
    "\n",
    "def removeStopWords(string):\n",
    "    '''\n",
    "    Removes any portuguese stopwords, using Spacy's standard package.\n",
    "    '''\n",
    "    doc = nlp(string)\n",
    "    return ' '.join([token.text for token in doc if token.is_stop is False])\n",
    "\n",
    "def lemmatize(string):\n",
    "    '''\n",
    "    Lemmatizes text word-by-word. Notice that lemmatizing is not as harsh as stemming, which makes the final text easier to read and understand in common language.\n",
    "    '''\n",
    "    doc = nlp(string)\n",
    "    return ' '.join([token.lemma_ for token in doc])\n",
    "\n",
    "def prep(string, useStopWords = False, lemma = False):\n",
    "    '''\n",
    "    Executes previously defined preprocessing in text.\n",
    "    '''\n",
    "\n",
    "    result = removeNumbers(removePunct(string)).lower()\n",
    "    \n",
    "    if useStopWords and lemma:\n",
    "        doc = nlp(result)\n",
    "        result = ' '.join([token.lemma_ for token in doc if token.is_stop is False])\n",
    "    elif useStopWords:\n",
    "        doc = nlp(result)\n",
    "        result = ' '.join([token.text for token in doc if token.is_stop is False])\n",
    "    elif lemma:\n",
    "        doc = nlp(result)\n",
    "        result = ' '.join([token.lemma_ for token in doc])\n",
    "\n",
    "    result = result.replace('\\n',\"\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58e24c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizando preprocessamento de textos presentes no Dataframe de notícias completo.\n",
    "\n",
    "complete_data['texts'] = complete_data['texts'].apply(prep)\n",
    "\n",
    "\n",
    "# Assignando variáveis dependentes e independentes\n",
    "\n",
    "y = complete_data['labels'].values # y is strings for labels; but should be fake-0/true-1\n",
    "X = [d.split() for d in complete_data['texts'].tolist()] # X is a list of lists of words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0849b4e4",
   "metadata": {},
   "source": [
    "## Tokenization (TensorFlow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec5c20fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando módulos para tokenização de textos\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a17729e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerando dicionário de tokens (utilizando a BASE COMPLETA de textos)\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X)\n",
    "vocab = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18a2d75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignando bases de teste e treino\n",
    "\n",
    "test_limit = 0.10\n",
    "training_sentences,testing_sentences,training_labels,testing_labels = train_test_split(X,y,test_size=test_limit, random_state=42)\n",
    "\n",
    "#training_sentences = X[0:test_limit]\n",
    "#testing_sentences = X[test_limit:]\n",
    "\n",
    "#training_labels = y[0:test_limit]\n",
    "#testing_labels = y[test_limit:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2fd48a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizando a sequencialização das bases de treinamento e teste\n",
    "training_sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "127841cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análise e truncagem de sequências\n",
    "\n",
    "def sequenceKill(min_length,max_length,training_sentences_local,training_labels_local,training_sequences_local):\n",
    "\n",
    "    #plt.hist([len(x) for x in X], bins = 750)\n",
    "    #plt.show()\n",
    "\n",
    "    nos = np.array([len(x) for x in training_sentences_local])\n",
    "    print(\"There are \"+ str(len(nos[nos>=max_length])) + \" news equal or longer than \"+ str(max_length) + \" words (will be truncated).\")\n",
    "    print(\"There are \"+ str(len(nos[nos<min_length])) + \" news shorter than \"+ str(min_length) + \" words (will be killed).\")\n",
    "    \n",
    "    print(\"Final database will have \"+str(len(training_sentences_local)-len(nos[nos<min_length]))+\" total entries.\")\n",
    "\n",
    "    \n",
    "    flag_dict = {}\n",
    "    for i in range(0,len(training_sequences_local)):\n",
    "        if len(training_sequences[i]) < min_length:\n",
    "            flag_dict[i] = True\n",
    "        else:\n",
    "            flag_dict[i] = False\n",
    "\n",
    "    training_labels_local = [training_labels_local[i] for i in range(0,len(flag_dict)) if (flag_dict[i] == False)]\n",
    "\n",
    "    training_sequences_local = [seq for seq in training_sequences_local if len(seq)>=min_length]\n",
    "    \n",
    "    return training_sequences_local, training_labels_local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42978a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3448 news equal or longer than 300 words (will be truncated).\n",
      "There are 3032 news shorter than 300 words (will be killed).\n",
      "Final database will have 3448 total entries.\n"
     ]
    }
   ],
   "source": [
    "# Analisando a base\n",
    "# Eliminando menores e truncando maiores\n",
    "\n",
    "min_length = 300\n",
    "max_length = 300\n",
    "\n",
    "training_sequences, training_labels = sequenceKill(min_length, max_length, training_sentences, training_labels,training_sequences)\n",
    "\n",
    "training_padded = pad_sequences(training_sequences,maxlen=max_length,padding='post',truncating='pre')\n",
    "#testing_padded = pad_sequences(testing_sequences,maxlen=max_length,padding='post',truncating='pre')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a046fba",
   "metadata": {},
   "source": [
    "## Vectorization (gensim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fdf4699b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "dea46cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerando o dicionário de vetores para cada palavra da base de treinamento\n",
    "\n",
    "DIM = 100\n",
    "w2v_model = gensim.models.Word2Vec(sentences=X, vector_size=DIM, window=10, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "59bb1d70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('instagram', 0.8593540191650391),\n",
       " ('twitter', 0.8096429109573364),\n",
       " ('youtube', 0.7637268304824829),\n",
       " ('whatsapp', 0.7504486441612244),\n",
       " ('google', 0.7348886132240295),\n",
       " ('perfil', 0.7300568222999573),\n",
       " ('zuckerberg', 0.724479079246521),\n",
       " ('postagem', 0.689237654209137),\n",
       " ('post', 0.6856954097747803),\n",
       " ('canal', 0.6841991543769836)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar('facebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bccc60",
   "metadata": {},
   "source": [
    "## Criação da rede neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9355159d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, Conv1D, MaxPool1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c18e1af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignando os vetores gerados para uma matriz de pesos, que será inserida na rede neural \n",
    "\n",
    "vocab_size = len(vocab)+1\n",
    "\n",
    "def get_weight_matrix(model):\n",
    "    ''' Inserts every word vector in a NumPy array, ordering them by token index (which is determined by their appearing frequencies). '''\n",
    "    weight_matrix = np.zeros((vocab_size, DIM))\n",
    "\n",
    "    for word, token in vocab.items():\n",
    "        weight_matrix[token] = model.wv[word]\n",
    "\n",
    "    return weight_matrix\n",
    "\n",
    "embedding_vectors = get_weight_matrix(model=w2v_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a75b4f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_4 (Embedding)     (None, 2500, 100)         9730400   \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 128)               117248    \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9,847,777\n",
      "Trainable params: 117,377\n",
      "Non-trainable params: 9,730,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=DIM, weights = [embedding_vectors], input_length=max_length, trainable=False))\n",
    "model.add(LSTM(units=128))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ef00d2a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "142/142 [==============================] - 401s 3s/step - loss: 0.6950 - acc: 0.5040 - val_loss: 0.6843 - val_acc: 0.5113\n",
      "Epoch 2/10\n",
      "142/142 [==============================] - 365s 3s/step - loss: 0.6827 - acc: 0.5141 - val_loss: 0.6840 - val_acc: 0.5113\n",
      "Epoch 3/10\n",
      "142/142 [==============================] - 400s 3s/step - loss: 0.6826 - acc: 0.5198 - val_loss: 0.6836 - val_acc: 0.5113\n",
      "Epoch 4/10\n",
      "142/142 [==============================] - 408s 3s/step - loss: 0.6825 - acc: 0.5198 - val_loss: 0.6837 - val_acc: 0.5113\n",
      "Epoch 5/10\n",
      "142/142 [==============================] - 414s 3s/step - loss: 0.6826 - acc: 0.5201 - val_loss: 0.6836 - val_acc: 0.5113\n",
      "Epoch 6/10\n",
      "142/142 [==============================] - 417s 3s/step - loss: 0.6826 - acc: 0.5201 - val_loss: 0.6836 - val_acc: 0.5113\n",
      "Epoch 7/10\n",
      "142/142 [==============================] - 416s 3s/step - loss: 0.6824 - acc: 0.5205 - val_loss: 0.6837 - val_acc: 0.5113\n",
      "Epoch 8/10\n",
      "142/142 [==============================] - 416s 3s/step - loss: 0.6824 - acc: 0.5152 - val_loss: 0.6846 - val_acc: 0.5118\n",
      "Epoch 9/10\n",
      "142/142 [==============================] - 417s 3s/step - loss: 0.6834 - acc: 0.5216 - val_loss: 0.6851 - val_acc: 0.5113\n",
      "Epoch 10/10\n",
      "142/142 [==============================] - 412s 3s/step - loss: 0.6825 - acc: 0.5172 - val_loss: 0.6836 - val_acc: 0.5129\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2a50e4c1a30>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(training_padded, np.transpose(np.asarray(training_labels)), validation_split=0.3, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d84b73b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 22s 922ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5027777777777778"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sequencialização e truncagem do conjunto de testes\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(testing_sentences) # Sequencing testing sentences.\n",
    "padded = pad_sequences(testing_sequences,maxlen=max_length,padding='post',truncating='pre')\n",
    "\n",
    "\n",
    "#Tesntando acurácia do modelo na base de testes\n",
    "\n",
    "y_pred = (model.predict(padded) >=0.5).astype(int)\n",
    "\n",
    "accuracy_score(testing_labels, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377801d9",
   "metadata": {},
   "source": [
    "## Salvando o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8d7ceff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nome do modelo:tokvec-trunc-0e2500-10epo-acc50\n",
      "Model saved!\n"
     ]
    }
   ],
   "source": [
    "def saveModel(model,dicto):\n",
    "    name = input('nome do modelo:')\n",
    "    model.save(str(name)+'.h5')\n",
    "    with open('wordIndex.json','w') as f:\n",
    "        json.dump(dicto,f,indent = \"\")\n",
    "    print('Model saved!')\n",
    "    return\n",
    "\n",
    "saveModel(model, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2de137",
   "metadata": {},
   "source": [
    "## Buscando notícias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec867b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_labels[14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148a6ef9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "testing_sentences[14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa1752b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
