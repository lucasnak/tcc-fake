{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5644c2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibliotecas além do gerenciador Anaconda\n",
    "!pip install spacy\n",
    "!python -m spacy download pt_core_news_sm\n",
    "!pip install wordcloud\n",
    "!pip install gensim\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f4eced3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Módulos básicos para manuseio de dados e arquivos\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from os.path import isfile, join\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "import json\n",
    "\n",
    "# Módulos para visualização de dados\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#from wordcloud import WordCloud\n",
    "%matplotlib inline\n",
    "\n",
    "# Módulo para processamento de linguagem\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788de39a",
   "metadata": {},
   "source": [
    "## Carregamento de textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4597ed41",
   "metadata": {},
   "outputs": [],
   "source": [
    "limited_news_path = r'Software\\Fake.br-Corpus' #\\fake_10 or \\true_10\n",
    "news_path = r'Software\\Fake.br-Corpus\\full_texts' #\\fake or \\true\n",
    "\n",
    "paths = [limited_news_path, news_path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97d20bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sortDir(dir_path: str, is_meta=False) ->list:\n",
    "    '''\n",
    "    Ordena os arquivos dentro de dir_path e os retorna no formato de lista.\n",
    "    '''\n",
    "    if is_meta:\n",
    "        number_separator = \"-meta.txt\"\n",
    "    else:\n",
    "        number_separator = \".txt\"\n",
    "\n",
    "    first_list = os.listdir(dir_path)\n",
    "    int_list = [int(element.split(number_separator)[0]) for element in first_list]\n",
    "    int_list.sort()\n",
    "    final_list = [(str(element) + number_separator) for element in int_list]\n",
    "\n",
    "    return final_list\n",
    "\n",
    "def txtToDataframe(path, is_limited=True):\n",
    "    '''\n",
    "    Function for converting full texts to a single DataFrame.\n",
    "    '''\n",
    "    if is_limited:\n",
    "        true_files = [path+\"\\\\true_10\\\\\"+f for f in sortDir(dir_path = path+'\\\\true_10') if isfile(join(path+'\\\\true_10', f))]\n",
    "        fake_files = [path+\"\\\\fake_10\\\\\"+f for f in sortDir(dir_path = path+'\\\\fake_10') if isfile(join(path+'\\\\fake_10', f))]\n",
    "    else:\n",
    "        true_files = [path+\"\\\\true\\\\\"+f for f in sortDir(dir_path = path+'\\\\true') if isfile(join(path+'\\\\true', f))]\n",
    "        fake_files = [path+\"\\\\fake\\\\\"+f for f in sortDir(dir_path = path+'\\\\fake') if isfile(join(path+'\\\\fake', f))]\n",
    "    \n",
    "    texts = []\n",
    "    labels = []\n",
    "    \n",
    "    for file in true_files:\n",
    "        with open(file, encoding='utf-8-sig') as f:\n",
    "            texts.append(f.read())\n",
    "            labels.append(0)\n",
    "    for file in fake_files:\n",
    "        with open(file, encoding='utf-8-sig') as f:\n",
    "            texts.append(f.read())\n",
    "            labels.append(1)\n",
    "            \n",
    "    df = pd.DataFrame(list(zip(texts,labels)),columns=['texts','labels'])\n",
    "    \n",
    "    # Com esta função, textos e labels foram inseridos em um DataFrame de maneira sequencial. Todas as notícias verdadeiras vêm\n",
    "    # ANTES do bloco de notícias falsas.\n",
    "    \n",
    "    return df\n",
    "\n",
    "def appendMetadata(path,df, is_limited=True):\n",
    "    '''\n",
    "    Function for appending metadata to previously generated news DataFrame.\n",
    "    '''\n",
    "    if is_limited:\n",
    "        true_meta = [path+\"\\\\true-meta-information-10\\\\\"+f for f in sortDir(dir_path = path+'\\\\true-meta-information-10',is_meta=True) if isfile(join(path+'\\\\true-meta-information-10', f))]\n",
    "        fake_meta = [path+\"\\\\fake-meta-information-10\\\\\"+f for f in sortDir(dir_path = path+'\\\\fake-meta-information-10',is_meta=True) if isfile(join(path+'\\\\fake-meta-information-10', f))]\n",
    "    else:\n",
    "        true_meta = [path+\"\\\\true-meta-information\\\\\"+f for f in sortDir(dir_path = path+'\\\\true-meta-information',is_meta=True) if isfile(join(path+'\\\\true-meta-information', f))]\n",
    "        fake_meta = [path+\"\\\\fake-meta-information\\\\\"+f for f in sortDir(dir_path = path+'\\\\fake-meta-information',is_meta=True) if isfile(join(path+'\\\\fake-meta-information', f))]\n",
    "    \n",
    "\n",
    "    #true_meta e fake_meta são listas com todas os paths para arquivos de metadata.\n",
    "    \n",
    "    columns = [\"author\", \"source\", \"category\", \"date\",\"tokens\",\"words_without_punctuation\",\"types\",\"number_of_links\",\"uppercase_words\",\"verbs\",\"subjuntive_imperative\",\"nouns\",\"adjectives\",\"adverbs\",\"modal_verbs\",\"singular_first_and_second_personal_pronouns\",\"plural_first_personal_pronouns\",\"pronouns\",\"pausality\",\"characters\",\"avg_sentence_length\",\"avg_word_length\",\"percentage_of_spelling_errors\",\"emotiveness\",\"diversity\"]\n",
    "    \n",
    "    true_metadata = pd.DataFrame(columns=columns)\n",
    "    fake_metadata = pd.DataFrame(columns=columns)\n",
    "    \n",
    "    for file in true_meta:\n",
    "        #print(file)\n",
    "        aux = pd.read_csv(file, header=None, sep = '\\n').transpose()\n",
    "        aux.columns = columns\n",
    "        true_metadata=true_metadata.append(aux)\n",
    "        \n",
    "        \n",
    "    for file in fake_meta:\n",
    "        #print(file)\n",
    "        aux = pd.read_csv(file, header=None, sep = '\\n').transpose()\n",
    "        aux.columns = columns\n",
    "        fake_metadata=fake_metadata.append(aux)\n",
    "        \n",
    "    \n",
    "    metadata = pd.DataFrame(columns=columns)\n",
    "    metadata = metadata.append(true_metadata,ignore_index=True)\n",
    "    metadata = metadata.append(fake_metadata,ignore_index=True) \n",
    "\n",
    "\n",
    "    complete_df = pd.concat([df,metadata],axis=1)\n",
    "    # Este DataFrame possui todos os textos/labels (2 colunas) e metadata (25 colunas).\n",
    "    \n",
    "    return complete_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9db387d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - Base com 10 notícias verdadeiras e 10 notícias falsas\n",
      "1 - Base completa de notícias\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "ai = int(input('''0 - Base com 10 notícias verdadeiras e 10 notícias falsas\n",
    "1 - Base completa de notícias\n",
    "'''))\n",
    "\n",
    "path = paths[ai]\n",
    "\n",
    "if ai == 0:\n",
    "    data = txtToDataframe(path) # Dataframe contendo notícias e labels.\n",
    "    complete_data = appendMetadata(path,data) # Dataframe contendo notícias, labels e metadata.\n",
    "else:\n",
    "    data = txtToDataframe(path,is_limited=False)\n",
    "    complete_data = appendMetadata(path,data,is_limited=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fbc2db81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'o podemos decidiu  expulsar o deputado federal carlos gaguim do partido após a polícia federal fazer buscas a apreensões no gabinete dele na câmara com isso a legenda abre espaço para receber a senadora expulsa pelo pmdb katia abreu por meio de nota a legenda informou que o afastamento do parlamentar já era algo acordado entre os filiados da sigla  ainda que o parlamentar tenha comunicado a conclusão de sua desfiliação para esta semana diante dos fatos noticiados hoje a executiva nacional do podemos solicita o imediato cancelamento de sua filiação dos quadros do partidoo partido que no passado chegou a cogitar lançar o parlamentar como candidato ao senado diz que apoia a investigação com a ampla apuração dos eventuais crimes cometidos e a consequente responsabilização dos envolvidos para que todos sejam punidos com o máximo rigor da lei independentemente de posição ou cargo ocupado '"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_data['texts'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8090df3d",
   "metadata": {},
   "source": [
    "## Preprocessamento de textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ea64800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando o pacote de língua portuguesa para o processador Spacy\n",
    "nlp = spacy.load('pt_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbf9fa87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defininido funções de preprocessamento\n",
    "\n",
    "def removePunct(text):\n",
    "    '''\n",
    "    Removes any punctuation included in string.punctuation.\n",
    "    '''\n",
    "    translator = text.maketrans({key:'' for key in string.punctuation+'“”'}) # Translates any punctuation into ''\n",
    "    return text.translate(translator)\n",
    "\n",
    "def removeNumbers(text):\n",
    "    '''\n",
    "    Removes any number character in text.\n",
    "    '''\n",
    "    return re.sub('[0-9]', '' , text) # Translates any number into ''\n",
    "\n",
    "def removeStopWords(string):\n",
    "    '''\n",
    "    Removes any portuguese stopwords, using Spacy's standard package.\n",
    "    '''\n",
    "    doc = nlp(string)\n",
    "    return ' '.join([token.text for token in doc if token.is_stop is False])\n",
    "\n",
    "def lemmatize(string):\n",
    "    '''\n",
    "    Lemmatizes text word-by-word. Notice that lemmatizing is not as harsh as stemming, which makes the final text easier to read and understand in common language.\n",
    "    '''\n",
    "    doc = nlp(string)\n",
    "    return ' '.join([token.lemma_ for token in doc])\n",
    "\n",
    "def prep(string, useStopWords = False, lemma = False):\n",
    "    '''\n",
    "    Executes previously defined preprocessing in text.\n",
    "    '''\n",
    "\n",
    "    result = removeNumbers(removePunct(string)).lower()\n",
    "    \n",
    "    if useStopWords and lemma:\n",
    "        doc = nlp(result)\n",
    "        result = ' '.join([token.lemma_ for token in doc if token.is_stop is False])\n",
    "    elif useStopWords:\n",
    "        doc = nlp(result)\n",
    "        result = ' '.join([token.text for token in doc if token.is_stop is False])\n",
    "    elif lemma:\n",
    "        doc = nlp(result)\n",
    "        result = ' '.join([token.lemma_ for token in doc])\n",
    "\n",
    "    result = result.replace('\\n',\"\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58e24c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizando preprocessamento de textos presentes no Dataframe de notícias completo.\n",
    "\n",
    "complete_data['texts'] = complete_data['texts'].apply(prep)\n",
    "\n",
    "\n",
    "# Assignando variáveis dependentes e independentes\n",
    "\n",
    "y = complete_data['labels'].values # y is strings for labels; but should be fake-0/true-1\n",
    "X = [d.split() for d in complete_data['texts'].tolist()] # X is a list of lists of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71650076",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'o podemos decidiu  expulsar o deputado federal carlos gaguim do partido após a polícia federal fazer buscas a apreensões no gabinete dele na câmara com isso a legenda abre espaço para receber a senadora expulsa pelo pmdb katia abreu por meio de nota a legenda informou que o afastamento do parlamentar já era algo acordado entre os filiados da sigla  ainda que o parlamentar tenha comunicado a conclusão de sua desfiliação para esta semana diante dos fatos noticiados hoje a executiva nacional do podemos solicita o imediato cancelamento de sua filiação dos quadros do partidoo partido que no passado chegou a cogitar lançar o parlamentar como candidato ao senado diz que apoia a investigação com a ampla apuração dos eventuais crimes cometidos e a consequente responsabilização dos envolvidos para que todos sejam punidos com o máximo rigor da lei independentemente de posição ou cargo ocupado '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_data['texts'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0849b4e4",
   "metadata": {},
   "source": [
    "## Tokenization (TensorFlow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec5c20fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando módulos para tokenização de textos\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18a2d75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignando bases de teste e treino\n",
    "\n",
    "test_limit = 0.10\n",
    "training_sentences,testing_sentences,training_labels,testing_labels = train_test_split(X,y,test_size=test_limit, random_state=42)\n",
    "\n",
    "#training_sentences = X[0:test_limit]\n",
    "#testing_sentences = X[test_limit:]\n",
    "\n",
    "#training_labels = y[0:test_limit]\n",
    "#testing_labels = y[test_limit:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "127841cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análise e truncagem de sequências\n",
    "\n",
    "def sequenceKill(min_length,max_length,training_sentences_local,training_labels_local,training_sequences_local):\n",
    "\n",
    "    #plt.hist([len(x) for x in X], bins = 750)\n",
    "    #plt.show()\n",
    "\n",
    "    nos = np.array([len(x) for x in training_sentences_local])\n",
    "    print(\"There are \"+ str(len(nos[nos>=max_length])) + \" news equal or longer than \"+ str(max_length) + \" words (will be truncated).\")\n",
    "    print(\"There are \"+ str(len(nos[nos<min_length])) + \" news shorter than \"+ str(min_length) + \" words (will be killed).\")\n",
    "    \n",
    "    print(\"Final database will have \"+str(len(training_sentences_local)-len(nos[nos<min_length]))+\" total entries.\")\n",
    "\n",
    "    \n",
    "    flag_dict = {}\n",
    "    for i in range(0,len(training_sequences_local)):\n",
    "        if len(training_sequences[i]) < min_length:\n",
    "            flag_dict[i] = True\n",
    "        else:\n",
    "            flag_dict[i] = False\n",
    "\n",
    "    training_labels_local = [training_labels_local[i] for i in range(0,len(flag_dict)) if (flag_dict[i] == False)]\n",
    "\n",
    "    training_sequences_local = [seq for seq in training_sequences_local if len(seq)>=min_length]\n",
    "    \n",
    "    return training_sequences_local, training_labels_local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ffb24c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerando dicionário de tokens (com base nos textos de treinamento)\n",
    "tokenizer = Tokenizer()  # não utilizaremos o oov_token='<OOV>'\n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "vocab = tokenizer.word_index\n",
    "\n",
    "# Realizando a sequencialização das bases de treinamento e teste\n",
    "training_sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d2bd38a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2823 news equal or longer than 500 words (will be truncated).\n",
      "There are 1586 news shorter than 150 words (will be killed).\n",
      "Final database will have 4894 total entries.\n"
     ]
    }
   ],
   "source": [
    "# Analisando a base\n",
    "# Eliminando menores e truncando maiores\n",
    "\n",
    "min_length = 150\n",
    "max_length = 500\n",
    "\n",
    "training_sequences, training_labels = sequenceKill(min_length, max_length, training_sentences, training_labels,training_sequences)\n",
    "\n",
    "training_padded = pad_sequences(training_sequences,maxlen=max_length,padding='post',truncating='pre')\n",
    "#testing_padded = pad_sequences(testing_sequences,maxlen=max_length,padding='post',truncating='pre')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a046fba",
   "metadata": {},
   "source": [
    "## Vectorization (gensim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fdf4699b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dea46cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerando o dicionário de vetores para cada palavra da base de treinamento\n",
    "\n",
    "DIM = 100\n",
    "w2v_model = gensim.models.Word2Vec(sentences=training_sentences, vector_size=DIM, window=10, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "59bb1d70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('diesel', 0.8367844223976135),\n",
       " ('líquido', 0.8134790658950806),\n",
       " ('dólar', 0.7868914604187012),\n",
       " ('piso', 0.7768596410751343),\n",
       " ('combustível', 0.7712751030921936),\n",
       " ('nacala', 0.764155924320221),\n",
       " ('combustíveis', 0.762214183807373),\n",
       " ('etanol', 0.7619045376777649),\n",
       " ('óleo', 0.7574036717414856),\n",
       " ('nasdaq', 0.7573919892311096)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar('gasolina')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bccc60",
   "metadata": {},
   "source": [
    "## Criação da rede neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9355159d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, Conv1D, MaxPool1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c18e1af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignando os vetores gerados para uma matriz de pesos, que será inserida na rede neural \n",
    "\n",
    "vocab_size = len(vocab)+1\n",
    "\n",
    "def get_weight_matrix(model):\n",
    "    ''' Inserts every word vector in a NumPy array, ordering them by token index (which is determined by their appearing frequencies). '''\n",
    "    weight_matrix = np.zeros((vocab_size, DIM))\n",
    "\n",
    "    for word, token in vocab.items():\n",
    "        weight_matrix[token] = model.wv[word]\n",
    "\n",
    "    return weight_matrix\n",
    "\n",
    "embedding_vectors = get_weight_matrix(model=w2v_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a75b4f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 500, 100)          9235900   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 128)               117248    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9,353,277\n",
      "Trainable params: 117,377\n",
      "Non-trainable params: 9,235,900\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=DIM, weights = [embedding_vectors], input_length=max_length, trainable=False))\n",
    "model.add(LSTM(units=128))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ef00d2a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "108/108 [==============================] - 79s 669ms/step - loss: 0.2990 - acc: 0.8809 - val_loss: 0.2594 - val_acc: 0.9088\n",
      "Epoch 2/10\n",
      "108/108 [==============================] - 76s 706ms/step - loss: 0.2773 - acc: 0.9010 - val_loss: 0.2411 - val_acc: 0.9156\n",
      "Epoch 3/10\n",
      "108/108 [==============================] - 74s 689ms/step - loss: 0.2386 - acc: 0.9063 - val_loss: 0.2137 - val_acc: 0.9210\n",
      "Epoch 4/10\n",
      "108/108 [==============================] - 79s 731ms/step - loss: 0.1890 - acc: 0.9273 - val_loss: 0.2017 - val_acc: 0.9306\n",
      "Epoch 5/10\n",
      "108/108 [==============================] - 86s 799ms/step - loss: 0.1750 - acc: 0.9355 - val_loss: 0.1905 - val_acc: 0.9374\n",
      "Epoch 6/10\n",
      "108/108 [==============================] - 74s 690ms/step - loss: 0.1498 - acc: 0.9445 - val_loss: 0.1829 - val_acc: 0.9381\n",
      "Epoch 7/10\n",
      "108/108 [==============================] - 69s 638ms/step - loss: 0.1307 - acc: 0.9542 - val_loss: 0.2126 - val_acc: 0.9340\n",
      "Epoch 8/10\n",
      "108/108 [==============================] - 67s 623ms/step - loss: 0.1268 - acc: 0.9568 - val_loss: 0.1869 - val_acc: 0.9408\n",
      "Epoch 9/10\n",
      "108/108 [==============================] - 82s 758ms/step - loss: 0.1117 - acc: 0.9638 - val_loss: 0.1875 - val_acc: 0.9408\n",
      "Epoch 10/10\n",
      "108/108 [==============================] - 71s 655ms/step - loss: 0.1115 - acc: 0.9644 - val_loss: 0.1905 - val_acc: 0.9401\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1733a3e3f10>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(training_padded, np.transpose(np.asarray(training_labels)), validation_split=0.3, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d84b73b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9597222222222223"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sequencialização e truncagem do conjunto de testes\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(testing_sentences) # Sequencing testing sentences.\n",
    "padded = pad_sequences(testing_sequences,maxlen=max_length,padding='post',truncating='pre')\n",
    "\n",
    "\n",
    "#Tesntando acurácia do modelo na base de testes\n",
    "\n",
    "y_pred = (model.predict(padded) >=0.5).astype(int)\n",
    "\n",
    "accuracy_score(testing_labels, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377801d9",
   "metadata": {},
   "source": [
    "## Salvando o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8d7ceff0",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20528/2957423738.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0msaveModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20528/2957423738.py\u001b[0m in \u001b[0;36msaveModel\u001b[1;34m(model, dicto)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msaveModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdicto\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'nome do modelo:'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'wordIndex.json'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdicto\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1004\u001b[0m                 \u001b[1;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1005\u001b[0m             )\n\u001b[1;32m-> 1006\u001b[1;33m         return self._input_request(\n\u001b[0m\u001b[0;32m   1007\u001b[0m             \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1008\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"shell\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1049\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1050\u001b[0m                 \u001b[1;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1051\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Interrupted by user\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Invalid Message:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "def saveModel(model,dicto):\n",
    "    name = input('nome do modelo:')\n",
    "    model.save(str(name)+'.h5')\n",
    "    with open('wordIndex.json','w') as f:\n",
    "        json.dump(dicto,f,indent = \"\")\n",
    "    print('Model saved!')\n",
    "    return\n",
    "\n",
    "saveModel(model, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2de137",
   "metadata": {},
   "source": [
    "## Buscando notícias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec867b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_labels[14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148a6ef9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "testing_sentences[14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa1752b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
