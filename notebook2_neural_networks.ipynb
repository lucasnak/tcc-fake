{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5644c2fb",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.3.1; however, version 22.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\nakam\\anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\nakam\\anaconda3\\lib\\site-packages (3.2.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from spacy) (1.8.2)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from spacy) (8.0.13)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from spacy) (20.9)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from spacy) (1.0.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from spacy) (2.25.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from spacy) (1.20.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from spacy) (4.59.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from spacy) (60.5.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from spacy) (2.4.2)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from spacy) (0.4.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from spacy) (3.0.6)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from spacy) (0.7.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from spacy) (0.6.1)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from spacy) (1.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy) (3.7.4.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (4.0.0)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from jinja2->spacy) (1.1.1)\n",
      "Collecting pt-core-news-sm==3.2.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-3.2.0/pt_core_news_sm-3.2.0-py3-none-any.whl (22.2 MB)\n",
      "Requirement already satisfied: spacy<3.3.0,>=3.2.0 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from pt-core-news-sm==3.2.0) (3.2.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (3.0.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (1.0.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (60.5.0)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (0.4.0)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (8.0.13)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (3.0.8)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (0.9.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (2.11.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (20.9)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (1.0.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (2.25.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (2.0.6)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-19 21:32:01.990937: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2022-05-19 21:32:01.991379: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "WARNING: You are using pip version 21.3.1; however, version 22.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\nakam\\anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (0.7.5)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (1.20.1)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (0.6.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (4.59.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (1.8.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (3.3.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (2.4.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (3.7.4.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (4.0.0)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (1.1.1)\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('pt_core_news_sm')\n",
      "Requirement already satisfied: wordcloud in c:\\users\\nakam\\anaconda3\\lib\\site-packages (1.8.1)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.3.1; however, version 22.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\nakam\\anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: matplotlib in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from wordcloud) (3.3.4)\n",
      "Requirement already satisfied: pillow in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from wordcloud) (8.2.0)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from wordcloud) (1.20.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (1.3.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (0.10.0)\n",
      "Requirement already satisfied: six in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib->wordcloud) (1.15.0)\n",
      "Requirement already satisfied: gensim in c:\\users\\nakam\\anaconda3\\lib\\site-packages (4.2.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from gensim) (1.20.1)\n",
      "Requirement already satisfied: Cython==0.29.28 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from gensim) (0.29.28)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from gensim) (1.6.2)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from gensim) (5.2.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.3.1; however, version 22.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\nakam\\anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\nakam\\anaconda3\\lib\\site-packages (2.9.0)\n",
      "Requirement already satisfied: flatbuffers<2,>=1.12 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from tensorflow) (1.12)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from tensorflow) (3.7.4.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.3.1; however, version 22.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\nakam\\anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: tensorboard<2.10,>=2.9 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from tensorflow) (2.9.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from tensorflow) (14.0.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from tensorflow) (60.5.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from tensorflow) (1.20.1)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from tensorflow) (3.20.1)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from tensorflow) (20.9)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from tensorflow) (2.9.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from tensorflow) (1.0.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from tensorflow) (1.46.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from tensorflow) (2.9.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from tensorflow) (0.26.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (3.3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.6.6)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.25.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from packaging->tensorflow) (2.4.7)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (5.1.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (4.11.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (3.2.0)\n"
     ]
    }
   ],
   "source": [
    "# Bibliotecas além do gerenciador Anaconda\n",
    "!pip install spacy\n",
    "!python -m spacy download pt_core_news_sm\n",
    "!pip install wordcloud\n",
    "!pip install gensim\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f4eced3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Módulos básicos para manuseio de dados e arquivos\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from os.path import isfile, join\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "\n",
    "# Módulos para visualização de dados\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "%matplotlib inline\n",
    "\n",
    "# Módulo para processamento de linguagem\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788de39a",
   "metadata": {},
   "source": [
    "## Carregamento de textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4597ed41",
   "metadata": {},
   "outputs": [],
   "source": [
    "limited_news_path = r'Software\\Fake.br-Corpus' #\\fake_10 or \\true_10\n",
    "news_path = r'Software\\Fake.br-Corpus\\full_texts' #\\fake or \\true\n",
    "\n",
    "paths = [limited_news_path, news_path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97d20bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sortDir(dir_path: str, is_meta=False) ->list:\n",
    "    '''\n",
    "    Ordena os arquivos dentro de dir_path e os retorna no formato de lista.\n",
    "    '''\n",
    "    if is_meta:\n",
    "        number_separator = \"-meta.txt\"\n",
    "    else:\n",
    "        number_separator = \".txt\"\n",
    "\n",
    "    first_list = os.listdir(dir_path)\n",
    "    int_list = [int(element.split(number_separator)[0]) for element in first_list]\n",
    "    int_list.sort()\n",
    "    final_list = [(str(element) + number_separator) for element in int_list]\n",
    "\n",
    "    return final_list\n",
    "\n",
    "def txtToDataframe(path, is_limited=True):\n",
    "    '''\n",
    "    Function for converting full texts to a single DataFrame.\n",
    "    '''\n",
    "    if is_limited:\n",
    "        true_files = [path+\"\\\\true_10\\\\\"+f for f in sortDir(dir_path = path+'\\\\true_10') if isfile(join(path+'\\\\true_10', f))]\n",
    "        fake_files = [path+\"\\\\fake_10\\\\\"+f for f in sortDir(dir_path = path+'\\\\fake_10') if isfile(join(path+'\\\\fake_10', f))]\n",
    "    else:\n",
    "        true_files = [path+\"\\\\true\\\\\"+f for f in sortDir(dir_path = path+'\\\\true') if isfile(join(path+'\\\\true', f))]\n",
    "        fake_files = [path+\"\\\\fake\\\\\"+f for f in sortDir(dir_path = path+'\\\\fake') if isfile(join(path+'\\\\fake', f))]\n",
    "    \n",
    "    texts = []\n",
    "    labels = []\n",
    "    \n",
    "    for file in true_files:\n",
    "        with open(file, encoding='utf-8-sig') as f:\n",
    "            texts.append(f.read())\n",
    "            labels.append('true')\n",
    "    for file in fake_files:\n",
    "        with open(file, encoding='utf-8-sig') as f:\n",
    "            texts.append(f.read())\n",
    "            labels.append('fake')\n",
    "            \n",
    "    df = pd.DataFrame(list(zip(texts,labels)),columns=['texts','labels'])\n",
    "    \n",
    "    # Com esta função, textos e labels foram inseridos em um DataFrame de maneira sequencial. Todas as notícias verdadeiras vêm\n",
    "    # ANTES do bloco de notícias falsas.\n",
    "    \n",
    "    return df\n",
    "\n",
    "def appendMetadata(path,df, is_limited=True):\n",
    "    '''\n",
    "    Function for appending metadata to previously generated news DataFrame.\n",
    "    '''\n",
    "    if is_limited:\n",
    "        true_meta = [path+\"\\\\true-meta-information-10\\\\\"+f for f in sortDir(dir_path = path+'\\\\true-meta-information-10',is_meta=True) if isfile(join(path+'\\\\true-meta-information-10', f))]\n",
    "        fake_meta = [path+\"\\\\fake-meta-information-10\\\\\"+f for f in sortDir(dir_path = path+'\\\\fake-meta-information-10',is_meta=True) if isfile(join(path+'\\\\fake-meta-information-10', f))]\n",
    "    else:\n",
    "        true_meta = [path+\"\\\\true-meta-information\\\\\"+f for f in sortDir(dir_path = path+'\\\\true-meta-information',is_meta=True) if isfile(join(path+'\\\\true-meta-information', f))]\n",
    "        fake_meta = [path+\"\\\\fake-meta-information\\\\\"+f for f in sortDir(dir_path = path+'\\\\fake-meta-information',is_meta=True) if isfile(join(path+'\\\\fake-meta-information', f))]\n",
    "    \n",
    "\n",
    "    #true_meta e fake_meta são listas com todas os paths para arquivos de metadata.\n",
    "    \n",
    "    columns = [\"author\", \"source\", \"category\", \"date\",\"tokens\",\"words_without_punctuation\",\"types\",\"number_of_links\",\"uppercase_words\",\"verbs\",\"subjuntive_imperative\",\"nouns\",\"adjectives\",\"adverbs\",\"modal_verbs\",\"singular_first_and_second_personal_pronouns\",\"plural_first_personal_pronouns\",\"pronouns\",\"pausality\",\"characters\",\"avg_sentence_length\",\"avg_word_length\",\"percentage_of_spelling_errors\",\"emotiveness\",\"diversity\"]\n",
    "    \n",
    "    true_metadata = pd.DataFrame(columns=columns)\n",
    "    fake_metadata = pd.DataFrame(columns=columns)\n",
    "    \n",
    "    for file in true_meta:\n",
    "        #print(file)\n",
    "        aux = pd.read_csv(file, header=None, sep = '\\n').transpose()\n",
    "        aux.columns = columns\n",
    "        true_metadata=true_metadata.append(aux)\n",
    "        \n",
    "        \n",
    "    for file in fake_meta:\n",
    "        #print(file)\n",
    "        aux = pd.read_csv(file, header=None, sep = '\\n').transpose()\n",
    "        aux.columns = columns\n",
    "        fake_metadata=fake_metadata.append(aux)\n",
    "        \n",
    "    \n",
    "    metadata = pd.DataFrame(columns=columns)\n",
    "    metadata = metadata.append(true_metadata,ignore_index=True)\n",
    "    metadata = metadata.append(fake_metadata,ignore_index=True) \n",
    "\n",
    "\n",
    "    complete_df = pd.concat([df,metadata],axis=1)\n",
    "    # Este DataFrame possui todos os textos/labels (2 colunas) e metadata (25 colunas).\n",
    "    \n",
    "    return complete_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9db387d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - Base com 10 notícias verdadeiras e 10 notícias falsas\n",
      "1 - Base completa de notícias\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "ai = int(input('''0 - Base com 10 notícias verdadeiras e 10 notícias falsas\n",
    "1 - Base completa de notícias\n",
    "'''))\n",
    "\n",
    "path = paths[ai]\n",
    "\n",
    "if ai == 0:\n",
    "    data = txtToDataframe(path) # Dataframe contendo notícias e labels.\n",
    "    complete_data = appendMetadata(path,data) # Dataframe contendo notícias, labels e metadata.\n",
    "else:\n",
    "    data = txtToDataframe(path,is_limited=False)\n",
    "    complete_data = appendMetadata(path,data,is_limited=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32982cf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['texts', 'labels', 'author', 'source', 'category', 'date', 'tokens',\n",
       "       'words_without_punctuation', 'types', 'number_of_links',\n",
       "       'uppercase_words', 'verbs', 'subjuntive_imperative', 'nouns',\n",
       "       'adjectives', 'adverbs', 'modal_verbs',\n",
       "       'singular_first_and_second_personal_pronouns',\n",
       "       'plural_first_personal_pronouns', 'pronouns', 'pausality', 'characters',\n",
       "       'avg_sentence_length', 'avg_word_length',\n",
       "       'percentage_of_spelling_errors', 'emotiveness', 'diversity'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fbc2db81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Em evento realizado nesta terça-feira para divulgar o Jogo das Estrelas, um amistoso beneficente que será realizado nesta quarta, no estádio do Maracanã, no Rio de Janeiro, Zico foi questionado sobre o ano do Flamengo. E o ídolo do clube não poupou críticas à montagem do elenco.\\n\\n\"Minha visão é que alguns investimentos que foram feito não surtiram efeito dentro do campo. Acho que é uma equipe supervalorizada, falei isso muito tempo antes\", reclamou. \"Não é fácil você trazer muita gente, ainda mais no meio de competição, com mudanças constantes, lesões. A coisa não funcionou\".\\n\\nNa avaliação de Zico, faltou um trabalho melhor de avaliação dos jogadores contratados, especialmente os que vieram do futebol exterior. \"O futebol é isso mesmo, precisa seguir em frente e analisar quando vocês faz os investimentos, principalmente quando você vai atrás de jogadores que estão no exterior. É preciso saber como eles estão, o rendimento, questões de saúde\", alertou. \"E infelizmente não foi isso que aconteceu com alguns jogadores que o Flamengo investiu\".\\n\\nZico avaliou também que a diretoria precisa se preocupar em trazer jogadores identificados com o clube. Na reta final da temporada, muito se criticou a suposta falta de garra de alguns atletas. \"Precisa saber também o que representa a camisa do Flamengo, a história do clube\", disse. \"É preciso ser bem entendido que o Flamengo é um clube muito diferente dos outros\".\\n\\nO resultado, assim, segundo ele, é que o ano termina de maneira frustrante ao torcedor. \"É lógico que o objetivo principal era ter uma boa atuação na Libertadores, o que não aconteceu. Consequentemente conseguiu chegar na Sul-Americana, chegou a duas finais. Lógico que pelo investimento era esperado que isto acontecesse. Mas ficou um gostinho amargo de não ter conquistado uma competição internacional\".'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_data['texts'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8090df3d",
   "metadata": {},
   "source": [
    "## Preprocessamento de textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ea64800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando o pacote de língua portuguesa para o processador Spacy\n",
    "nlp = spacy.load('pt_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dbf9fa87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defininido funções de preprocessamento\n",
    "\n",
    "def removePunct(text):\n",
    "    '''\n",
    "    Removes any punctuation included in string.punctuation.\n",
    "    '''\n",
    "    translator = text.maketrans({key:'' for key in string.punctuation+'“”'}) # Translates any punctuation into ''\n",
    "    return text.translate(translator)\n",
    "\n",
    "def removeNumbers(text):\n",
    "    '''\n",
    "    Removes any number character in text.\n",
    "    '''\n",
    "    return re.sub('[0-9]', '' , text) # Translates any number into ''\n",
    "\n",
    "def removeStopWords(string):\n",
    "    '''\n",
    "    Removes any portuguese stopwords, using Spacy's standard package.\n",
    "    '''\n",
    "    doc = nlp(string)\n",
    "    return ' '.join([token.text for token in doc if token.is_stop is False])\n",
    "\n",
    "def lemmatize(string):\n",
    "    '''\n",
    "    Lemmatizes text word-by-word. Notice that lemmatizing is not as harsh as stemming, which makes the final text easier to read and understand in common language.\n",
    "    '''\n",
    "    doc = nlp(string)\n",
    "    return ' '.join([token.lemma_ for token in doc])\n",
    "\n",
    "def prep(string, useStopWords = False, lemma = False):\n",
    "    '''\n",
    "    Executes previously defined preprocessing in text.\n",
    "    '''\n",
    "\n",
    "    result = removeNumbers(removePunct(string)).lower()\n",
    "    \n",
    "    if useStopWords and lemma:\n",
    "        doc = nlp(result)\n",
    "        result = ' '.join([token.lemma_ for token in doc if token.is_stop is False])\n",
    "    elif useStopWords:\n",
    "        doc = nlp(result)\n",
    "        result = ' '.join([token.text for token in doc if token.is_stop is False])\n",
    "    elif lemma:\n",
    "        doc = nlp(result)\n",
    "        result = ' '.join([token.lemma_ for token in doc])\n",
    "    return result.replace('\\n',\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58e24c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizando preprocessamento de textos presentes no Dataframe de notícias completo.\n",
    "\n",
    "complete_data['texts'] = complete_data['texts'].apply(prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8bba55be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'o podemos decidiu  expulsar o deputado federal carlos gaguim do partido após a polícia federal fazer buscas a apreensões no gabinete dele na câmara com isso a legenda abre espaço para receber a senadora expulsa pelo pmdb katia abreu por meio de nota a legenda informou que o afastamento do parlamentar já era algo acordado entre os filiados da sigla  ainda que o parlamentar tenha comunicado a conclusão de sua desfiliação para esta semana diante dos fatos noticiados hoje a executiva nacional do podemos solicita o imediato cancelamento de sua filiação dos quadros do partidoo partido que no passado chegou a cogitar lançar o parlamentar como candidato ao senado diz que apoia a investigação com a ampla apuração dos eventuais crimes cometidos e a consequente responsabilização dos envolvidos para que todos sejam punidos com o máximo rigor da lei independentemente de posição ou cargo ocupado '"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_data['texts'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "63fe9f7b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texts</th>\n",
       "      <th>labels</th>\n",
       "      <th>author</th>\n",
       "      <th>source</th>\n",
       "      <th>category</th>\n",
       "      <th>date</th>\n",
       "      <th>tokens</th>\n",
       "      <th>words_without_punctuation</th>\n",
       "      <th>types</th>\n",
       "      <th>number_of_links</th>\n",
       "      <th>...</th>\n",
       "      <th>singular_first_and_second_personal_pronouns</th>\n",
       "      <th>plural_first_personal_pronouns</th>\n",
       "      <th>pronouns</th>\n",
       "      <th>pausality</th>\n",
       "      <th>characters</th>\n",
       "      <th>avg_sentence_length</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>percentage_of_spelling_errors</th>\n",
       "      <th>emotiveness</th>\n",
       "      <th>diversity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>o podemos decidiu  expulsar o deputado federal...</td>\n",
       "      <td>true</td>\n",
       "      <td>Naira Trindade</td>\n",
       "      <td>http://politica.estadao.com.br/blogs/coluna-do...</td>\n",
       "      <td>politica</td>\n",
       "      <td>13/12/2017</td>\n",
       "      <td>168</td>\n",
       "      <td>148</td>\n",
       "      <td>107</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>3.33333</td>\n",
       "      <td>761</td>\n",
       "      <td>24.6667</td>\n",
       "      <td>5.14189</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.134328</td>\n",
       "      <td>0.722973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>em evento realizado nesta terçafeira para divu...</td>\n",
       "      <td>true</td>\n",
       "      <td>Estadão Conteúdo</td>\n",
       "      <td>http://esportes.estadao.com.br/noticias/futebo...</td>\n",
       "      <td>sociedade_cotidiano</td>\n",
       "      <td>26/12/2017</td>\n",
       "      <td>349</td>\n",
       "      <td>294</td>\n",
       "      <td>182</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>2.75</td>\n",
       "      <td>1477</td>\n",
       "      <td>14.7</td>\n",
       "      <td>5.02381</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.325758</td>\n",
       "      <td>0.619048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>após o prefeito de manaus arthur virgílio psdb...</td>\n",
       "      <td>true</td>\n",
       "      <td>Juliana Diógenes</td>\n",
       "      <td>http://politica.estadao.com.br/noticias/geral,...</td>\n",
       "      <td>politica</td>\n",
       "      <td>26/12/2017</td>\n",
       "      <td>249</td>\n",
       "      <td>204</td>\n",
       "      <td>128</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1019</td>\n",
       "      <td>13.6</td>\n",
       "      <td>4.9951</td>\n",
       "      <td>0.0147059</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.627451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>doria vai receber zé celso após reunião com re...</td>\n",
       "      <td>true</td>\n",
       "      <td>Mônica Bergamo</td>\n",
       "      <td>http://www1.folha.uol.com.br/colunas/monicaber...</td>\n",
       "      <td>politica</td>\n",
       "      <td>11/1/2018</td>\n",
       "      <td>170</td>\n",
       "      <td>147</td>\n",
       "      <td>102</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2.875</td>\n",
       "      <td>700</td>\n",
       "      <td>18.375</td>\n",
       "      <td>4.7619</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0694444</td>\n",
       "      <td>0.693878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gustavo pedreira ferraz que admitiu buscar mal...</td>\n",
       "      <td>true</td>\n",
       "      <td>Luiz Vassallo E Breno Pires</td>\n",
       "      <td>http://politica.estadao.com.br/blogs/fausto-ma...</td>\n",
       "      <td>politica</td>\n",
       "      <td>28/12/2017</td>\n",
       "      <td>389</td>\n",
       "      <td>341</td>\n",
       "      <td>181</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>2.66667</td>\n",
       "      <td>1587</td>\n",
       "      <td>18.9444</td>\n",
       "      <td>4.65396</td>\n",
       "      <td>0.00879765</td>\n",
       "      <td>0.11039</td>\n",
       "      <td>0.530792</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               texts labels  \\\n",
       "0  o podemos decidiu  expulsar o deputado federal...   true   \n",
       "1  em evento realizado nesta terçafeira para divu...   true   \n",
       "2  após o prefeito de manaus arthur virgílio psdb...   true   \n",
       "3  doria vai receber zé celso após reunião com re...   true   \n",
       "4  gustavo pedreira ferraz que admitiu buscar mal...   true   \n",
       "\n",
       "                        author  \\\n",
       "0               Naira Trindade   \n",
       "1             Estadão Conteúdo   \n",
       "2             Juliana Diógenes   \n",
       "3               Mônica Bergamo   \n",
       "4  Luiz Vassallo E Breno Pires   \n",
       "\n",
       "                                              source             category  \\\n",
       "0  http://politica.estadao.com.br/blogs/coluna-do...             politica   \n",
       "1  http://esportes.estadao.com.br/noticias/futebo...  sociedade_cotidiano   \n",
       "2  http://politica.estadao.com.br/noticias/geral,...             politica   \n",
       "3  http://www1.folha.uol.com.br/colunas/monicaber...             politica   \n",
       "4  http://politica.estadao.com.br/blogs/fausto-ma...             politica   \n",
       "\n",
       "         date tokens words_without_punctuation types number_of_links  ...  \\\n",
       "0  13/12/2017    168                       148   107            None  ...   \n",
       "1  26/12/2017    349                       294   182            None  ...   \n",
       "2  26/12/2017    249                       204   128            None  ...   \n",
       "3   11/1/2018    170                       147   102            None  ...   \n",
       "4  28/12/2017    389                       341   181            None  ...   \n",
       "\n",
       "  singular_first_and_second_personal_pronouns plural_first_personal_pronouns  \\\n",
       "0                                           0                              0   \n",
       "1                                           0                              0   \n",
       "2                                           3                              0   \n",
       "3                                           0                              0   \n",
       "4                                           0                              0   \n",
       "\n",
       "  pronouns pausality characters avg_sentence_length avg_word_length  \\\n",
       "0        7   3.33333        761             24.6667         5.14189   \n",
       "1       29      2.75       1477                14.7         5.02381   \n",
       "2        8       3.0       1019                13.6          4.9951   \n",
       "3        7     2.875        700              18.375          4.7619   \n",
       "4       14   2.66667       1587             18.9444         4.65396   \n",
       "\n",
       "  percentage_of_spelling_errors emotiveness diversity  \n",
       "0                           0.0    0.134328  0.722973  \n",
       "1                           0.0    0.325758  0.619048  \n",
       "2                     0.0147059        0.16  0.627451  \n",
       "3                           0.0   0.0694444  0.693878  \n",
       "4                    0.00879765     0.11039  0.530792  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "68fe0241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignando variáveis dependentes e independentes\n",
    "\n",
    "y = complete_data['labels'].values # y is strings for labels; but should be fake-0/true-1\n",
    "X = [d.split() for d in complete_data['texts'].tolist()] # X is a list of lists of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "95bd2774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145\n"
     ]
    }
   ],
   "source": [
    "print(X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0849b4e4",
   "metadata": {},
   "source": [
    "## Tokenization (TensorFlow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ec5c20fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando módulos para tokenização de textos\n",
    "import gensim\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "18a2d75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizando slicing de textos\n",
    "\n",
    "\n",
    "#NÃO ESTÁ ALEATÓRIO AINDA - jogar train_test_split\n",
    "\n",
    "test_limit = 15\n",
    "\n",
    "training_sentences = X[0:test_limit]\n",
    "testing_sentences = X[test_limit:]\n",
    "\n",
    "training_labels = y[0:test_limit]\n",
    "testing_labels = y[test_limit:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "52174586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerando dicionário de tokens (com base nos textos de treinamento)\n",
    "\n",
    "tokenizer = Tokenizer(oov_token='<OOV>')\n",
    "\n",
    "tokenizer.fit_on_texts(training_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "014ec728",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<OOV>': 1,\n",
       " 'de': 2,\n",
       " 'o': 3,\n",
       " 'que': 4,\n",
       " 'a': 5,\n",
       " 'do': 6,\n",
       " 'e': 7,\n",
       " 'da': 8,\n",
       " 'em': 9,\n",
       " 'não': 10,\n",
       " 'no': 11,\n",
       " 'para': 12,\n",
       " 'um': 13,\n",
       " 'uma': 14,\n",
       " 'na': 15,\n",
       " 'foi': 16,\n",
       " 'ao': 17,\n",
       " 'é': 18,\n",
       " 'com': 19,\n",
       " 'por': 20,\n",
       " 'à': 21,\n",
       " 'dos': 22,\n",
       " 'como': 23,\n",
       " 'se': 24,\n",
       " 'os': 25,\n",
       " 'sua': 26,\n",
       " 'as': 27,\n",
       " 'partido': 28,\n",
       " 'pelo': 29,\n",
       " 'federal': 30,\n",
       " 'diz': 31,\n",
       " 'ser': 32,\n",
       " 'seu': 33,\n",
       " 'das': 34,\n",
       " 'disse': 35,\n",
       " 'ele': 36,\n",
       " 'ter': 37,\n",
       " 'são': 38,\n",
       " 'presidente': 39,\n",
       " 'muito': 40,\n",
       " 'também': 41,\n",
       " 'governo': 42,\n",
       " 'público': 43,\n",
       " 'isso': 44,\n",
       " 'já': 45,\n",
       " 'ou': 46,\n",
       " 'você': 47,\n",
       " 'vai': 48,\n",
       " 'mas': 49,\n",
       " 'prefeito': 50,\n",
       " 'psdb': 51,\n",
       " 'alckmin': 52,\n",
       " 'tem': 53,\n",
       " 'silvio': 54,\n",
       " 'pt': 55,\n",
       " 'anos': 56,\n",
       " 'após': 57,\n",
       " 'ainda': 58,\n",
       " 'candidato': 59,\n",
       " 'sobre': 60,\n",
       " 'ano': 61,\n",
       " 'estão': 62,\n",
       " 'paulo': 63,\n",
       " 'dia': 64,\n",
       " 'até': 65,\n",
       " 'bolsonaro': 66,\n",
       " 'tempo': 67,\n",
       " 'segundo': 68,\n",
       " 'santos': 69,\n",
       " 'campanha': 70,\n",
       " 'estado': 71,\n",
       " 'dinheiro': 72,\n",
       " 'r': 73,\n",
       " 'suas': 74,\n",
       " 'projeto': 75,\n",
       " 'pela': 76,\n",
       " 'ministério': 77,\n",
       " 'kátia': 78,\n",
       " 'era': 79,\n",
       " 'lei': 80,\n",
       " 'nesta': 81,\n",
       " 'dois': 82,\n",
       " 'pode': 83,\n",
       " 'outro': 84,\n",
       " 'está': 85,\n",
       " 'senador': 86,\n",
       " 'país': 87,\n",
       " 'bivar': 88,\n",
       " 'pmdb': 89,\n",
       " 'nacional': 90,\n",
       " 'clube': 91,\n",
       " 'alguns': 92,\n",
       " 'foram': 93,\n",
       " 'mais': 94,\n",
       " 'quando': 95,\n",
       " 'eles': 96,\n",
       " 'presidência': 97,\n",
       " 'prévia': 98,\n",
       " 'doria': 99,\n",
       " 'grande': 100,\n",
       " 'celso': 101,\n",
       " 'teatro': 102,\n",
       " 'ferraz': 103,\n",
       " 'às': 104,\n",
       " 'contra': 105,\n",
       " 'portal': 106,\n",
       " 'direitos': 107,\n",
       " 'pessoas': 108,\n",
       " 'momento': 109,\n",
       " 'fidel': 110,\n",
       " 'deve': 111,\n",
       " 'raúl': 112,\n",
       " 'depois': 113,\n",
       " 'psl': 114,\n",
       " 'luislinda': 115,\n",
       " 'fazer': 116,\n",
       " 'câmara': 117,\n",
       " 'legenda': 118,\n",
       " 'abreu': 119,\n",
       " 'entre': 120,\n",
       " 'tenha': 121,\n",
       " 'passado': 122,\n",
       " 'cargo': 123,\n",
       " 'janeiro': 124,\n",
       " 'flamengo': 125,\n",
       " 'feito': 126,\n",
       " 'jogadores': 127,\n",
       " 'futebol': 128,\n",
       " 'exterior': 129,\n",
       " 'falta': 130,\n",
       " 'virgílio': 131,\n",
       " 'governador': 132,\n",
       " 'menos': 133,\n",
       " 'eu': 134,\n",
       " 'político': 135,\n",
       " 'vitória': 136,\n",
       " 'joão': 137,\n",
       " 'zé': 138,\n",
       " 'oficina': 139,\n",
       " 'grupo': 140,\n",
       " 'afirma': 141,\n",
       " 'geddel': 142,\n",
       " 'então': 143,\n",
       " 'assessoria': 144,\n",
       " 'valor': 145,\n",
       " 'palavra': 146,\n",
       " 'juiz': 147,\n",
       " 'sérgio': 148,\n",
       " 'volta': 149,\n",
       " 'justiça': 150,\n",
       " 'mpma': 151,\n",
       " 'ação': 152,\n",
       " 'paço': 153,\n",
       " 'lumiar': 154,\n",
       " 'domingos': 155,\n",
       " 'dutra': 156,\n",
       " 'município': 157,\n",
       " 'acesso': 158,\n",
       " 'vezes': 159,\n",
       " 'dias': 160,\n",
       " 'nova': 161,\n",
       " 'cuba': 162,\n",
       " 'aos': 163,\n",
       " 'irmãos': 164,\n",
       " 'durante': 165,\n",
       " 'atual': 166,\n",
       " 'fez': 167,\n",
       " 'onde': 168,\n",
       " 'bala': 169,\n",
       " 'liberal': 170,\n",
       " 'cada': 171,\n",
       " 'ustra': 172,\n",
       " 'ela': 173,\n",
       " 'expulsão': 174,\n",
       " 'deputado': 175,\n",
       " 'carlos': 176,\n",
       " 'polícia': 177,\n",
       " 'dele': 178,\n",
       " 'espaço': 179,\n",
       " 'receber': 180,\n",
       " 'meio': 181,\n",
       " 'nota': 182,\n",
       " 'informou': 183,\n",
       " 'parlamentar': 184,\n",
       " 'algo': 185,\n",
       " 'sigla': 186,\n",
       " 'semana': 187,\n",
       " 'hoje': 188,\n",
       " 'chegou': 189,\n",
       " 'terçafeira': 190,\n",
       " 'jogo': 191,\n",
       " 'rio': 192,\n",
       " 'críticas': 193,\n",
       " 'dentro': 194,\n",
       " 'acho': 195,\n",
       " 'antes': 196,\n",
       " 'trazer': 197,\n",
       " 'gente': 198,\n",
       " 'avaliação': 199,\n",
       " 'trabalho': 200,\n",
       " 'melhor': 201,\n",
       " 'mesmo': 202,\n",
       " 'precisa': 203,\n",
       " 'preciso': 204,\n",
       " 'final': 205,\n",
       " 'representa': 206,\n",
       " 'história': 207,\n",
       " 'resultado': 208,\n",
       " 'duas': 209,\n",
       " 'arthur': 210,\n",
       " 'maior': 211,\n",
       " 'me': 212,\n",
       " 'imprensa': 213,\n",
       " 'primeiro': 214,\n",
       " 'prefeitura': 215,\n",
       " 'somente': 216,\n",
       " 'reunião': 217,\n",
       " 'recebeu': 218,\n",
       " 'processo': 219,\n",
       " 'porque': 220,\n",
       " 'gustavo': 221,\n",
       " 'pf': 222,\n",
       " 'presidencial': 223,\n",
       " 'aécio': 224,\n",
       " 'salvador': 225,\n",
       " 'bahia': 226,\n",
       " 'milhões': 227,\n",
       " 'encontradas': 228,\n",
       " 'nas': 229,\n",
       " 'função': 230,\n",
       " 'fim': 231,\n",
       " 'política': 232,\n",
       " 'operação': 233,\n",
       " 'lava': 234,\n",
       " 'jato': 235,\n",
       " 'situação': 236,\n",
       " 'seja': 237,\n",
       " 'eleitoral': 238,\n",
       " 'acordo': 239,\n",
       " 'caso': 240,\n",
       " 'pública': 241,\n",
       " 'novembro': 242,\n",
       " 'informação': 243,\n",
       " 'transparência': 244,\n",
       " 'poder': 245,\n",
       " 'três': 246,\n",
       " 'sem': 247,\n",
       " 'há': 248,\n",
       " 'morte': 249,\n",
       " 'líder': 250,\n",
       " 'municipal': 251,\n",
       " 'exgovernador': 252,\n",
       " 'cabral': 253,\n",
       " 'juca': 254,\n",
       " 'tony': 255,\n",
       " 'chegaram': 256,\n",
       " 'tão': 257,\n",
       " 'militar': 258,\n",
       " 'dilma': 259,\n",
       " 'nunca': 260,\n",
       " 'feita': 261,\n",
       " 'sentido': 262,\n",
       " 'jogador': 263,\n",
       " 'dedada': 264,\n",
       " 'dedadas': 265,\n",
       " 'tréllez': 266,\n",
       " 'dedo': 267,\n",
       " 'bunda': 268,\n",
       " 'apenas': 269,\n",
       " 'busca': 270,\n",
       " 'propriedade': 271,\n",
       " 'podemos': 272,\n",
       " 'decidiu': 273,\n",
       " 'senadora': 274,\n",
       " 'diante': 275,\n",
       " 'filiação': 276,\n",
       " 'senado': 277,\n",
       " 'apoia': 278,\n",
       " 'crimes': 279,\n",
       " 'todos': 280,\n",
       " 'realizado': 281,\n",
       " 'zico': 282,\n",
       " 'questionado': 283,\n",
       " 'investimentos': 284,\n",
       " 'competição': 285,\n",
       " 'especialmente': 286,\n",
       " 'frente': 287,\n",
       " 'faz': 288,\n",
       " 'saber': 289,\n",
       " 'aconteceu': 290,\n",
       " 'bem': 291,\n",
       " 'assim': 292,\n",
       " 'termina': 293,\n",
       " 'lógico': 294,\n",
       " 'objetivo': 295,\n",
       " 'boa': 296,\n",
       " 'atuação': 297,\n",
       " 'conseguiu': 298,\n",
       " 'manaus': 299,\n",
       " 'carta': 300,\n",
       " 'prévias': 301,\n",
       " 'geraldo': 302,\n",
       " 'divide': 303,\n",
       " 'escolhe': 304,\n",
       " 'escolher': 305,\n",
       " 'quanto': 306,\n",
       " 'podem': 307,\n",
       " 'dar': 308,\n",
       " 'exemplo': 309,\n",
       " 'tucanocídio': 310,\n",
       " 'afirmou': 311,\n",
       " 'terça': 312,\n",
       " 'colega': 313,\n",
       " 'santoso': 314,\n",
       " 'josé': 315,\n",
       " 'torres': 316,\n",
       " 'construir': 317,\n",
       " 'sede': 318,\n",
       " 'maestro': 319,\n",
       " 'martins': 320,\n",
       " 'quintafeira': 321,\n",
       " 'defesa': 322,\n",
       " 'fará': 323,\n",
       " 'local': 324,\n",
       " 'movimento': 325,\n",
       " 'admitiu': 326,\n",
       " 'malas': 327,\n",
       " 'trabalhado': 328,\n",
       " 'neves': 329,\n",
       " 'civil': 330,\n",
       " 'bunker': 331,\n",
       " 'digitais': 332,\n",
       " 'mala': 333,\n",
       " 'exministro': 334,\n",
       " 'voo': 335,\n",
       " 'verdade': 336,\n",
       " 'estadual': 337,\n",
       " 'época': 338,\n",
       " 'alega': 339,\n",
       " 'naquele': 340,\n",
       " 'souto': 341,\n",
       " 'preso': 342,\n",
       " 'tesouro': 343,\n",
       " 'pagou': 344,\n",
       " 'cadeia': 345,\n",
       " 'supremo': 346,\n",
       " 'aberto': 347,\n",
       " 'existir': 348,\n",
       " 'delcídio': 349,\n",
       " 'amaral': 350,\n",
       " 'moro': 351,\n",
       " 'recursos': 352,\n",
       " 'pelos': 353,\n",
       " 'anistia': 354,\n",
       " 'entrevista': 355,\n",
       " 'fiscais': 356,\n",
       " 'tipo': 357,\n",
       " 'vara': 358,\n",
       " 'criminal': 359,\n",
       " 'políticos': 360,\n",
       " 'corrupção': 361,\n",
       " 'difícil': 362,\n",
       " 'adiante': 363,\n",
       " 'outros': 364,\n",
       " 'teve': 365,\n",
       " 'contas': 366,\n",
       " 'mil': 367,\n",
       " 'condenação': 368,\n",
       " 'comunicação': 369,\n",
       " 'sendo': 370,\n",
       " 'todas': 371,\n",
       " 'providências': 372,\n",
       " 'atenda': 373,\n",
       " 'exigências': 374,\n",
       " 'veja': 375,\n",
       " 'completa': 376,\n",
       " 'levar': 377,\n",
       " 'pagamento': 378,\n",
       " 'benefícios': 379,\n",
       " 'prazo': 380,\n",
       " 'danos': 381,\n",
       " 'procuradoria': 382,\n",
       " 'site': 383,\n",
       " 'dentre': 384,\n",
       " 'impossibilidade': 385,\n",
       " 'real': 386,\n",
       " 'conteúdo': 387,\n",
       " 'pessoal': 388,\n",
       " 'informações': 389,\n",
       " 'acompanhamento': 390,\n",
       " 'além': 391,\n",
       " 'go': 392,\n",
       " 'acrescentou': 393,\n",
       " 'gestão': 394,\n",
       " 'castro': 395,\n",
       " 'partir': 396,\n",
       " 'estados': 397,\n",
       " 'unidos': 398,\n",
       " 'período': 399,\n",
       " 'qual': 400,\n",
       " 'muitos': 401,\n",
       " 'desde': 402,\n",
       " 'reformas': 403,\n",
       " 'algumas': 404,\n",
       " 'relação': 405,\n",
       " 'sob': 406,\n",
       " 'comando': 407,\n",
       " 'votação': 408,\n",
       " 'eleição': 409,\n",
       " 'transição': 410,\n",
       " 'comunista': 411,\n",
       " 'futuro': 412,\n",
       " 'nós': 413,\n",
       " 'sabemos': 414,\n",
       " 'mandato': 415,\n",
       " 'vejo': 416,\n",
       " 'saída': 417,\n",
       " 'algum': 418,\n",
       " 'desaparecimento': 419,\n",
       " 'cidade': 420,\n",
       " 'santiago': 421,\n",
       " 'doleiros': 422,\n",
       " 'acusados': 423,\n",
       " 'lavar': 424,\n",
       " 'claret': 425,\n",
       " 'barbosa': 426,\n",
       " 'h': 427,\n",
       " 'agentes': 428,\n",
       " 'zona': 429,\n",
       " 'parte': 430,\n",
       " 'através': 431,\n",
       " 'esquema': 432,\n",
       " 'chefiado': 433,\n",
       " 'presos': 434,\n",
       " 'uruguai': 435,\n",
       " 'marcelo': 436,\n",
       " 'chebar': 437,\n",
       " 'mpf': 438,\n",
       " 'contaram': 439,\n",
       " 'assumir': 440,\n",
       " 'disso': 441,\n",
       " 'expressão': 442,\n",
       " 'quatro': 443,\n",
       " 'précandidato': 444,\n",
       " 'luz': 445,\n",
       " 'rompeu': 446,\n",
       " 'união': 447,\n",
       " 'comportamento': 448,\n",
       " 'aceitar': 449,\n",
       " 'essa': 450,\n",
       " 'coronel': 451,\n",
       " 'brilhante': 452,\n",
       " 'ditadura': 453,\n",
       " 'memória': 454,\n",
       " 'meu': 455,\n",
       " 'elogios': 456,\n",
       " 'entender': 457,\n",
       " 'militares': 458,\n",
       " 'regime': 459,\n",
       " 'próprios': 460,\n",
       " 'ninguém': 461,\n",
       " 'tribunal': 462,\n",
       " 'incitação': 463,\n",
       " 'discurso': 464,\n",
       " 'exministra': 465,\n",
       " 'humanos': 466,\n",
       " 'deputada': 467,\n",
       " 'nenhuma': 468,\n",
       " 'agora': 469,\n",
       " 'cara': 470,\n",
       " 'direito': 471,\n",
       " 'constitucional': 472,\n",
       " 'essas': 473,\n",
       " 'homem': 474,\n",
       " 'sport': 475,\n",
       " 'decisão': 476,\n",
       " 'marketing': 477,\n",
       " 'deveria': 478,\n",
       " 'colocar': 479,\n",
       " 'moldura': 480,\n",
       " 'reclamar': 481,\n",
       " 'currículo': 482,\n",
       " 'servem': 483,\n",
       " 'seus': 484,\n",
       " 'parece': 485,\n",
       " 'demais': 486,\n",
       " 'blog': 487,\n",
       " 'jornal': 488,\n",
       " 'rodrigo': 489,\n",
       " 'ponte': 490,\n",
       " 'preta': 491,\n",
       " '\\x93o': 492,\n",
       " 'medo': 493,\n",
       " 'sequer': 494,\n",
       " 'conseguem': 495,\n",
       " 'falar': 496,\n",
       " 'hetero': 497,\n",
       " 'blogueiro': 498,\n",
       " 'tucana': 499,\n",
       " '\\x93desembarque\\x94': 500,\n",
       " 'mesma': 501,\n",
       " 'teatral': 502,\n",
       " 'privada': 503,\n",
       " 'diálogo': 504,\n",
       " 'temer': 505,\n",
       " 'problema': 506,\n",
       " 'ministra': 507,\n",
       " '\\x93sem': 508,\n",
       " 'expulsar': 509,\n",
       " 'gaguim': 510,\n",
       " 'buscas': 511,\n",
       " 'apreensões': 512,\n",
       " 'gabinete': 513,\n",
       " 'abre': 514,\n",
       " 'expulsa': 515,\n",
       " 'katia': 516,\n",
       " 'afastamento': 517,\n",
       " 'acordado': 518,\n",
       " 'filiados': 519,\n",
       " 'comunicado': 520,\n",
       " 'conclusão': 521,\n",
       " 'desfiliação': 522,\n",
       " 'esta': 523,\n",
       " 'fatos': 524,\n",
       " 'noticiados': 525,\n",
       " 'executiva': 526,\n",
       " 'solicita': 527,\n",
       " 'imediato': 528,\n",
       " 'cancelamento': 529,\n",
       " 'quadros': 530,\n",
       " 'partidoo': 531,\n",
       " 'cogitar': 532,\n",
       " 'lançar': 533,\n",
       " 'investigação': 534,\n",
       " 'ampla': 535,\n",
       " 'apuração': 536,\n",
       " 'eventuais': 537,\n",
       " 'cometidos': 538,\n",
       " 'consequente': 539,\n",
       " 'responsabilização': 540,\n",
       " 'envolvidos': 541,\n",
       " 'sejam': 542,\n",
       " 'punidos': 543,\n",
       " 'máximo': 544,\n",
       " 'rigor': 545,\n",
       " 'independentemente': 546,\n",
       " 'posição': 547,\n",
       " 'ocupado': 548,\n",
       " 'evento': 549,\n",
       " 'divulgar': 550,\n",
       " 'estrelas': 551,\n",
       " 'amistoso': 552,\n",
       " 'beneficente': 553,\n",
       " 'será': 554,\n",
       " 'quarta': 555,\n",
       " 'estádio': 556,\n",
       " 'maracanã': 557,\n",
       " 'ídolo': 558,\n",
       " 'poupou': 559,\n",
       " 'montagem': 560,\n",
       " 'elencominha': 561,\n",
       " 'visão': 562,\n",
       " 'surtiram': 563,\n",
       " 'efeito': 564,\n",
       " 'campo': 565,\n",
       " 'equipe': 566,\n",
       " 'supervalorizada': 567,\n",
       " 'falei': 568,\n",
       " 'reclamou': 569,\n",
       " 'fácil': 570,\n",
       " 'muita': 571,\n",
       " 'mudanças': 572,\n",
       " 'constantes': 573,\n",
       " 'lesões': 574,\n",
       " 'coisa': 575,\n",
       " 'funcionouna': 576,\n",
       " 'faltou': 577,\n",
       " 'contratados': 578,\n",
       " 'vieram': 579,\n",
       " 'seguir': 580,\n",
       " 'analisar': 581,\n",
       " 'vocês': 582,\n",
       " 'principalmente': 583,\n",
       " 'atrás': 584,\n",
       " 'rendimento': 585,\n",
       " 'questões': 586,\n",
       " 'saúde': 587,\n",
       " 'alertou': 588,\n",
       " 'infelizmente': 589,\n",
       " 'investiuzico': 590,\n",
       " 'avaliou': 591,\n",
       " 'diretoria': 592,\n",
       " 'preocupar': 593,\n",
       " 'identificados': 594,\n",
       " 'reta': 595,\n",
       " 'temporada': 596,\n",
       " 'criticou': 597,\n",
       " 'suposta': 598,\n",
       " 'garra': 599,\n",
       " 'atletas': 600,\n",
       " 'camisa': 601,\n",
       " 'entendido': 602,\n",
       " 'diferente': 603,\n",
       " 'outroso': 604,\n",
       " 'maneira': 605,\n",
       " 'frustrante': 606,\n",
       " 'torcedor': 607,\n",
       " 'principal': 608,\n",
       " 'libertadores': 609,\n",
       " 'consequentemente': 610,\n",
       " 'chegar': 611,\n",
       " 'sulamericana': 612,\n",
       " 'finais': 613,\n",
       " 'investimento': 614,\n",
       " 'esperado': 615,\n",
       " 'isto': 616,\n",
       " 'acontecesse': 617,\n",
       " 'ficou': 618,\n",
       " 'gostinho': 619,\n",
       " 'amargo': 620,\n",
       " 'conquistado': 621,\n",
       " 'internacional': 622,\n",
       " 'enviado': 623,\n",
       " 'cobrando': 624,\n",
       " 'definir': 625,\n",
       " 'república': 626,\n",
       " 'negou': 627,\n",
       " 'racha': 628,\n",
       " 'postulam': 629,\n",
       " 'ambiente': 630,\n",
       " 'restrito': 631,\n",
       " 'universo': 632,\n",
       " 'amplia': 633,\n",
       " 'escuta': 634,\n",
       " 'erra': 635,\n",
       " 'acerta': 636,\n",
       " 'democracias': 637,\n",
       " 'partidos': 638,\n",
       " 'cartório': 639,\n",
       " 'precisam': 640,\n",
       " 'vida': 641,\n",
       " 'democráticaquestionado': 642,\n",
       " 'possível': 643,\n",
       " 'desunião': 644,\n",
       " 'tucanos': 645,\n",
       " 'minimizou': 646,\n",
       " 'olha': 647,\n",
       " 'lembro': 648,\n",
       " 'matéria': 649,\n",
       " 'capital': 650,\n",
       " 'chama': 651,\n",
       " 'turno': 652,\n",
       " 'referência': 653,\n",
       " 'escreveu': 654,\n",
       " 'pedindo': 655,\n",
       " 'acabe': 656,\n",
       " 'dissemedisse': 657,\n",
       " 'apreço': 658,\n",
       " 'representante': 659,\n",
       " 'receberá': 660,\n",
       " 'dramaturgo': 661,\n",
       " 'martinez': 662,\n",
       " 'corrêa': 663,\n",
       " 'arquitetas': 664,\n",
       " 'discutir': 665,\n",
       " 'imbróglio': 666,\n",
       " 'quer': 667,\n",
       " 'lado': 668,\n",
       " 'companhia': 669,\n",
       " 'vereador': 670,\n",
       " 'eduardo': 671,\n",
       " 'suplicy': 672,\n",
       " 'pediu': 673,\n",
       " 'devem': 674,\n",
       " 'comparecer': 675,\n",
       " 'representantes': 676,\n",
       " 'gerou': 677,\n",
       " 'protestos': 678,\n",
       " 'ladotô': 679,\n",
       " 'dentroo': 680,\n",
       " 'entrando': 681,\n",
       " 'valer': 682,\n",
       " 'participou': 683,\n",
       " 'tombamento': 684,\n",
       " 'secretário': 685,\n",
       " 'cultura': 686,\n",
       " 'causa': 687,\n",
       " 'considera': 688,\n",
       " 'herói': 689,\n",
       " 'concerto': 690,\n",
       " 'apoio': 691,\n",
       " 'pedreira': 692,\n",
       " 'buscar': 693,\n",
       " 'vieira': 694,\n",
       " 'lima': 695,\n",
       " 'exdiretor': 696,\n",
       " 'resolveu': 697,\n",
       " 'colaborar': 698,\n",
       " 'investigações': 699,\n",
       " 'cédulas': 700,\n",
       " 'rechearam': 701,\n",
       " 'caixas': 702,\n",
       " 'apreensão': 703,\n",
       " 'nega': 704,\n",
       " 'campanhaconselheiro': 705,\n",
       " 'ética': 706,\n",
       " 'buscou': 707,\n",
       " 'hotel': 708,\n",
       " 'voltou': 709,\n",
       " 'fretado': 710,\n",
       " 'levado': 711,\n",
       " 'motorista': 712,\n",
       " 'casa': 713,\n",
       " 'aonde': 714,\n",
       " 'contado': 715,\n",
       " 'traído': 716,\n",
       " 'esperava': 717,\n",
       " 'repassasse': 718,\n",
       " 'montante': 719,\n",
       " 'campanhas': 720,\n",
       " 'candidatos': 721,\n",
       " 'baianos': 722,\n",
       " 'prefeituras': 723,\n",
       " 'parar': 724,\n",
       " 'bunkerele': 725,\n",
       " 'colaborou': 726,\n",
       " 'eleições': 727,\n",
       " 'sido': 728,\n",
       " 'assessor': 729,\n",
       " 'bancada': 730,\n",
       " 'assembleia': 731,\n",
       " 'legislativa': 732,\n",
       " 'bahiaferraz': 733,\n",
       " 'assumiu': 734,\n",
       " 'superintendência': 735,\n",
       " 'indústria': 736,\n",
       " 'comércio': 737,\n",
       " 'exerceu': 738,\n",
       " 'presidenteferraz': 739,\n",
       " 'perdido': 740,\n",
       " 'serem': 741,\n",
       " 'notas': 742,\n",
       " 'fiança': 743,\n",
       " 'deixar': 744,\n",
       " 'estipulado': 745,\n",
       " 'salários': 746,\n",
       " 'mínimoscom': 747,\n",
       " 'neveso': 748,\n",
       " 'sr': 749,\n",
       " 'trabalhou': 750,\n",
       " 'soutoa': 751,\n",
       " 'reportagem': 752,\n",
       " 'localizou': 753,\n",
       " 'manifestação': 754,\n",
       " 'tentar': 755,\n",
       " 'interferir': 756,\n",
       " 'estiveram': 757,\n",
       " 'atagonismo': 758,\n",
       " 'sulmatogrossense': 759,\n",
       " 'autor': 760,\n",
       " 'premitiria': 761,\n",
       " 'declarados': 762,\n",
       " 'proposta': 763,\n",
       " 'amplamente': 764,\n",
       " 'criticada': 765,\n",
       " 'magistrados': 766,\n",
       " 'qualificaram': 767,\n",
       " 'corruptos': 768,\n",
       " 'comanda': 769,\n",
       " 'qualificou': 770,\n",
       " 'vergonha': 771,\n",
       " 'embora': 772,\n",
       " 'destinada': 773,\n",
       " 'descaminho': 774,\n",
       " 'financeiros': 775,\n",
       " 'incluindo': 776,\n",
       " 'evasão': 777,\n",
       " 'divisas': 778,\n",
       " 'prática': 779,\n",
       " 'favorecer': 780,\n",
       " 'todo': 781,\n",
       " 'criminoso': 782,\n",
       " 'titular': 783,\n",
       " 'curitiba': 784,\n",
       " 'magistrado': 785,\n",
       " 'poderia': 786,\n",
       " 'contemplar': 787,\n",
       " 'fraudadores': 788,\n",
       " 'remeteram': 789,\n",
       " 'públicos': 790,\n",
       " 'paraísos': 791,\n",
       " 'corrupto': 792,\n",
       " 'internar': 793,\n",
       " 'declarando': 794,\n",
       " 'produto': 795,\n",
       " 'investigar': 796,\n",
       " 'discriminar': 797,\n",
       " 'origem': 798,\n",
       " 'desse': 799,\n",
       " 'naquela': 800,\n",
       " 'esse': 801,\n",
       " 'aposta': 802,\n",
       " 'aprovação': 803,\n",
       " 'reforçar': 804,\n",
       " 'orçamentoveja': 805,\n",
       " 'pontos': 806,\n",
       " 'polêmicos': 807,\n",
       " 'carreira': 808,\n",
       " 'investigadas': 809,\n",
       " 'ms': 810,\n",
       " 'ligado': 811,\n",
       " 'roberto': 812,\n",
       " 'costa': 813,\n",
       " 'pinho': 814,\n",
       " 'sacou': 815,\n",
       " 'conta': 816,\n",
       " 'agência': 817,\n",
       " 'smpb': 818,\n",
       " 'marcos': 819,\n",
       " 'valérioquando': 820,\n",
       " 'cpi': 821,\n",
       " 'correios': 822,\n",
       " 'costurou': 823,\n",
       " 'preservar': 824,\n",
       " 'fundos': 825,\n",
       " 'pensão': 826,\n",
       " 'quebra': 827,\n",
       " 'sigilo': 828,\n",
       " 'bancário': 829,\n",
       " 'fiscal': 830,\n",
       " 'mato': 831,\n",
       " 'grosso': 832,\n",
       " 'sul': 833,\n",
       " 'investigou': 834,\n",
       " 'envolvimento': 835,\n",
       " 'mensalão': 836,\n",
       " 'zeca': 837,\n",
       " 'maranhão': 838,\n",
       " 'ingressou': 839,\n",
       " 'francisco': 840,\n",
       " 'filho': 841,\n",
       " 'pc': 842,\n",
       " 'b': 843,\n",
       " 'promotoria': 844,\n",
       " 'comarca': 845,\n",
       " 'referente': 846,\n",
       " 'pede': 847,\n",
       " 'gestor': 848,\n",
       " 'improbidade': 849,\n",
       " 'administrativa': 850,\n",
       " 'descumpre': 851,\n",
       " 'possui': 852,\n",
       " 'adequado': 853,\n",
       " 'exigência': 854,\n",
       " 'legala': 855,\n",
       " 'tomadas': 856,\n",
       " 'reportagemsegundo': 857,\n",
       " 'perda': 858,\n",
       " 'suspensão': 859,\n",
       " 'multa': 860,\n",
       " 'remuneração': 861,\n",
       " 'proibição': 862,\n",
       " 'contratar': 863,\n",
       " 'pedida': 864,\n",
       " 'morais': 865,\n",
       " 'difusos': 866,\n",
       " 'milde': 867,\n",
       " 'encaminhado': 868,\n",
       " 'recomendação': 869,\n",
       " 'regularização': 870,\n",
       " 'pendências': 871,\n",
       " 'resposta': 872,\n",
       " 'sustentou': 873,\n",
       " 'haveria': 874,\n",
       " 'prova': 875,\n",
       " 'demonstrasse': 876,\n",
       " 'inoperância': 877,\n",
       " 'siteem': 878,\n",
       " 'abril': 879,\n",
       " 'técnica': 880,\n",
       " 'geral': 881,\n",
       " 'concluiu': 882,\n",
       " 'inadequação': 883,\n",
       " 'problemas': 884,\n",
       " 'apontados': 885,\n",
       " 'acompanhar': 886,\n",
       " 'execução': 887,\n",
       " 'orçamentária': 888,\n",
       " 'financeira': 889,\n",
       " 'receita': 890,\n",
       " 'despesa': 891,\n",
       " 'seções': 892,\n",
       " 'prestação': 893,\n",
       " 'licitações': 894,\n",
       " 'contratos': 895,\n",
       " 'convênios': 896,\n",
       " 'patrimôniotambém': 897,\n",
       " 'constavam': 898,\n",
       " 'estrutura': 899,\n",
       " 'organizacional': 900,\n",
       " 'endereços': 901,\n",
       " 'telefones': 902,\n",
       " 'horários': 903,\n",
       " 'atendimento': 904,\n",
       " 'unidades': 905,\n",
       " 'dados': 906,\n",
       " 'programas': 907,\n",
       " 'ações': 908,\n",
       " 'projetos': 909,\n",
       " 'obras': 910,\n",
       " 'ferramentas': 911,\n",
       " 'garantissem': 912,\n",
       " 'deficiênciano': 913,\n",
       " '°': 914,\n",
       " 'realizada': 915,\n",
       " 'análise': 916,\n",
       " 'identificouse': 917,\n",
       " 'receitas': 918,\n",
       " 'despesas': 919,\n",
       " 'seção': 920,\n",
       " 'legislação': 921,\n",
       " 'outras': 922,\n",
       " 'irregularidades': 923,\n",
       " 'enviada': 924,\n",
       " 'encontrou': 925,\n",
       " 'tomando': 926,\n",
       " 'aguardar': 927,\n",
       " 'citar': 928,\n",
       " 'mostrar': 929,\n",
       " 'provar': 930,\n",
       " 'máfé': 931,\n",
       " 'intenção': 932,\n",
       " 'esconder': 933,\n",
       " 'qualquer': 934,\n",
       " 'ato': 935,\n",
       " 'comemorará': 936,\n",
       " 'aniversário': 937,\n",
       " 'emblemático': 938,\n",
       " 'sábado': 939,\n",
       " 'vigílias': 940,\n",
       " 'ilha': 941,\n",
       " 'põe': 942,\n",
       " 'marcha': 943,\n",
       " 'castrofidel': 944,\n",
       " 'símbolo': 945,\n",
       " 'esquerda': 946,\n",
       " 'revolucionária': 947,\n",
       " 'construiu': 948,\n",
       " 'socialista': 949,\n",
       " 'portas': 950,\n",
       " 'desafiou': 951,\n",
       " 'décadas': 952,\n",
       " 'esforços': 953,\n",
       " 'washington': 954,\n",
       " 'derrubálo': 955,\n",
       " 'morrendo': 956,\n",
       " 'estava': 957,\n",
       " 'distante': 958,\n",
       " 'olhos': 959,\n",
       " 'década': 960,\n",
       " 'substituído': 961,\n",
       " 'irmão': 962,\n",
       " 'vitimado': 963,\n",
       " 'doença': 964,\n",
       " 'intestinal': 965,\n",
       " 'cubanos': 966,\n",
       " 'acreditam': 967,\n",
       " 'mudou': 968,\n",
       " 'pouco': 969,\n",
       " 'falecimentoo': 970,\n",
       " 'ritmo': 971,\n",
       " 'empreendidas': 972,\n",
       " 'atualizar': 973,\n",
       " 'modelo': 974,\n",
       " 'econômico': 975,\n",
       " 'estilo': 976,\n",
       " 'soviético': 977,\n",
       " 'continuou': 978,\n",
       " 'moderação': 979,\n",
       " 'congelou': 980,\n",
       " 'agosto': 981,\n",
       " 'emissão': 982,\n",
       " 'licenças': 983,\n",
       " 'setor': 984,\n",
       " 'privado': 985,\n",
       " 'piorou': 986,\n",
       " 'donald': 987,\n",
       " 'trumpanalistas': 988,\n",
       " 'argumentam': 989,\n",
       " 'significativo': 990,\n",
       " 'politicamente': 991,\n",
       " 'ciclo': 992,\n",
       " 'começa': 993,\n",
       " 'domingo': 994,\n",
       " 'terminará': 995,\n",
       " 'novo': 996,\n",
       " 'fevereiro': 997,\n",
       " 'reiterou': 998,\n",
       " 'retirará': 999,\n",
       " 'cumprir': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0290bf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizando a sequencialização das bases de treinamento e teste\n",
    "\n",
    "training_sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "127841cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 8 news longer than 250 words.\n"
     ]
    }
   ],
   "source": [
    "# Análise da quantidade de notícias maiores do que max_length\n",
    "\n",
    "max_length = 250\n",
    "\n",
    "#plt.hist([len(x) for x in X], bins = 750)\n",
    "#plt.show()\n",
    "\n",
    "nos = np.array([len(x) for x in X])\n",
    "print(\"There are \"+ str(len(nos[nos>max_length])) + \" news longer than \"+ str(max_length) + \" words.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "cebdd2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matar notícias menores do que min_length\n",
    "\n",
    "min_length = 150\n",
    "\n",
    "flag_dict = {}\n",
    "for i in range(0,len(training_sequences)):\n",
    "    if len(training_sequences[i]) < min_length:\n",
    "        flag_dict[i] = True\n",
    "    else:\n",
    "        flag_dict[i] = False\n",
    "        \n",
    "training_labels = [training_labels[i] for i in range(0,len(flag_dict)) if (flag_dict[i] == False)]\n",
    "\n",
    "training_sequences = [seq for seq in training_sequences if len(seq)>=min_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0db3ec50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "319\n",
      "436\n",
      "419\n",
      "818\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(training_sequences)):    \n",
    "    print(len(training_sequences[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "11936835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realização da truncagem (padding) das sequências tokenizadas\n",
    "\n",
    "max_length = 150\n",
    "\n",
    "training_padded = pad_sequences(training_sequences,maxlen=max_length,padding='post',truncating='pre')\n",
    "#testing_padded = pad_sequences(testing_sequences,maxlen=max_length,padding='post',truncating='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "932623fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n",
      "150\n",
      "150\n",
      "150\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(training_padded)):    \n",
    "    print(len(training_padded[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64eb8aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = tokenizer.word_index\n",
    "inv_vocab = {v: k for k, v in vocab.items()} # reconverte o token para a palavra correpondente.\n",
    "\n",
    "training_texts = []\n",
    "testing_texts = []\n",
    "\n",
    "# Com estas listas \"_texts\", temos os textos novamente completos, mas agora com 'OOV's para palavras desconhecidas\n",
    "# na base de testes.\n",
    "for sequence in training_sequences:\n",
    "    news=[]\n",
    "    for token in sequence:\n",
    "        news.append(inv_vocab[token])  #inv_vocab[token] é a palavra correspondente ao token.\n",
    "    training_texts.append(news)\n",
    "\n",
    "\n",
    "for sequence in testing_sequences:\n",
    "    news=[]\n",
    "    for token in sequence:\n",
    "        news.append(inv_vocab[token])\n",
    "    testing_texts.append(news)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a046fba",
   "metadata": {},
   "source": [
    "Quando formos vetorizar, deveremos gerar um w2v.model somente para a base de treino.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9355159d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, Conv1D, MaxPool1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
