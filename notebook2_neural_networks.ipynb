{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5644c2fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\nakam\\anaconda3\\lib\\site-packages (3.2.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from spacy) (2.4.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from spacy) (20.9)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from spacy) (2.25.1)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from spacy) (0.7.5)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from spacy) (1.0.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from spacy) (60.5.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from spacy) (1.20.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from spacy) (8.0.13)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from spacy) (3.0.6)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from spacy) (0.6.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from spacy) (1.8.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from spacy) (4.59.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from spacy) (1.0.6)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from spacy) (0.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy) (3.7.4.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from jinja2->spacy) (1.1.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.3.1; however, version 22.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\nakam\\anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pt-core-news-sm==3.2.0\n",
      "  Using cached https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-3.2.0/pt_core_news_sm-3.2.0-py3-none-any.whl (22.2 MB)\n",
      "Requirement already satisfied: spacy<3.3.0,>=3.2.0 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from pt-core-news-sm==3.2.0) (3.2.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (3.3.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (2.25.1)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (0.4.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (0.9.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (3.0.6)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (2.4.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (1.20.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (3.0.8)\n",
      "Requirement already satisfied: setuptools in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (60.5.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (4.59.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (2.11.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (1.8.2)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (8.0.13)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (1.0.1)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (0.7.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (1.0.6)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (0.6.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (20.9)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (3.7.4.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (4.0.0)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.3.0,>=3.2.0->pt-core-news-sm==3.2.0) (1.1.1)\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('pt_core_news_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.3.1; however, version 22.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\nakam\\anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wordcloud in c:\\users\\nakam\\anaconda3\\lib\\site-packages (1.8.1)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from wordcloud) (1.20.1)\n",
      "Requirement already satisfied: pillow in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from wordcloud) (8.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.3.1; however, version 22.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\nakam\\anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from wordcloud) (3.3.4)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (1.3.1)\n",
      "Requirement already satisfied: six in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib->wordcloud) (1.15.0)\n",
      "Requirement already satisfied: gensim in c:\\users\\nakam\\anaconda3\\lib\\site-packages (4.2.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from gensim) (1.6.2)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from gensim) (1.20.1)\n",
      "Requirement already satisfied: Cython==0.29.28 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from gensim) (0.29.28)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.3.1; however, version 22.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\nakam\\anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.9.0-cp38-cp38-win_amd64.whl (444.1 MB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.26.0-cp38-cp38-win_amd64.whl (1.5 MB)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.46.1-cp38-cp38-win_amd64.whl (3.5 MB)\n",
      "Collecting flatbuffers<2,>=1.12\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from tensorflow) (20.9)\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from tensorflow) (3.7.4.3)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting protobuf>=3.9.2\n",
      "  Downloading protobuf-3.20.1-cp38-cp38-win_amd64.whl (904 kB)\n",
      "Collecting tensorboard<2.10,>=2.9\n",
      "  Downloading tensorboard-2.9.0-py3-none-any.whl (5.8 MB)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Downloading absl_py-1.0.0-py3-none-any.whl (126 kB)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from tensorflow) (1.20.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from tensorflow) (1.12.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.3.1; however, version 22.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\nakam\\anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from tensorflow) (1.15.0)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting keras-preprocessing>=1.1.1\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Collecting keras<2.10.0,>=2.9.0rc0\n",
      "  Downloading keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-14.0.1-py2.py3-none-win_amd64.whl (14.2 MB)\n",
      "Collecting tensorflow-estimator<2.10.0,>=2.9.0rc0\n",
      "  Downloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from tensorflow) (60.5.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.7-py3-none-any.whl (97 kB)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-any.whl (2.4 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.6.6-py2.py3-none-any.whl (156 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.25.1)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from packaging->tensorflow) (2.4.7)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.8-py3-none-any.whl (39 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.1.0-py3-none-any.whl (9.2 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting importlib-metadata>=4.4\n",
      "  Downloading importlib_metadata-4.11.3-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2.10)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\nakam\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (3.4.1)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
      "Building wheels for collected packages: termcolor\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4848 sha256=88b54f6b2d203d924b80881792d38d47ecb4d3814f706eb86587fd15ec4e0ea0\n",
      "  Stored in directory: c:\\users\\nakam\\appdata\\local\\pip\\cache\\wheels\\a0\\16\\9c\\5473df82468f958445479c59e784896fa24f4a5fc024b0f501\n",
      "Successfully built termcolor\n",
      "Installing collected packages: pyasn1, rsa, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, importlib-metadata, google-auth, tensorboard-plugin-wit, tensorboard-data-server, protobuf, markdown, grpcio, google-auth-oauthlib, absl-py, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard, opt-einsum, libclang, keras-preprocessing, keras, google-pasta, gast, flatbuffers, astunparse, tensorflow\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib-metadata 3.10.0\n",
      "    Uninstalling importlib-metadata-3.10.0:\n",
      "      Successfully uninstalled importlib-metadata-3.10.0\n",
      "Successfully installed absl-py-1.0.0 astunparse-1.6.3 cachetools-5.1.0 flatbuffers-1.12 gast-0.4.0 google-auth-2.6.6 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.46.1 importlib-metadata-4.11.3 keras-2.9.0 keras-preprocessing-1.1.2 libclang-14.0.1 markdown-3.3.7 oauthlib-3.2.0 opt-einsum-3.3.0 protobuf-3.20.1 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.1 rsa-4.8 tensorboard-2.9.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.9.0 tensorflow-estimator-2.9.0 tensorflow-io-gcs-filesystem-0.26.0 termcolor-1.1.0\n"
     ]
    }
   ],
   "source": [
    "# Bibliotecas além do gerenciador Anaconda\n",
    "!pip install spacy\n",
    "!python -m spacy download pt_core_news_sm\n",
    "\n",
    "!pip install wordcloud\n",
    "\n",
    "!pip install gensim\n",
    "\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5f4eced3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Módulos básicos para manuseio de dados e arquivos\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os.path import isfile, join\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "\n",
    "# Módulos para visualização de dados\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "%matplotlib inline\n",
    "\n",
    "# Módulo para processamento de linguagem\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788de39a",
   "metadata": {},
   "source": [
    "## Carregamento de textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4597ed41",
   "metadata": {},
   "outputs": [],
   "source": [
    "limited_news_path = r'Software\\Fake.br-Corpus' #\\fake_10 or \\true_10\n",
    "news_path = r'Software\\Fake.br-Corpus\\full_texts' #\\fake or \\true\n",
    "\n",
    "paths = [limited_news_path, news_path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "97d20bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sortDir(dir_path: str, is_meta=False) ->list:\n",
    "    '''\n",
    "    Ordena os arquivos dentro de dir_path e os retorna no formato de lista.\n",
    "    '''\n",
    "    if is_meta:\n",
    "        number_separator = \"-meta.txt\"\n",
    "    else:\n",
    "        number_separator = \".txt\"\n",
    "\n",
    "    first_list = os.listdir(dir_path)\n",
    "    int_list = [int(element.split(number_separator)[0]) for element in first_list]\n",
    "    int_list.sort()\n",
    "    final_list = [(str(element) + number_separator) for element in int_list]\n",
    "\n",
    "    return final_list\n",
    "\n",
    "def txtToDataframe(path, is_limited=True):\n",
    "    '''\n",
    "    Function for converting full texts to a single DataFrame.\n",
    "    '''\n",
    "    if is_limited:\n",
    "        true_files = [path+\"\\\\true_10\\\\\"+f for f in sortDir(dir_path = path+'\\\\true_10') if isfile(join(path+'\\\\true_10', f))]\n",
    "        fake_files = [path+\"\\\\fake_10\\\\\"+f for f in sortDir(dir_path = path+'\\\\fake_10') if isfile(join(path+'\\\\fake_10', f))]\n",
    "    else:\n",
    "        true_files = [path+\"\\\\true\\\\\"+f for f in sortDir(dir_path = path+'\\\\true') if isfile(join(path+'\\\\true', f))]\n",
    "        fake_files = [path+\"\\\\fake\\\\\"+f for f in sortDir(dir_path = path+'\\\\fake') if isfile(join(path+'\\\\fake', f))]\n",
    "    \n",
    "    texts = []\n",
    "    labels = []\n",
    "    \n",
    "    for file in true_files:\n",
    "        with open(file, encoding='utf-8-sig') as f:\n",
    "            texts.append(f.read())\n",
    "            labels.append('true')\n",
    "    for file in fake_files:\n",
    "        with open(file, encoding='utf-8-sig') as f:\n",
    "            texts.append(f.read())\n",
    "            labels.append('fake')\n",
    "            \n",
    "    df = pd.DataFrame(list(zip(texts,labels)),columns=['texts','labels'])\n",
    "    \n",
    "    # Com esta função, textos e labels foram inseridos em um DataFrame de maneira sequencial. Todas as notícias verdadeiras vêm\n",
    "    # ANTES do bloco de notícias falsas.\n",
    "    \n",
    "    return df\n",
    "\n",
    "def appendMetadata(path,df, is_limited=True):\n",
    "    '''\n",
    "    Function for appending metadata to previously generated news DataFrame.\n",
    "    '''\n",
    "    if is_limited:\n",
    "        true_meta = [path+\"\\\\true-meta-information-10\\\\\"+f for f in sortDir(dir_path = path+'\\\\true-meta-information-10',is_meta=True) if isfile(join(path+'\\\\true-meta-information-10', f))]\n",
    "        fake_meta = [path+\"\\\\fake-meta-information-10\\\\\"+f for f in sortDir(dir_path = path+'\\\\fake-meta-information-10',is_meta=True) if isfile(join(path+'\\\\fake-meta-information-10', f))]\n",
    "    else:\n",
    "        true_meta = [path+\"\\\\true-meta-information\\\\\"+f for f in sortDir(dir_path = path+'\\\\true-meta-information',is_meta=True) if isfile(join(path+'\\\\true-meta-information', f))]\n",
    "        fake_meta = [path+\"\\\\fake-meta-information\\\\\"+f for f in sortDir(dir_path = path+'\\\\fake-meta-information',is_meta=True) if isfile(join(path+'\\\\fake-meta-information', f))]\n",
    "    \n",
    "\n",
    "    #true_meta e fake_meta são listas com todas os paths para arquivos de metadata.\n",
    "    \n",
    "    columns = [\"author\", \"source\", \"category\", \"date\",\"tokens\",\"words_without_punctuation\",\"types\",\"number_of_links\",\"uppercase_words\",\"verbs\",\"subjuntive_imperative\",\"nouns\",\"adjectives\",\"adverbs\",\"modal_verbs\",\"singular_first_and_second_personal_pronouns\",\"plural_first_personal_pronouns\",\"pronouns\",\"pausality\",\"characters\",\"avg_sentence_length\",\"avg_word_length\",\"percentage_of_spelling_errors\",\"emotiveness\",\"diversity\"]\n",
    "    \n",
    "    true_metadata = pd.DataFrame(columns=columns)\n",
    "    fake_metadata = pd.DataFrame(columns=columns)\n",
    "    \n",
    "    for file in true_meta:\n",
    "        #print(file)\n",
    "        aux = pd.read_csv(file, header=None, sep = '\\n').transpose()\n",
    "        aux.columns = columns\n",
    "        true_metadata=true_metadata.append(aux)\n",
    "        \n",
    "        \n",
    "    for file in fake_meta:\n",
    "        #print(file)\n",
    "        aux = pd.read_csv(file, header=None, sep = '\\n').transpose()\n",
    "        aux.columns = columns\n",
    "        fake_metadata=fake_metadata.append(aux)\n",
    "        \n",
    "    \n",
    "    metadata = pd.DataFrame(columns=columns)\n",
    "    metadata = metadata.append(true_metadata,ignore_index=True)\n",
    "    metadata = metadata.append(fake_metadata,ignore_index=True) \n",
    "\n",
    "\n",
    "    complete_df = pd.concat([df,metadata],axis=1)\n",
    "    # Este DataFrame possui todos os textos/labels (2 colunas) e metadata (25 colunas).\n",
    "    \n",
    "    return complete_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d9db387d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - Base com 10 notícias verdadeiras e 10 notícias falsas\n",
      "1 - Base completa de notícias\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "ai = int(input('''0 - Base com 10 notícias verdadeiras e 10 notícias falsas\n",
    "1 - Base completa de notícias\n",
    "'''))\n",
    "\n",
    "path = paths[ai]\n",
    "\n",
    "if ai == 0:\n",
    "    data = txtToDataframe(path) # Dataframe contendo notícias e labels.\n",
    "    complete_data = appendMetadata(path,data) # Dataframe contendo notícias, labels e metadata.\n",
    "else:\n",
    "    data = txtToDataframe(path,is_limited=False)\n",
    "    complete_data = appendMetadata(path,data,is_limited=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32982cf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['texts', 'labels', 'author', 'source', 'category', 'date', 'tokens',\n",
       "       'words_without_punctuation', 'types', 'number_of_links',\n",
       "       'uppercase_words', 'verbs', 'subjuntive_imperative', 'nouns',\n",
       "       'adjectives', 'adverbs', 'modal_verbs',\n",
       "       'singular_first_and_second_personal_pronouns',\n",
       "       'plural_first_personal_pronouns', 'pronouns', 'pausality', 'characters',\n",
       "       'avg_sentence_length', 'avg_word_length',\n",
       "       'percentage_of_spelling_errors', 'emotiveness', 'diversity'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fbc2db81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'O Podemos decidiu  expulsar o deputado federal Carlos Gaguim do partido após a Polícia Federal fazer buscas a apreensões no gabinete dele na Câmara. Com isso, a legenda abre espaço para receber a senadora expulsa pelo PMDB, Katia Abreu. Por meio de nota, a legenda informou que o afastamento do parlamentar já era algo acordado entre os filiados da sigla.  “Ainda que o parlamentar tenha comunicado a conclusão de sua desfiliação para esta semana, diante dos fatos noticiados hoje, a Executiva Nacional do Podemos solicita o imediato cancelamento de sua filiação dos quadros do partido”.\\n\\nO partido, que no passado chegou a cogitar lançar o parlamentar como candidato ao Senado, diz que “apoia a investigação com a ampla apuração dos eventuais crimes cometidos e a consequente responsabilização dos envolvidos, para que todos sejam punidos com o máximo rigor da Lei, independentemente de posição ou cargo ocupado”. '"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_data['texts'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8090df3d",
   "metadata": {},
   "source": [
    "## Preprocessamento de textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ea64800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando o pacote de língua portuguesa para o processador Spacy\n",
    "nlp = spacy.load('pt_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dbf9fa87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defininido funções de preprocessamento\n",
    "\n",
    "def removePunct(text):\n",
    "    '''\n",
    "    Removes any punctuation included in string.punctuation.\n",
    "    '''\n",
    "    translator = text.maketrans({key:'' for key in string.punctuation+'“”'}) # Translates any punctuation into ''\n",
    "    return text.translate(translator)\n",
    "\n",
    "def removeNumbers(text):\n",
    "    '''\n",
    "    Removes any number character in text.\n",
    "    '''\n",
    "    return re.sub('[0-9]', '' , text) # Translates any number into ''\n",
    "\n",
    "def removeStopWords(string):\n",
    "    '''\n",
    "    Removes any portuguese stopwords, using Spacy's standard package.\n",
    "    '''\n",
    "    doc = nlp(string)\n",
    "    return ' '.join([token.text for token in doc if token.is_stop is False])\n",
    "\n",
    "def lemmatize(string):\n",
    "    '''\n",
    "    Lemmatizes text word-by-word. Notice that lemmatizing is not as harsh as stemming, which makes the final text easier to read and understand in common language.\n",
    "    '''\n",
    "    doc = nlp(string)\n",
    "    return ' '.join([token.lemma_ for token in doc])\n",
    "\n",
    "def prep(string, useStopWords = True, lemma = True):\n",
    "    '''\n",
    "    Executes previously defined preprocessing in text.\n",
    "    '''\n",
    "\n",
    "    result = removeNumbers(removePunct(string)).lower()\n",
    "    \n",
    "    if useStopWords and lemma:\n",
    "        doc = nlp(result)\n",
    "        result = ' '.join([token.lemma_ for token in doc if token.is_stop is False])\n",
    "    elif useStopWords:\n",
    "        doc = nlp(result)\n",
    "        result = ' '.join([token.text for token in doc if token.is_stop is False])\n",
    "    elif lemma:\n",
    "        doc = nlp(result)\n",
    "        result = ' '.join([token.lemma_ for token in doc])\n",
    "    return result.replace('\\n',\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "58e24c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizando preprocessamento de textos presentes no Dataframe de notícias completo.\n",
    "\n",
    "complete_data['texts'] = complete_data['texts'].apply(prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8bba55be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'decidir       expulsar deputar federal carlos gaguim polícia federal buscar apreensão gabinete dele câmara legendar abrir espaçar receber senador expulsar pmdb katia abreu notar legendar informar afastamento parlamentar acordar filiar sigla        parlamentar comunicar conclusão desfiliação semana fato noticiar hoje executivo nacional solicitar imediato cancelamento filiação quadrar       passar chegar cogitar lançar parlamentar candidatar senado   investigação amplo apuração crime cometer consequente responsabilização envolver punir rigor lei independentemente cargo ocupar'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_data['texts'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "68fe0241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignando variáveis dependentes e independentes\n",
    "\n",
    "y = complete_data['labels'].values # y is strings for labels; but should be fake-0/true-1\n",
    "X = [d.split() for d in complete_data['texts'].tolist()] # X is a list of lists of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "95bd2774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['decidir', 'expulsar', 'deputar', 'federal', 'carlos', 'gaguim', 'polícia', 'federal', 'buscar', 'apreensão', 'gabinete', 'dele', 'câmara', 'legendar', 'abrir', 'espaçar', 'receber', 'senador', 'expulsar', 'pmdb', 'katia', 'abreu', 'notar', 'legendar', 'informar', 'afastamento', 'parlamentar', 'acordar', 'filiar', 'sigla', 'parlamentar', 'comunicar', 'conclusão', 'desfiliação', 'semana', 'fato', 'noticiar', 'hoje', 'executivo', 'nacional', 'solicitar', 'imediato', 'cancelamento', 'filiação', 'quadrar', 'passar', 'chegar', 'cogitar', 'lançar', 'parlamentar', 'candidatar', 'senado', 'investigação', 'amplo', 'apuração', 'crime', 'cometer', 'consequente', 'responsabilização', 'envolver', 'punir', 'rigor', 'lei', 'independentemente', 'cargo', 'ocupar']\n"
     ]
    }
   ],
   "source": [
    "print(X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0849b4e4",
   "metadata": {},
   "source": [
    "## Tokenization (TensorFlow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ec5c20fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando módulos para tokenização de textos\n",
    "import gensim\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "18a2d75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizando slicing de textos\n",
    "\n",
    "test_limit = 15\n",
    "\n",
    "training_sentences = X[0:test_limit]\n",
    "testing_sentences = X[test_limit:]\n",
    "\n",
    "training_labels = y[0:test_limit]\n",
    "testing_labels = y[test_limit:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "52174586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerando dicionário de tokens (com base nos textos de treinamento)\n",
    "\n",
    "tokenizer = Tokenizer(oov_token='<OOV>')\n",
    "\n",
    "tokenizer.fit_on_texts(training_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0290bf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizando a sequencialização das bases de treinamento e teste\n",
    "\n",
    "training_sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127841cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análise da quantidade de notícias maiores do que max_length\n",
    "\n",
    "max_length = 2000\n",
    "\n",
    "#plt.hist([len(x) for x in X], bins = 750)\n",
    "#plt.show()\n",
    "\n",
    "nos = np.array([len(x) for x in X])\n",
    "print(\"There are \"+ str(len(nos[nos>max_length])) + \" news longer than \"+ str(max_length) + \" words.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11936835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realização da truncagem (padding) das sequências tokenizadas\n",
    "\n",
    "training_padded = pad_sequences(training_sequences,maxlen=max_length,padding='post',truncating='pre')\n",
    "testing_padded = pad_sequences(testing_sequences,maxlen=max_length,padding='post',truncating='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64eb8aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = tokenizer.word_index\n",
    "inv_vocab = {v: k for k, v in vocab.items()} # reconverte o token para a palavra correpondente.\n",
    "\n",
    "training_texts = []\n",
    "testing_texts = []\n",
    "\n",
    "# Com estas listas \"_texts\", temos os textos novamente completos, mas agora com 'OOV's para palavras desconhecidas\n",
    "# na base de testes.\n",
    "for sequence in training_sequences:\n",
    "    news=[]\n",
    "    for token in sequence:\n",
    "        news.append(inv_vocab[token])  #inv_vocab[token] é a palavra correspondente ao token.\n",
    "    training_texts.append(news)\n",
    "\n",
    "\n",
    "for sequence in testing_sequences:\n",
    "    news=[]\n",
    "    for token in sequence:\n",
    "        news.append(inv_vocab[token])\n",
    "    testing_texts.append(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ac6f6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9355159d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, Conv1D, MaxPool1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
